{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2e37af17",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from cuda import cudart\n",
    "import torch\n",
    "from torch import Tensor, nn\n",
    "import tensorrt as trt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b6bb1c0",
   "metadata": {},
   "source": [
    "## Generate input and data shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6b4876cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = dict()\n",
    "batch_size, seq_len, hidden_size = 4, 8, 4096\n",
    "intermediate_size = 11008\n",
    "config['hidden_size'] = hidden_size\n",
    "config['intermediate_size'] = intermediate_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1a3cd398",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = torch.ones(batch_size, seq_len, hidden_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2ae7572",
   "metadata": {},
   "source": [
    "## torch MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "47b9b884",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SiLUActivation(nn.Module):\n",
    "    \"\"\"\n",
    "    See Gaussian Error Linear Units (Hendrycks et al., https://arxiv.org/abs/1606.08415) where the SiLU (Sigmoid Linear\n",
    "    Unit) was originally introduced and coined, and see Sigmoid-Weighted Linear Units for Neural Network Function\n",
    "    Approximation in Reinforcement Learbatch_sizeg (Elfwing et al., https://arxiv.org/abs/1702.03118) and Swish: a Self-Gated\n",
    "    Activation Function (Ramachandran et al., https://arxiv.org/abs/1710.05941v1) where the SiLU was experimented with\n",
    "    later.\n",
    "    \"\"\"\n",
    "\n",
    "    def forward(self, input: Tensor) -> Tensor:\n",
    "        return nn.functional.silu(input)\n",
    "    \n",
    "    def b_forward(self, input: Tensor) -> Tensor:\n",
    "        return torch.matmul(input.T, nn.functional.sigmoid(input))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "04c1a1cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LlamaMLP(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.hidden_size = config['hidden_size']\n",
    "        self.intermediate_size = config['intermediate_size']\n",
    "        self.gate_proj = nn.Linear(self.hidden_size, self.intermediate_size, bias=False)\n",
    "        self.up_proj = nn.Linear(self.hidden_size, self.intermediate_size, bias=False)\n",
    "        self.down_proj = nn.Linear(self.intermediate_size, self.hidden_size, bias=False)\n",
    "        self.act_fn = SiLUActivation()\n",
    "        self.init = False\n",
    "\n",
    "    def load(self, dir):\n",
    "        weights = torch.load(dir)\n",
    "        mlp_weights = dict()\n",
    "        for key in weights.keys():\n",
    "            if key.split(\".\")[3] == \"mlp\":\n",
    "                mlp_weights[key[key.find(key.split(\".\")[4]):]] = weights[key]\n",
    "\n",
    "        self.load_state_dict(mlp_weights)\n",
    "\n",
    "    def forward(self, x):\n",
    "        down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\n",
    "\n",
    "        return down_proj"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f71defb6",
   "metadata": {},
   "source": [
    "## Test torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e9d86807",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 2.0781,  4.5118,  3.0771,  ..., -2.6065, -2.1167, -2.7058],\n",
      "         [ 2.0781,  4.5118,  3.0771,  ..., -2.6065, -2.1167, -2.7058],\n",
      "         [ 2.0781,  4.5118,  3.0771,  ..., -2.6065, -2.1167, -2.7058],\n",
      "         ...,\n",
      "         [ 2.0781,  4.5118,  3.0771,  ..., -2.6065, -2.1167, -2.7058],\n",
      "         [ 2.0781,  4.5118,  3.0771,  ..., -2.6065, -2.1167, -2.7058],\n",
      "         [ 2.0781,  4.5118,  3.0771,  ..., -2.6065, -2.1167, -2.7058]],\n",
      "\n",
      "        [[ 2.0781,  4.5118,  3.0771,  ..., -2.6065, -2.1167, -2.7058],\n",
      "         [ 2.0781,  4.5118,  3.0771,  ..., -2.6065, -2.1167, -2.7058],\n",
      "         [ 2.0781,  4.5118,  3.0771,  ..., -2.6065, -2.1167, -2.7058],\n",
      "         ...,\n",
      "         [ 2.0781,  4.5118,  3.0771,  ..., -2.6065, -2.1167, -2.7058],\n",
      "         [ 2.0781,  4.5118,  3.0771,  ..., -2.6065, -2.1167, -2.7058],\n",
      "         [ 2.0781,  4.5118,  3.0771,  ..., -2.6065, -2.1167, -2.7058]],\n",
      "\n",
      "        [[ 2.0781,  4.5118,  3.0771,  ..., -2.6065, -2.1167, -2.7058],\n",
      "         [ 2.0781,  4.5118,  3.0771,  ..., -2.6065, -2.1167, -2.7058],\n",
      "         [ 2.0781,  4.5118,  3.0771,  ..., -2.6065, -2.1167, -2.7058],\n",
      "         ...,\n",
      "         [ 2.0781,  4.5118,  3.0771,  ..., -2.6065, -2.1167, -2.7058],\n",
      "         [ 2.0781,  4.5118,  3.0771,  ..., -2.6065, -2.1167, -2.7058],\n",
      "         [ 2.0781,  4.5118,  3.0771,  ..., -2.6065, -2.1167, -2.7058]],\n",
      "\n",
      "        [[ 2.0781,  4.5118,  3.0771,  ..., -2.6065, -2.1167, -2.7058],\n",
      "         [ 2.0781,  4.5118,  3.0771,  ..., -2.6065, -2.1167, -2.7058],\n",
      "         [ 2.0781,  4.5118,  3.0771,  ..., -2.6065, -2.1167, -2.7058],\n",
      "         ...,\n",
      "         [ 2.0781,  4.5118,  3.0771,  ..., -2.6065, -2.1167, -2.7058],\n",
      "         [ 2.0781,  4.5118,  3.0771,  ..., -2.6065, -2.1167, -2.7058],\n",
      "         [ 2.0781,  4.5118,  3.0771,  ..., -2.6065, -2.1167, -2.7058]]],\n",
      "       device='cuda:0', grad_fn=<UnsafeViewBackward0>)\n",
      "torch.Size([4, 8, 4096])\n"
     ]
    }
   ],
   "source": [
    "model = LlamaMLP(config)\n",
    "\n",
    "device = torch.device(\"cuda\")\n",
    "\n",
    "model.load(\"/home/fuchiang137/.cache/huggingface/hub/models--decapoda-research--llama-7b-hf/snapshots/5f98eefcc80e437ef68d457ad7bf167c2c6a1348/pytorch_model-00019-of-00033.bin\")\n",
    "model = model.to(device)\n",
    "\n",
    "data_D = data.to(device)\n",
    "# output = model(data)\n",
    "output = model(data_D)\n",
    "\n",
    "print(output)\n",
    "print(output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "91d55d18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([11008, 4096])\n"
     ]
    }
   ],
   "source": [
    "print(model.up_proj.weight.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f317bcc5",
   "metadata": {},
   "source": [
    "## tensorRT MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8f343f0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# seq length is not specified, since it is a dynamic size\n",
    "def trt_create(batch_size, hidden_size, intermediate_size, model):\n",
    "    \n",
    "    logger = trt.Logger(trt.Logger.ERROR)\n",
    "    builder = trt.Builder(logger)\n",
    "\n",
    "    network = builder.create_network(1 << int(trt.NetworkDefinitionCreationFlag.EXPLICIT_BATCH))\n",
    "    config = builder.create_builder_config()\n",
    "\n",
    "    # input\n",
    "    inputT0 = network.add_input('inputT0', trt.DataType.FLOAT, (batch_size, -1, hidden_size))\n",
    "\n",
    "    # dynamic shape optimization\n",
    "    profile = builder.create_optimization_profile();\n",
    "    profile.set_shape(\"inputT0\", (batch_size, 1, hidden_size), (batch_size, 1, hidden_size), (batch_size, 45, hidden_size))\n",
    "    config.add_optimization_profile(profile)\n",
    "\n",
    "\n",
    "    # self.up_proj(x)\n",
    "    up_proj_weight = model.up_proj.weight.clone().detach().cpu().numpy()\n",
    "    up_proj_weight = np.expand_dims(up_proj_weight, 0)\n",
    "    up_proj_weight_shape = list(up_proj_weight.shape)\n",
    "    up_proj_weight_layer = network.add_constant(shape=up_proj_weight_shape, weights=trt.Weights(up_proj_weight))\n",
    "\n",
    "    up_proj_layer = network.add_matrix_multiply(inputT0, trt.MatrixOperation.NONE, up_proj_weight_layer.get_output(0), trt.MatrixOperation.TRANSPOSE)\n",
    "\n",
    "    # self.gate_proj(x)\n",
    "    gate_proj_weight = model.gate_proj.weight.clone().detach().cpu().numpy()\n",
    "    gate_proj_weight = np.expand_dims(gate_proj_weight, 0)\n",
    "    gate_proj_weight_shape = list(gate_proj_weight.shape)\n",
    "    gate_proj_weight_layer = network.add_constant(shape=gate_proj_weight_shape, weights=trt.Weights(gate_proj_weight))\n",
    "\n",
    "    gate_proj_layer = network.add_matrix_multiply(inputT0, trt.MatrixOperation.NONE, gate_proj_weight_layer.get_output(0), trt.MatrixOperation.TRANSPOSE)\n",
    "\n",
    "    # act_fn(self.gate_proj(x))\n",
    "    selu_sigmoid_layer = network.add_activation(gate_proj_layer.get_output(0), type=trt.ActivationType.SIGMOID)\n",
    "    selu_mult_layer = network.add_elementwise(gate_proj_layer.get_output(0), selu_sigmoid_layer.get_output(0), op=trt.ElementWiseOperation.PROD)\n",
    "\n",
    "    # act_fn(self.gate_proj(x)) * self.up_proj(x)\n",
    "    before_down_proj_layer = network.add_elementwise(selu_mult_layer.get_output(0), up_proj_layer.get_output(0), op=trt.ElementWiseOperation.PROD)\n",
    "\n",
    "    down_proj_weight = model.down_proj.weight.clone().detach().cpu().numpy()\n",
    "    down_proj_weight = np.expand_dims(down_proj_weight, 0)\n",
    "    down_proj_weight_shape = list(down_proj_weight.shape)\n",
    "    down_proj_weight_layer = network.add_constant(shape=down_proj_weight_shape, weights=trt.Weights(down_proj_weight))\n",
    "\n",
    "    down_proj_layer = network.add_matrix_multiply(before_down_proj_layer.get_output(0), trt.MatrixOperation.NONE, down_proj_weight_layer.get_output(0), trt.MatrixOperation.TRANSPOSE)\n",
    "\n",
    "    # output\n",
    "    network.mark_output(down_proj_layer.get_output(0))\n",
    "\n",
    "    engineString = builder.build_serialized_network(network, config)\n",
    "    \n",
    "    return engineString"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f0a53481",
   "metadata": {},
   "outputs": [],
   "source": [
    "trt_engineStr = trt_create(batch_size, hidden_size, intermediate_size, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d22dd1ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def trt_inference(batch_size, hidden_size, engineString, raw_data):\n",
    "#     print(engineString)\n",
    "#     print(\"Runtime\")\n",
    "    logger = trt.Logger(trt.Logger.ERROR)\n",
    "    engine = trt.Runtime(logger).deserialize_cuda_engine(engineString)\n",
    "    context = engine.create_execution_context()\n",
    "\n",
    "    # dynamic shape configure\n",
    "    print(\"Set input shape\", (batch_size, seq_len, hidden_size))\n",
    "    context.set_input_shape(\"inputT0\", (batch_size, seq_len, hidden_size))\n",
    "    context.set_binding_shape(0, (batch_size, seq_len, hidden_size))\n",
    "    origin_inputshape = context.get_binding_shape(0)\n",
    "\n",
    "#     print(\"Set input shape completed\")\n",
    "\n",
    "    data = np.array(raw_data)\n",
    "\n",
    "    _, stream = cudart.cudaStreamCreate()\n",
    "#     print(\"Reshaping\")\n",
    "\n",
    "    inputH0 = np.ascontiguousarray(data.reshape(-1))\n",
    "    outputH0 = np.empty(context.get_binding_shape(1), dtype=trt.nptype(engine.get_binding_dtype(1)))\n",
    "#     print(\"Reshaped\")\n",
    "\n",
    "    # initialize input and output data\n",
    "    _, inputD0 = cudart.cudaMallocAsync(inputH0.nbytes, stream)\n",
    "    _, outputD0 = cudart.cudaMallocAsync(outputH0.nbytes, stream)\n",
    "\n",
    "    # move input to device\n",
    "    cudart.cudaMemcpyAsync(inputD0, inputH0.ctypes.data, inputH0.nbytes, cudart.cudaMemcpyKind.cudaMemcpyHostToDevice, stream)\n",
    "\n",
    "    # execute\n",
    "#     print(\"execute\")\n",
    "    context.execute_async_v2([int(inputD0), int(outputD0)], stream)\n",
    "\n",
    "    # move output back to host\n",
    "    cudart.cudaMemcpyAsync(outputH0.ctypes.data, outputD0, outputH0.nbytes, cudart.cudaMemcpyKind.cudaMemcpyDeviceToHost, stream)\n",
    "\n",
    "    # wait for everythidden_sizeg\n",
    "    cudart.cudaStreamSynchronize(stream)\n",
    "\n",
    "    cudart.cudaStreamDestroy(stream)\n",
    "    cudart.cudaFree(inputD0)\n",
    "    cudart.cudaFree(outputD0)\n",
    "\n",
    "    return outputH0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5a62b009",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Set input shape (4, 8, 4096)\n",
      "output_trt : (4, 8, 4096)\n",
      "[[[ 2.0781107  4.51184    3.0770535 ... -2.6064963 -2.1167407 -2.7058408]\n",
      "  [ 2.0781107  4.51184    3.0770535 ... -2.6064963 -2.1167407 -2.7058408]\n",
      "  [ 2.0781107  4.51184    3.0770535 ... -2.6064963 -2.1167407 -2.7058408]\n",
      "  ...\n",
      "  [ 2.0781107  4.51184    3.0770535 ... -2.6064963 -2.1167407 -2.7058408]\n",
      "  [ 2.0781107  4.51184    3.0770535 ... -2.6064963 -2.1167407 -2.7058408]\n",
      "  [ 2.0781107  4.51184    3.0770535 ... -2.6064963 -2.1167407 -2.7058408]]\n",
      "\n",
      " [[ 2.0781107  4.51184    3.0770535 ... -2.6064963 -2.1167407 -2.7058408]\n",
      "  [ 2.0781107  4.51184    3.0770535 ... -2.6064963 -2.1167407 -2.7058408]\n",
      "  [ 2.0781107  4.51184    3.0770535 ... -2.6064963 -2.1167407 -2.7058408]\n",
      "  ...\n",
      "  [ 2.0781107  4.51184    3.0770535 ... -2.6064963 -2.1167407 -2.7058408]\n",
      "  [ 2.0781107  4.51184    3.0770535 ... -2.6064963 -2.1167407 -2.7058408]\n",
      "  [ 2.0781107  4.51184    3.0770535 ... -2.6064963 -2.1167407 -2.7058408]]\n",
      "\n",
      " [[ 2.0781107  4.51184    3.0770535 ... -2.6064963 -2.1167407 -2.7058408]\n",
      "  [ 2.0781107  4.51184    3.0770535 ... -2.6064963 -2.1167407 -2.7058408]\n",
      "  [ 2.0781107  4.51184    3.0770535 ... -2.6064963 -2.1167407 -2.7058408]\n",
      "  ...\n",
      "  [ 2.0781107  4.51184    3.0770535 ... -2.6064963 -2.1167407 -2.7058408]\n",
      "  [ 2.0781107  4.51184    3.0770535 ... -2.6064963 -2.1167407 -2.7058408]\n",
      "  [ 2.0781107  4.51184    3.0770535 ... -2.6064963 -2.1167407 -2.7058408]]\n",
      "\n",
      " [[ 2.0781107  4.51184    3.0770535 ... -2.6064963 -2.1167407 -2.7058408]\n",
      "  [ 2.0781107  4.51184    3.0770535 ... -2.6064963 -2.1167407 -2.7058408]\n",
      "  [ 2.0781107  4.51184    3.0770535 ... -2.6064963 -2.1167407 -2.7058408]\n",
      "  ...\n",
      "  [ 2.0781107  4.51184    3.0770535 ... -2.6064963 -2.1167407 -2.7058408]\n",
      "  [ 2.0781107  4.51184    3.0770535 ... -2.6064963 -2.1167407 -2.7058408]\n",
      "  [ 2.0781107  4.51184    3.0770535 ... -2.6064963 -2.1167407 -2.7058408]]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1301/776708356.py:11: DeprecationWarning: Use set_input_shape instead.\n",
      "  context.set_binding_shape(0, (batch_size, seq_len, hidden_size))\n",
      "/tmp/ipykernel_1301/776708356.py:12: DeprecationWarning: Use get_tensor_shape instead.\n",
      "  origin_inputshape = context.get_binding_shape(0)\n",
      "/tmp/ipykernel_1301/776708356.py:22: DeprecationWarning: Use get_tensor_shape instead.\n",
      "  outputH0 = np.empty(context.get_binding_shape(1), dtype=trt.nptype(engine.get_binding_dtype(1)))\n",
      "/tmp/ipykernel_1301/776708356.py:22: DeprecationWarning: Use get_tensor_dtype instead.\n",
      "  outputH0 = np.empty(context.get_binding_shape(1), dtype=trt.nptype(engine.get_binding_dtype(1)))\n"
     ]
    }
   ],
   "source": [
    "up_proj_weight = model.up_proj.weight.clone().detach().cpu().numpy()\n",
    "\n",
    "trt_output = trt_inference(batch_size, hidden_size, trt_engineStr, data)\n",
    "\n",
    "# trt_output = trt_output.reshape(batch_size, seq_len, hidden_size)\n",
    "print(\"output_trt :\", trt_output.shape)\n",
    "print(trt_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "930f0cfe",
   "metadata": {},
   "source": [
    "## Benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3b954afc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b056bb4d",
   "metadata": {},
   "source": [
    "### Torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b47186a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch memory exe 0.1318738 ms\n"
     ]
    }
   ],
   "source": [
    "torch_start = time.time_ns()\n",
    "\n",
    "output = model(data_D)\n",
    "\n",
    "torch_complete = time.time_ns()\n",
    "\n",
    "print(\"torch memory exe\", (torch_complete - torch_start) / 10e6, \"ms\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "333825ba",
   "metadata": {},
   "source": [
    "### TensorRT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37d084a7",
   "metadata": {},
   "source": [
    "### profile CPU/GPU time for tensorRT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8e982096",
   "metadata": {},
   "outputs": [],
   "source": [
    "def profile_trt_inference(batch_size, hidden_size, engineString, raw_data, up_proj):\n",
    "    trt_prep_start = time.time_ns()\n",
    "    \n",
    "    logger = trt.Logger(trt.Logger.ERROR)\n",
    "    engine = trt.Runtime(logger).deserialize_cuda_engine(engineString)\n",
    "    context = engine.create_execution_context()\n",
    "\n",
    "    # dynamic shape configure\n",
    "    print(\"Set input shape\", (batch_size, seq_len, hidden_size))\n",
    "    context.set_input_shape(\"inputT0\", (batch_size, seq_len, hidden_size))\n",
    "    context.set_binding_shape(0, (batch_size, seq_len, hidden_size))\n",
    "    origin_inputshape = context.get_binding_shape(0)\n",
    "\n",
    "    trt_prep_complete = time.time_ns()\n",
    "\n",
    "    data = np.array(raw_data)\n",
    "\n",
    "    inputH0 = np.ascontiguousarray(data.reshape(-1))\n",
    "    outputH0 = np.empty(context.get_binding_shape(1), dtype=trt.nptype(engine.get_binding_dtype(1)))\n",
    "\n",
    "    memory_alloc_complete = time.time_ns()\n",
    "\n",
    "    _, stream = cudart.cudaStreamCreate()\n",
    "\n",
    "    # initialize input and output data\n",
    "    _, inputD0 = cudart.cudaMallocAsync(inputH0.nbytes, stream)\n",
    "    _, outputD0 = cudart.cudaMallocAsync(outputH0.nbytes, stream)\n",
    "\n",
    "    # move input to device\n",
    "    cudart.cudaMemcpyAsync(inputD0, inputH0.ctypes.data, inputH0.nbytes, cudart.cudaMemcpyKind.cudaMemcpyHostToDevice, stream)\n",
    "\n",
    "    # execute\n",
    "    context.execute_async_v2([int(inputD0), int(outputD0)], stream)\n",
    "\n",
    "    # move output back to host\n",
    "    cudart.cudaMemcpyAsync(outputH0.ctypes.data, outputD0, outputH0.nbytes, cudart.cudaMemcpyKind.cudaMemcpyDeviceToHost, stream)\n",
    "\n",
    "    # wait for everythidden_sizeg\n",
    "    cudart.cudaStreamSynchronize(stream)\n",
    "\n",
    "    cudart.cudaStreamDestroy(stream)\n",
    "    cudart.cudaFree(inputD0)\n",
    "    cudart.cudaFree(outputD0)\n",
    "\n",
    "    trt_complete = time.time_ns()\n",
    "    \n",
    "    print(\"trt_prep\", (trt_prep_complete - trt_prep_start) / 10e6, \"ms\")\n",
    "    print(\"memory_alloc CPU\", (memory_alloc_complete - trt_prep_complete) / 10e6, \"ms\")\n",
    "    print(\"trt memory alloc & mv & exe\", (trt_complete - memory_alloc_complete) / 10e6, \"ms\")\n",
    "\n",
    "    return outputH0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8964f661",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Set input shape (4, 8, 4096)\n",
      "trt_prep 15.402589 ms\n",
      "memory_alloc CPU 0.0693197 ms\n",
      "trt memory alloc & mv & exe 0.1856379 ms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1301/3463549145.py:11: DeprecationWarning: Use set_input_shape instead.\n",
      "  context.set_binding_shape(0, (batch_size, seq_len, hidden_size))\n",
      "/tmp/ipykernel_1301/3463549145.py:12: DeprecationWarning: Use get_tensor_shape instead.\n",
      "  origin_inputshape = context.get_binding_shape(0)\n",
      "/tmp/ipykernel_1301/3463549145.py:19: DeprecationWarning: Use get_tensor_shape instead.\n",
      "  outputH0 = np.empty(context.get_binding_shape(1), dtype=trt.nptype(engine.get_binding_dtype(1)))\n",
      "/tmp/ipykernel_1301/3463549145.py:19: DeprecationWarning: Use get_tensor_dtype instead.\n",
      "  outputH0 = np.empty(context.get_binding_shape(1), dtype=trt.nptype(engine.get_binding_dtype(1)))\n"
     ]
    }
   ],
   "source": [
    "trt_output = profile_trt_inference(batch_size, hidden_size, trt_engineStr, data, up_proj_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "553d159a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

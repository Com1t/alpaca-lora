{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ff3138c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from cuda import cudart\n",
    "import torch\n",
    "from torch import Tensor, nn\n",
    "import tensorrt as trt\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb81b60f",
   "metadata": {},
   "source": [
    "## Integration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "92f36e57",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = dict()\n",
    "\n",
    "batch_size, seq_len, hidden_size = 4, 45, 4096\n",
    "intermediate_size = 11008\n",
    "num_attention_heads = 32\n",
    "num_key_value_heads = 32\n",
    "max_position_embeddings = 2048\n",
    "rope_theta = 10000.0\n",
    "rms_norm_eps = 1e-6\n",
    "\n",
    "config[\"hidden_size\"] = hidden_size\n",
    "config[\"intermediate_size\"] = intermediate_size\n",
    "config[\"num_heads\"] = num_attention_heads\n",
    "config[\"head_dim\"] = config[\"hidden_size\"] // config[\"num_heads\"]\n",
    "config[\"num_key_value_heads\"] = num_key_value_heads\n",
    "config[\"num_key_value_groups\"] = config[\"num_heads\"] // config[\"num_key_value_heads\"]\n",
    "config[\"max_position_embeddings\"] = max_position_embeddings\n",
    "config[\"rope_theta\"] = rope_theta\n",
    "config[\"rms_norm_eps\"] = rms_norm_eps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bb10eb59",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = torch.ones(batch_size, seq_len, hidden_size)\n",
    "attention_mask = torch.ones(batch_size, 1, seq_len, seq_len)\n",
    "position_ids = torch.arange(0, seq_len)\n",
    "position_ids = position_ids.repeat(batch_size, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e33b57b9",
   "metadata": {},
   "source": [
    "## Llama layernorm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "79363b49",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LlamaRMSNorm(nn.Module):\n",
    "    def __init__(self, hidden_size, eps=1e-6):\n",
    "        \"\"\"\n",
    "        LlamaRMSNorm is equivalent to T5LayerNorm\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.weight = nn.Parameter(torch.ones(hidden_size))\n",
    "        self.variance_epsilon = eps\n",
    "\n",
    "    def load(self, dir, norm_name):\n",
    "        weights = torch.load(dir)\n",
    "        RMSnorm_weight = dict()\n",
    "        for key in weights.keys():\n",
    "            if key.split(\".\")[3] == norm_name:\n",
    "                RMSnorm_weight[key[key.find(key.split(\".\")[4]):]] = weights[key]\n",
    "        self.load_state_dict(RMSnorm_weight)\n",
    "\n",
    "    def forward(self, hidden_states):\n",
    "        input_dtype = hidden_states.dtype\n",
    "        hidden_states = hidden_states.to(torch.float32)\n",
    "        variance = hidden_states.pow(2).mean(-1, keepdim=True)\n",
    "        hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)\n",
    "        return self.weight * hidden_states.to(input_dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0712ce81",
   "metadata": {},
   "source": [
    "## LlamaAttention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "75c9bc52",
   "metadata": {},
   "outputs": [],
   "source": [
    "def repeat_kv(hidden_states: torch.Tensor, n_rep: int) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    This is the equivalent of torch.repeat_interleave(x, dim=1, repeats=n_rep). The hidden states go from (batch,\n",
    "    num_key_value_heads, seqlen, head_dim) to (batch, num_attention_heads, seqlen, head_dim)\n",
    "    \n",
    "    repeat at the second dimension\n",
    "    \"\"\"\n",
    "    batch, num_key_value_heads, slen, head_dim = hidden_states.shape\n",
    "    if n_rep == 1:\n",
    "        return hidden_states\n",
    "    hidden_states = hidden_states[:, :, None, :, :].expand(batch, num_key_value_heads, n_rep, slen, head_dim)\n",
    "    return hidden_states.reshape(batch, num_key_value_heads * n_rep, slen, head_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bf52a637",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LlamaRotaryEmbedding(torch.nn.Module):\n",
    "    def __init__(self, dim, max_position_embeddings=2048, base=10000, device=None):\n",
    "        super().__init__()\n",
    "\n",
    "        self.dim = dim\n",
    "        self.max_position_embeddings = max_position_embeddings\n",
    "        self.base = base\n",
    "        inv_freq = 1.0 / (self.base ** (torch.arange(0, self.dim, 2).float().to(device) / self.dim))\n",
    "        \n",
    "        self.register_buffer(\"inv_freq\", inv_freq, persistent=False)\n",
    "\n",
    "        t = torch.arange(max_position_embeddings, device=device, dtype=self.inv_freq.dtype)\n",
    "\n",
    "        freqs = torch.einsum(\"i,j->ij\", t, self.inv_freq)\n",
    "        # Different from paper, but it uses a different permutation in order to obtain the same calculation\n",
    "        emb = torch.cat((freqs, freqs), dim=-1)\n",
    "        self.register_buffer(\"cos_cached\", emb.cos()[None, None, :, :].to(torch.get_default_dtype()), persistent=False)\n",
    "        self.register_buffer(\"sin_cached\", emb.sin()[None, None, :, :].to(torch.get_default_dtype()), persistent=False)\n",
    "\n",
    "    def rotate_half(self, x):\n",
    "        \"\"\"Rotates half the hidden dims of the input.\"\"\"\n",
    "        x1 = x[..., : x.shape[-1] // 2]\n",
    "        x2 = x[..., x.shape[-1] // 2 :]\n",
    "        return torch.cat((-x2, x1), dim=-1)\n",
    "\n",
    "    def forward(self, q, k, v, position_ids, seq_len=None):\n",
    "        # v: [bs, num_attention_heads, seq_len, head_size]\n",
    "        cos = self.cos_cached[:, :, :seq_len, ...].to(dtype=v.dtype)\n",
    "        sin = self.sin_cached[:, :, :seq_len, ...].to(dtype=v.dtype)\n",
    "        cos = cos.squeeze(1).squeeze(0)  # [seq_len, dim]\n",
    "        sin = sin.squeeze(1).squeeze(0)  # [seq_len, dim]\n",
    "        cos = cos[position_ids].unsqueeze(1)  # [bs, 1, seq_len, dim]\n",
    "        sin = sin[position_ids].unsqueeze(1)  # [bs, 1, seq_len, dim]\n",
    "\n",
    "        # The first two dimensions of cos and sin are always 1, so we can `squeeze` them.\n",
    "        q_embed = (q * cos) + (self.rotate_half(q) * sin)\n",
    "        k_embed = (k * cos) + (self.rotate_half(k) * sin)\n",
    "        return q_embed, k_embed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7627e6e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LlamaAttention(nn.Module):\n",
    "    \"\"\"Multi-headed attention from 'Attention Is All You Need' paper\"\"\"\n",
    "\n",
    "    def __init__(self, config: dict):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.hidden_size = config[\"hidden_size\"]\n",
    "        self.num_heads = config[\"num_heads\"]\n",
    "        self.head_dim = config[\"hidden_size\"] // config[\"num_heads\"]\n",
    "        self.num_key_value_heads = config[\"num_key_value_heads\"]\n",
    "        self.num_key_value_groups = config[\"num_heads\"] // config[\"num_key_value_heads\"]\n",
    "        self.max_position_embeddings = config[\"max_position_embeddings\"]\n",
    "        self.rope_theta = config[\"rope_theta\"]\n",
    "\n",
    "        if (self.head_dim * self.num_heads) != self.hidden_size:\n",
    "            raise ValueError(\n",
    "                f\"hidden_size must be divisible by num_heads (got `hidden_size`: {self.hidden_size}\"\n",
    "                f\" and `num_heads`: {self.num_heads}).\"\n",
    "            )\n",
    "        self.q_proj = nn.Linear(self.hidden_size, self.num_heads * self.head_dim, bias=False)\n",
    "        self.k_proj = nn.Linear(self.hidden_size, self.num_key_value_heads * self.head_dim, bias=False)\n",
    "        self.v_proj = nn.Linear(self.hidden_size, self.num_key_value_heads * self.head_dim, bias=False)\n",
    "        self.o_proj = nn.Linear(self.num_heads * self.head_dim, self.hidden_size, bias=False)\n",
    "        self._init_rope()\n",
    "\n",
    "    def _init_rope(self):\n",
    "        print(\n",
    "            \"init rope\",\n",
    "            self.head_dim,\n",
    "            self.max_position_embeddings,\n",
    "            self.rope_theta,\n",
    "        )\n",
    "        self.rotary_emb = LlamaRotaryEmbedding(\n",
    "            self.head_dim,\n",
    "            max_position_embeddings=self.max_position_embeddings,\n",
    "            base=self.rope_theta,\n",
    "        )\n",
    "\n",
    "    def _shape(self, tensor: torch.Tensor, seq_len: int, bsz: int):\n",
    "        return tensor.view(bsz, seq_len, self.num_heads, self.head_dim).transpose(1, 2).contiguous()\n",
    "\n",
    "\n",
    "    def load(self, dir):\n",
    "        weights = torch.load(dir)\n",
    "        self_attn_weights = dict()\n",
    "        for key in weights.keys():\n",
    "            print(key)\n",
    "            if key == \"model.layers.18.self_attn.rotary_emb.inv_freq\":\n",
    "                print(weights[key])\n",
    "                continue\n",
    "            if key.split(\".\")[3] == \"self_attn\":\n",
    "                self_attn_weights[key[key.find(key.split(\".\")[4]):]] = weights[key]\n",
    "        self.load_state_dict(self_attn_weights)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        hidden_states: torch.Tensor,\n",
    "        attention_mask: None,\n",
    "        position_ids: None,\n",
    "        past_key_value: None,\n",
    "        output_attentions: bool = False,\n",
    "        use_cache: bool = False,\n",
    "    ):\n",
    "        # bsz = batch size; q_len = query length; _ = hidden size\n",
    "        bsz, q_len, _ = hidden_states.size()\n",
    "\n",
    "        # do projection\n",
    "        query_states = self.q_proj(hidden_states)\n",
    "        key_states = self.k_proj(hidden_states)\n",
    "        value_states = self.v_proj(hidden_states)\n",
    "\n",
    "        # reshape\n",
    "        query_states = query_states.view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        key_states = key_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n",
    "        value_states = value_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n",
    "\n",
    "#         #####################################################\n",
    "#         # in hugging face, they do have kv cache, however, they don't have other attention optimization\n",
    "#         # this could be done directly in tensorRT by using dynamic shape\n",
    "#         kv_seq_len = key_states.shape[-2]\n",
    "#         if past_key_value is not None:\n",
    "#             kv_seq_len += past_key_value[0].shape[-2]\n",
    "\n",
    "#         query_states, key_states = self.rotary_emb(query_states, key_states, value_states, position_ids, seq_len=q_len)\n",
    "\n",
    "#         if past_key_value is not None:\n",
    "#             # reuse k, v, self_attention\n",
    "#             key_states = torch.cat([past_key_value[0], key_states], dim=2)\n",
    "#             value_states = torch.cat([past_key_value[1], value_states], dim=2)\n",
    "\n",
    "#         past_key_value = (key_states, value_states) if use_cache else None\n",
    "\n",
    "#         print(self.num_key_value_groups)\n",
    "#         # repeat k/v heads if n_kv_heads < n_heads\n",
    "#         key_states = repeat_kv(key_states, self.num_key_value_groups)\n",
    "#         value_states = repeat_kv(value_states, self.num_key_value_groups)\n",
    "#         #####################################################\n",
    "\n",
    "        attn_weights = torch.matmul(query_states, key_states.transpose(2, 3)) / math.sqrt(self.head_dim)\n",
    "\n",
    "        # attention_mask needs to be infered\n",
    "        attn_weights = attn_weights + attention_mask\n",
    "\n",
    "        # upcast attention to fp32\n",
    "        attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query_states.dtype)\n",
    "\n",
    "        attn_output = torch.matmul(attn_weights, value_states)\n",
    "\n",
    "        attn_output = attn_output.transpose(1, 2) # .contiguous()\n",
    "        attn_output = attn_output.reshape(bsz, q_len, self.hidden_size)\n",
    "\n",
    "        attn_output = self.o_proj(attn_output)\n",
    "\n",
    "        return attn_output, attn_weights, None\n",
    "\n",
    "        # since normally it will be false\n",
    "#         if not output_attentions:\n",
    "#             attn_weights = None\n",
    "\n",
    "#         return attn_output, attn_weights, past_key_value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4ebb897",
   "metadata": {},
   "source": [
    "## Llama MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4b20cf20",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SiLUActivation(nn.Module):\n",
    "    \"\"\"\n",
    "    See Gaussian Error Linear Units (Hendrycks et al., https://arxiv.org/abs/1606.08415) where the SiLU (Sigmoid Linear\n",
    "    Unit) was originally introduced and coined, and see Sigmoid-Weighted Linear Units for Neural Network Function\n",
    "    Approximation in Reinforcement Learbatch_sizeg (Elfwing et al., https://arxiv.org/abs/1702.03118) and Swish: a Self-Gated\n",
    "    Activation Function (Ramachandran et al., https://arxiv.org/abs/1710.05941v1) where the SiLU was experimented with\n",
    "    later.\n",
    "    \"\"\"\n",
    "\n",
    "    def forward(self, input: Tensor) -> Tensor:\n",
    "        return nn.functional.silu(input)\n",
    "    \n",
    "    def b_forward(self, input: Tensor) -> Tensor:\n",
    "        return torch.matmul(input.T, nn.functional.sigmoid(input))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "768f90f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LlamaMLP(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.hidden_size = config['hidden_size']\n",
    "        self.intermediate_size = config['intermediate_size']\n",
    "        self.gate_proj = nn.Linear(self.hidden_size, self.intermediate_size, bias=False)\n",
    "        self.up_proj = nn.Linear(self.hidden_size, self.intermediate_size, bias=False)\n",
    "        self.down_proj = nn.Linear(self.intermediate_size, self.hidden_size, bias=False)\n",
    "        self.act_fn = SiLUActivation()\n",
    "        self.init = False\n",
    "\n",
    "    def load(self, dir):\n",
    "        weights = torch.load(dir)\n",
    "        mlp_weights = dict()\n",
    "        for key in weights.keys():\n",
    "            if key.split(\".\")[3] == \"mlp\":\n",
    "                mlp_weights[key[key.find(key.split(\".\")[4]):]] = weights[key]\n",
    "        self.load_state_dict(mlp_weights)\n",
    "\n",
    "    def forward(self, x):\n",
    "        down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\n",
    "\n",
    "        return down_proj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "af62f54b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LlamaDecoderLayer(nn.Module):\n",
    "    def __init__(self, config: dict):\n",
    "        super().__init__()\n",
    "        # if config.tensorRT:\n",
    "        #     exit()\n",
    "        self.hidden_size = config['hidden_size']\n",
    "        \n",
    "        self.num_heads = config[\"num_heads\"]\n",
    "        self.head_dim = config[\"hidden_size\"] // config[\"num_heads\"]\n",
    "        self.num_key_value_heads = config[\"num_key_value_heads\"]\n",
    "        self.num_key_value_groups = config[\"num_heads\"] // config[\"num_key_value_heads\"]\n",
    "        self.max_position_embeddings = config[\"max_position_embeddings\"]\n",
    "        self.rope_theta = config[\"rope_theta\"]\n",
    "        \n",
    "        self.self_attn = LlamaAttention(config=config)\n",
    "        self.mlp = LlamaMLP(config)\n",
    "        self.input_layernorm = LlamaRMSNorm(config['hidden_size'], eps=config['rms_norm_eps'])\n",
    "        self.post_attention_layernorm = LlamaRMSNorm(config['hidden_size'], eps=config['rms_norm_eps'])\n",
    "\n",
    "\n",
    "    def load(self, dir):\n",
    "        self.self_attn.load(dir)\n",
    "        self.mlp.load(dir)\n",
    "        self.input_layernorm.load(dir, \"input_layernorm\")\n",
    "        self.post_attention_layernorm.load(dir, \"post_attention_layernorm\")\n",
    "\n",
    "    def forward(\n",
    "        self,        \n",
    "        hidden_states: torch.Tensor,\n",
    "        attention_mask: None,\n",
    "        position_ids: None,\n",
    "        past_key_value: None,\n",
    "        output_attentions: bool = False,\n",
    "        use_cache: bool = False,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            hidden_states (`torch.FloatTensor`): input to the layer of shape `(batch, seq_len, embed_dim)`\n",
    "            attention_mask (`torch.FloatTensor`, *optional*): attention mask of size\n",
    "                `(batch, 1, tgt_len, src_len)` where padding elements are indicated by very large negative values.\n",
    "            output_attentions (`bool`, *optional*):\n",
    "                Whether or not to return the attentions tensors of all attention layers. See `attentions` under\n",
    "                returned tensors for more detail.\n",
    "            use_cache (`bool`, *optional*):\n",
    "                If set to `True`, `past_key_values` key value states are returned and can be used to speed up decoding\n",
    "                (see `past_key_values`).\n",
    "            past_key_value (`Tuple(torch.FloatTensor)`, *optional*): cached past key and value projection states\n",
    "        \"\"\"\n",
    "\n",
    "        residual = hidden_states\n",
    "\n",
    "        hidden_states = self.input_layernorm(hidden_states)\n",
    "\n",
    "        # Self Attention\n",
    "        hidden_states, self_attn_weights, present_key_value = self.self_attn(\n",
    "            hidden_states=hidden_states,\n",
    "            attention_mask=attention_mask,\n",
    "            position_ids=position_ids,\n",
    "            past_key_value=past_key_value,\n",
    "            output_attentions=output_attentions,\n",
    "            use_cache=use_cache,\n",
    "        )\n",
    "\n",
    "#         if past_key_value is not None:\n",
    "#             for value in past_key_value:\n",
    "#                 print(f\"past_key_value={value.shape}\")\n",
    "\n",
    "#         hidden_states = residual + hidden_states\n",
    "\n",
    "#         # Fully Connected\n",
    "#         residual = hidden_states\n",
    "#         hidden_states = self.post_attention_layernorm(hidden_states)\n",
    "#         hidden_states = self.mlp(hidden_states)\n",
    "#         hidden_states = residual + hidden_states\n",
    "\n",
    "        outputs = (hidden_states,)\n",
    "\n",
    "#         if output_attentions:\n",
    "#             outputs += (self_attn_weights,)\n",
    "\n",
    "#         if use_cache:\n",
    "#             outputs += (present_key_value,)\n",
    "\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ff65b35",
   "metadata": {},
   "source": [
    "## Test torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "574ae2be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "init rope 128 2048 10000.0\n",
      "model.layers.18.self_attn.q_proj.weight\n",
      "model.layers.18.self_attn.k_proj.weight\n",
      "model.layers.18.self_attn.v_proj.weight\n",
      "model.layers.18.self_attn.o_proj.weight\n",
      "model.layers.18.mlp.gate_proj.weight\n",
      "model.layers.18.mlp.down_proj.weight\n",
      "model.layers.18.mlp.up_proj.weight\n",
      "model.layers.18.input_layernorm.weight\n",
      "model.layers.18.post_attention_layernorm.weight\n",
      "model.layers.18.self_attn.rotary_emb.inv_freq\n",
      "tensor([1.0000e+00, 8.6596e-01, 7.4989e-01, 6.4938e-01, 5.6234e-01, 4.8697e-01,\n",
      "        4.2170e-01, 3.6517e-01, 3.1623e-01, 2.7384e-01, 2.3714e-01, 2.0535e-01,\n",
      "        1.7783e-01, 1.5399e-01, 1.3335e-01, 1.1548e-01, 1.0000e-01, 8.6596e-02,\n",
      "        7.4989e-02, 6.4938e-02, 5.6234e-02, 4.8697e-02, 4.2170e-02, 3.6517e-02,\n",
      "        3.1623e-02, 2.7384e-02, 2.3714e-02, 2.0535e-02, 1.7783e-02, 1.5399e-02,\n",
      "        1.3335e-02, 1.1548e-02, 1.0000e-02, 8.6596e-03, 7.4989e-03, 6.4938e-03,\n",
      "        5.6234e-03, 4.8697e-03, 4.2170e-03, 3.6517e-03, 3.1623e-03, 2.7384e-03,\n",
      "        2.3714e-03, 2.0535e-03, 1.7783e-03, 1.5399e-03, 1.3335e-03, 1.1548e-03,\n",
      "        1.0000e-03, 8.6596e-04, 7.4989e-04, 6.4938e-04, 5.6234e-04, 4.8697e-04,\n",
      "        4.2170e-04, 3.6517e-04, 3.1623e-04, 2.7384e-04, 2.3714e-04, 2.0535e-04,\n",
      "        1.7783e-04, 1.5399e-04, 1.3335e-04, 1.1548e-04])\n"
     ]
    }
   ],
   "source": [
    "model = LlamaDecoderLayer(config)\n",
    "\n",
    "device = torch.device(\"cuda\")\n",
    "\n",
    "model.load(\"/home/fuchiang137/.cache/huggingface/hub/models--decapoda-research--llama-7b-hf/snapshots/5f98eefcc80e437ef68d457ad7bf167c2c6a1348/pytorch_model-00019-of-00033.bin\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9bc0bf9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (rotary_emb): LlamaRotaryEmbedding()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "    (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "    (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
      "    (act_fn): SiLUActivation()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm()\n",
      "  (post_attention_layernorm): LlamaRMSNorm()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e184880a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([[[-0.1110, -0.5344,  1.2985,  ..., -0.5545, -0.2015,  0.6036],\n",
      "         [-0.1110, -0.5344,  1.2985,  ..., -0.5545, -0.2015,  0.6036],\n",
      "         [-0.1110, -0.5344,  1.2985,  ..., -0.5545, -0.2015,  0.6036],\n",
      "         ...,\n",
      "         [-0.1110, -0.5344,  1.2985,  ..., -0.5545, -0.2015,  0.6036],\n",
      "         [-0.1110, -0.5344,  1.2985,  ..., -0.5545, -0.2015,  0.6036],\n",
      "         [-0.1110, -0.5344,  1.2985,  ..., -0.5545, -0.2015,  0.6036]],\n",
      "\n",
      "        [[-0.1110, -0.5344,  1.2985,  ..., -0.5545, -0.2015,  0.6036],\n",
      "         [-0.1110, -0.5344,  1.2985,  ..., -0.5545, -0.2015,  0.6036],\n",
      "         [-0.1110, -0.5344,  1.2985,  ..., -0.5545, -0.2015,  0.6036],\n",
      "         ...,\n",
      "         [-0.1110, -0.5344,  1.2985,  ..., -0.5545, -0.2015,  0.6036],\n",
      "         [-0.1110, -0.5344,  1.2985,  ..., -0.5545, -0.2015,  0.6036],\n",
      "         [-0.1110, -0.5344,  1.2985,  ..., -0.5545, -0.2015,  0.6036]],\n",
      "\n",
      "        [[-0.1110, -0.5344,  1.2985,  ..., -0.5545, -0.2015,  0.6036],\n",
      "         [-0.1110, -0.5344,  1.2985,  ..., -0.5545, -0.2015,  0.6036],\n",
      "         [-0.1110, -0.5344,  1.2985,  ..., -0.5545, -0.2015,  0.6036],\n",
      "         ...,\n",
      "         [-0.1110, -0.5344,  1.2985,  ..., -0.5545, -0.2015,  0.6036],\n",
      "         [-0.1110, -0.5344,  1.2985,  ..., -0.5545, -0.2015,  0.6036],\n",
      "         [-0.1110, -0.5344,  1.2985,  ..., -0.5545, -0.2015,  0.6036]],\n",
      "\n",
      "        [[-0.1110, -0.5344,  1.2985,  ..., -0.5545, -0.2015,  0.6036],\n",
      "         [-0.1110, -0.5344,  1.2985,  ..., -0.5545, -0.2015,  0.6036],\n",
      "         [-0.1110, -0.5344,  1.2985,  ..., -0.5545, -0.2015,  0.6036],\n",
      "         ...,\n",
      "         [-0.1110, -0.5344,  1.2985,  ..., -0.5545, -0.2015,  0.6036],\n",
      "         [-0.1110, -0.5344,  1.2985,  ..., -0.5545, -0.2015,  0.6036],\n",
      "         [-0.1110, -0.5344,  1.2985,  ..., -0.5545, -0.2015,  0.6036]]],\n",
      "       device='cuda:0', grad_fn=<UnsafeViewBackward0>),)\n"
     ]
    }
   ],
   "source": [
    "model = model.to(device)\n",
    "\n",
    "data_D = data.to(device)\n",
    "attention_mask_D = attention_mask.to(device)\n",
    "position_ids_D = position_ids.to(device)\n",
    "# output = model(data)\n",
    "\n",
    "past_key_value = None\n",
    "\n",
    "# attentiona mask\n",
    "# position_ids\n",
    "# specifies the position id of the corresponding hidden state tensor element\n",
    "# e.g. hid = [3, 4, 6] => pos_id = [0, 1, 2]\n",
    "# past_key_value\n",
    "# if use cache, past key value will contain past kv values\n",
    "output = model(hidden_states=data_D,\n",
    "               attention_mask=attention_mask_D,\n",
    "               position_ids=position_ids_D,\n",
    "               past_key_value=past_key_value,\n",
    "               output_attentions=False,\n",
    "               use_cache=True,)\n",
    "\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a6783df",
   "metadata": {},
   "source": [
    "## tensorRT attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9a428eaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# seq length is not specified, since it is a dynamic size\n",
    "def trt_create(batch_size, hidden_size, intermediate_size, model):\n",
    "    \n",
    "    logger = trt.Logger(trt.Logger.ERROR)\n",
    "    builder = trt.Builder(logger)\n",
    "\n",
    "    network = builder.create_network(1 << int(trt.NetworkDefinitionCreationFlag.EXPLICIT_BATCH))\n",
    "    config = builder.create_builder_config()\n",
    "\n",
    "    # input\n",
    "    hidden_states = network.add_input('hidden_states', trt.DataType.FLOAT, (batch_size, -1, hidden_size))\n",
    "    attention_mask = network.add_input('attention_mask', trt.DataType.FLOAT, (batch_size, 1, -1, -1))\n",
    "\n",
    "    # dynamic shape optimization\n",
    "    profile = builder.create_optimization_profile();\n",
    "    profile.set_shape(\"hidden_states\", (batch_size, 1, hidden_size), (batch_size, 1, hidden_size), (batch_size, 45, hidden_size))\n",
    "    profile.set_shape(\"attention_mask\", (batch_size, 1, 1, 1), (batch_size, 1, 1, 1), (batch_size, 1, 45, 45))\n",
    "    config.add_optimization_profile(profile)\n",
    "\n",
    "    ############################# input layer norm begins #############################\n",
    "    # RMSNorm Layer: 1) Square: X^2 -> 2) Sum: sum of all x^2 -> 3) Mean: 1/N -> 4) Root: sqrt(X) -> 5) Division: 1/X\n",
    "    # 1) Square: X^2\n",
    "    power_layer = network.add_elementwise(hidden_states, hidden_states, op=trt.ElementWiseOperation.PROD)\n",
    "\n",
    "    # 2) Sum: sum of all X^2\n",
    "    sum_layer = network.add_reduce(power_layer.get_output(0), op=trt.ReduceOperation.SUM, axes=1, keep_dims=True)\n",
    "\n",
    "    # 3) Mean: 1/N\n",
    "    mean_layer = network.add_reduce(sum_layer.get_output(0), op=trt.ReduceOperation.AVG, axes=7, keep_dims=True)\n",
    "\n",
    "    # 4) Root: sqrt(X)\n",
    "    sqrt_layer = network.add_unary(mean_layer.get_output(0), op=trt.UnaryOperation.SQRT)\n",
    "\n",
    "    # 5) Division: 1/X\n",
    "    div_layer = network.add_elementwise(hidden_states, sqrt_layer.get_output(0), op=trt.ElementWiseOperation.DIV)\n",
    "    \n",
    "    # 6) times weight\n",
    "    rms_weight = model.input_layernorm.weight.clone().detach().cpu().numpy()\n",
    "    rms_weight = np.expand_dims(rms_weight, 0)\n",
    "    rms_weight = np.expand_dims(rms_weight, 0)\n",
    "    rms_weight_shape = list(rms_weight.shape)\n",
    "    rms_weight_layer = network.add_constant(shape=rms_weight_shape, weights=trt.Weights(rms_weight))\n",
    "\n",
    "    rms_final_layer = network.add_elementwise(hidden_states,\n",
    "                                              rms_weight_layer.get_output(0),\n",
    "                                              op=trt.ElementWiseOperation.PROD)\n",
    "    ############################# input layer norm ends #############################\n",
    "\n",
    "    ############################# self_attention begins #############################\n",
    "    # self.q_proj(hidden_states)\n",
    "    q_proj_weight = model.self_attn.q_proj.weight.clone().detach().cpu().numpy()\n",
    "    q_proj_weight = np.expand_dims(q_proj_weight, 0)\n",
    "    q_proj_weight_shape = list(q_proj_weight.shape)\n",
    "    q_proj_weight_layer = network.add_constant(shape=q_proj_weight_shape, weights=trt.Weights(q_proj_weight))\n",
    "\n",
    "    q_proj_layer = network.add_matrix_multiply(rms_final_layer.get_output(0),\n",
    "                                               trt.MatrixOperation.NONE,\n",
    "                                               q_proj_weight_layer.get_output(0),\n",
    "                                               trt.MatrixOperation.TRANSPOSE)\n",
    "\n",
    "    # query_states = query_states.view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "    q_proj_shuffle_layer = network.add_shuffle(q_proj_layer.get_output(0))\n",
    "    q_proj_shuffle_layer.reshape_dims = trt.Dims([batch_size, -1, model.num_heads, model.head_dim])\n",
    "    q_proj_shuffle_layer.second_transpose = trt.Permutation([0, 2, 1, 3])\n",
    "\n",
    "    \n",
    "    # self.k_proj(hidden_states)\n",
    "    k_proj_weight = model.self_attn.k_proj.weight.clone().detach().cpu().numpy()\n",
    "    k_proj_weight = np.expand_dims(k_proj_weight, 0)\n",
    "    k_proj_weight_shape = list(k_proj_weight.shape)\n",
    "    k_proj_weight_layer = network.add_constant(shape=k_proj_weight_shape, weights=trt.Weights(k_proj_weight))\n",
    "\n",
    "    k_proj_layer = network.add_matrix_multiply(rms_final_layer.get_output(0),\n",
    "                                               trt.MatrixOperation.NONE,\n",
    "                                               k_proj_weight_layer.get_output(0),\n",
    "                                               trt.MatrixOperation.TRANSPOSE)\n",
    "\n",
    "    # key_states = key_states.view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "    k_proj_shuffle_layer = network.add_shuffle(k_proj_layer.get_output(0))\n",
    "    k_proj_shuffle_layer.reshape_dims = trt.Dims([batch_size, -1, model.num_heads, model.head_dim])\n",
    "    k_proj_shuffle_layer.second_transpose = trt.Permutation([0, 2, 3, 1])\n",
    "\n",
    "\n",
    "    # self.v_proj(hidden_states)\n",
    "    v_proj_weight = model.self_attn.v_proj.weight.clone().detach().cpu().numpy()\n",
    "    v_proj_weight = np.expand_dims(v_proj_weight, 0)\n",
    "    v_proj_weight_shape = list(v_proj_weight.shape)\n",
    "    v_proj_weight_layer = network.add_constant(shape=v_proj_weight_shape, weights=trt.Weights(v_proj_weight))\n",
    "\n",
    "    v_proj_layer = network.add_matrix_multiply(rms_final_layer.get_output(0),\n",
    "                                               trt.MatrixOperation.NONE,\n",
    "                                               v_proj_weight_layer.get_output(0),\n",
    "                                               trt.MatrixOperation.TRANSPOSE)\n",
    "\n",
    "    # value_states = value_states.view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "    v_proj_shuffle_layer = network.add_shuffle(v_proj_layer.get_output(0))\n",
    "    v_proj_shuffle_layer.reshape_dims = trt.Dims([batch_size, -1, model.num_heads, model.head_dim])\n",
    "    v_proj_shuffle_layer.second_transpose = trt.Permutation([0, 2, 1, 3])\n",
    "\n",
    "    # attn_weights = torch.matmul(query_states, key_states.transpose(2, 3)) / math.sqrt(self.head_dim)\n",
    "    attn_weights_mult_layer = network.add_matrix_multiply(q_proj_shuffle_layer.get_output(0),\n",
    "                                                          trt.MatrixOperation.NONE,\n",
    "                                                          k_proj_shuffle_layer.get_output(0),\n",
    "                                                          trt.MatrixOperation.NONE)\n",
    "\n",
    "    sqrt_head_dim = np.array([1 / math.sqrt(model.head_dim)], np.float32).reshape(-1)\n",
    "    attn_weights_layer = network.add_scale(attn_weights_mult_layer.get_output(0),\n",
    "                                           trt.ScaleMode.UNIFORM,\n",
    "                                           scale=sqrt_head_dim)\n",
    "\n",
    "    # attn_weights = attn_weights + attention_mask\n",
    "    attn_mix_mask = network.add_elementwise(attn_weights_layer.get_output(0),\n",
    "                                            attention_mask,\n",
    "                                            op=trt.ElementWiseOperation.SUM)\n",
    "\n",
    "    # attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query_states.dtype)\n",
    "    softmax_layer = network.add_softmax(attn_mix_mask.get_output(0))\n",
    "    softmax_layer.axes = 1 << 3\n",
    "\n",
    "    # attn_output = torch.matmul(attn_weights, value_states)\n",
    "    attn_output_layer = network.add_matrix_multiply(softmax_layer.get_output(0),\n",
    "                                                    trt.MatrixOperation.NONE,\n",
    "                                                    v_proj_shuffle_layer.get_output(0),\n",
    "                                                    trt.MatrixOperation.NONE)\n",
    "\n",
    "    # attn_output = attn_output.transpose(1, 2).contiguous()\n",
    "    # attn_output = attn_output.reshape(bsz, q_len, self.hidden_size)\n",
    "    attn_output_shuffle_layer = network.add_shuffle(attn_output_layer.get_output(0))\n",
    "    attn_output_shuffle_layer.first_transpose = trt.Permutation([0, 2, 1, 3])\n",
    "    attn_output_shuffle_layer.reshape_dims = trt.Dims([batch_size, -1, hidden_size])\n",
    "\n",
    "    # attn_output = self.o_proj(attn_output)\n",
    "    o_proj_weight = model.self_attn.o_proj.weight.clone().detach().cpu().numpy()\n",
    "    o_proj_weight = np.expand_dims(o_proj_weight, 0)\n",
    "    o_proj_weight_shape = list(o_proj_weight.shape)\n",
    "    o_proj_weight_layer = network.add_constant(shape=o_proj_weight_shape, weights=trt.Weights(o_proj_weight))\n",
    "    \n",
    "    o_proj_layer = network.add_matrix_multiply(attn_output_shuffle_layer.get_output(0),\n",
    "                                               trt.MatrixOperation.NONE,\n",
    "                                               o_proj_weight_layer.get_output(0),\n",
    "                                               trt.MatrixOperation.TRANSPOSE)\n",
    "    ############################# self_attention ends #############################\n",
    "\n",
    "    # output\n",
    "    # the order of output will be related to the order of the tensor creation\n",
    "    network.mark_output(softmax_layer.get_output(0))\n",
    "    network.mark_output(o_proj_layer.get_output(0))\n",
    "    network.mark_output(v_proj_shuffle_layer.get_output(0))\n",
    "\n",
    "    engineString = builder.build_serialized_network(network, config)\n",
    "    \n",
    "    return engineString"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bcd72471",
   "metadata": {},
   "outputs": [],
   "source": [
    "trt_engineStr = trt_create(batch_size, hidden_size, intermediate_size, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fe6d3522",
   "metadata": {},
   "outputs": [],
   "source": [
    "def trt_inference(batch_size, hidden_size, engineString, raw_data, raw_attn_mask):\n",
    "#     print(engineString)\n",
    "#     print(\"Runtime\")\n",
    "    logger = trt.Logger(trt.Logger.ERROR)\n",
    "    engine = trt.Runtime(logger).deserialize_cuda_engine(engineString)\n",
    "    context = engine.create_execution_context()\n",
    "\n",
    "    # dynamic shape configure\n",
    "    print(\"Set input shape\", (batch_size, seq_len, hidden_size))\n",
    "\n",
    "    context.set_input_shape(\"hidden_states\", (batch_size, seq_len, hidden_size))\n",
    "    context.set_binding_shape(0, (batch_size, seq_len, hidden_size))\n",
    "\n",
    "    context.set_input_shape(\"attention_mask\", (batch_size, 1, seq_len, seq_len))\n",
    "    context.set_binding_shape(1, (batch_size, 1, seq_len, seq_len))\n",
    "    print(\"Set input shape completed\")\n",
    "\n",
    "    data = np.array(raw_data)\n",
    "    attention_mask = np.array(raw_attn_mask)\n",
    "\n",
    "    _, stream = cudart.cudaStreamCreate()\n",
    "#     print(\"Reshaping\")\n",
    "\n",
    "    inputH0 = np.ascontiguousarray(data.reshape(-1))\n",
    "    inputH1 = np.ascontiguousarray(attention_mask.reshape(-1))\n",
    "    outputH0 = np.empty(context.get_binding_shape(2), dtype=trt.nptype(engine.get_binding_dtype(2)))\n",
    "    outputH1 = np.empty(context.get_binding_shape(3), dtype=trt.nptype(engine.get_binding_dtype(3)))\n",
    "    outputH2 = np.empty(context.get_binding_shape(4), dtype=trt.nptype(engine.get_binding_dtype(4)))\n",
    "#     print(\"Reshaped\")\n",
    "\n",
    "    # initialize input and output data\n",
    "    _, inputD0 = cudart.cudaMallocAsync(inputH0.nbytes, stream)\n",
    "    _, inputD1 = cudart.cudaMallocAsync(inputH1.nbytes, stream)\n",
    "    _, outputD0 = cudart.cudaMallocAsync(outputH0.nbytes, stream)\n",
    "    _, outputD1 = cudart.cudaMallocAsync(outputH1.nbytes, stream)\n",
    "    _, outputD2 = cudart.cudaMallocAsync(outputH2.nbytes, stream)\n",
    "\n",
    "    # move input to device\n",
    "    cudart.cudaMemcpyAsync(inputD0, inputH0.ctypes.data, inputH0.nbytes, cudart.cudaMemcpyKind.cudaMemcpyHostToDevice, stream)\n",
    "    cudart.cudaMemcpyAsync(inputD1, inputH1.ctypes.data, inputH1.nbytes, cudart.cudaMemcpyKind.cudaMemcpyHostToDevice, stream)\n",
    "\n",
    "    # execute\n",
    "#     print(\"execute\")\n",
    "    context.execute_async_v2([int(inputD0), int(inputD1), int(outputD0), int(outputD1), int(outputD2)], stream)\n",
    "\n",
    "    # move output back to host\n",
    "    cudart.cudaMemcpyAsync(outputH0.ctypes.data, outputD0, outputH0.nbytes, cudart.cudaMemcpyKind.cudaMemcpyDeviceToHost, stream)\n",
    "    cudart.cudaMemcpyAsync(outputH1.ctypes.data, outputD1, outputH1.nbytes, cudart.cudaMemcpyKind.cudaMemcpyDeviceToHost, stream)\n",
    "    cudart.cudaMemcpyAsync(outputH2.ctypes.data, outputD2, outputH2.nbytes, cudart.cudaMemcpyKind.cudaMemcpyDeviceToHost, stream)\n",
    "\n",
    "    # wait for everythidden_sizeg\n",
    "    cudart.cudaStreamSynchronize(stream)\n",
    "\n",
    "    cudart.cudaStreamDestroy(stream)\n",
    "    cudart.cudaFree(inputD0)\n",
    "    cudart.cudaFree(outputD0)\n",
    "    cudart.cudaFree(outputD1)\n",
    "    cudart.cudaFree(outputD2)\n",
    "\n",
    "    return outputH0, outputH1, outputH2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c8f831bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Set input shape (4, 45, 4096)\n",
      "Set input shape completed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1696/2974348848.py:12: DeprecationWarning: Use set_input_shape instead.\n",
      "  context.set_binding_shape(0, (batch_size, seq_len, hidden_size))\n",
      "/tmp/ipykernel_1696/2974348848.py:15: DeprecationWarning: Use set_input_shape instead.\n",
      "  context.set_binding_shape(1, (batch_size, 1, seq_len, seq_len))\n",
      "/tmp/ipykernel_1696/2974348848.py:26: DeprecationWarning: Use get_tensor_shape instead.\n",
      "  outputH0 = np.empty(context.get_binding_shape(2), dtype=trt.nptype(engine.get_binding_dtype(2)))\n",
      "/tmp/ipykernel_1696/2974348848.py:26: DeprecationWarning: Use get_tensor_dtype instead.\n",
      "  outputH0 = np.empty(context.get_binding_shape(2), dtype=trt.nptype(engine.get_binding_dtype(2)))\n",
      "/tmp/ipykernel_1696/2974348848.py:27: DeprecationWarning: Use get_tensor_shape instead.\n",
      "  outputH1 = np.empty(context.get_binding_shape(3), dtype=trt.nptype(engine.get_binding_dtype(3)))\n",
      "/tmp/ipykernel_1696/2974348848.py:27: DeprecationWarning: Use get_tensor_dtype instead.\n",
      "  outputH1 = np.empty(context.get_binding_shape(3), dtype=trt.nptype(engine.get_binding_dtype(3)))\n",
      "/tmp/ipykernel_1696/2974348848.py:28: DeprecationWarning: Use get_tensor_shape instead.\n",
      "  outputH2 = np.empty(context.get_binding_shape(4), dtype=trt.nptype(engine.get_binding_dtype(4)))\n",
      "/tmp/ipykernel_1696/2974348848.py:28: DeprecationWarning: Use get_tensor_dtype instead.\n",
      "  outputH2 = np.empty(context.get_binding_shape(4), dtype=trt.nptype(engine.get_binding_dtype(4)))\n"
     ]
    }
   ],
   "source": [
    "trt_output = trt_inference(batch_size, hidden_size, trt_engineStr, data, attention_mask)\n",
    "\n",
    "trt_query_states, trt_key_states, trt_value_states = trt_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "79510430",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 32, 45, 128)\n",
      "[[[-0.27432635 -0.15507364  0.2143606  ...  0.34303248  0.5192882\n",
      "    0.7248092 ]\n",
      "  [-0.27432635 -0.15507364  0.2143606  ...  0.34303248  0.5192882\n",
      "    0.7248092 ]\n",
      "  [-0.27432635 -0.15507364  0.2143606  ...  0.34303248  0.5192882\n",
      "    0.7248092 ]\n",
      "  ...\n",
      "  [-0.27432635 -0.15507364  0.2143606  ...  0.34303248  0.5192882\n",
      "    0.7248092 ]\n",
      "  [-0.27432635 -0.15507364  0.2143606  ...  0.34303248  0.5192882\n",
      "    0.7248092 ]\n",
      "  [-0.27432635 -0.15507364  0.2143606  ...  0.34303248  0.5192882\n",
      "    0.7248092 ]]\n",
      "\n",
      " [[ 0.8556773   0.26205993  1.2780979  ...  0.5574117   0.05978028\n",
      "    0.02156186]\n",
      "  [ 0.8556773   0.26205993  1.2780979  ...  0.5574117   0.05978028\n",
      "    0.02156186]\n",
      "  [ 0.8556773   0.26205993  1.2780979  ...  0.5574117   0.05978028\n",
      "    0.02156186]\n",
      "  ...\n",
      "  [ 0.8556773   0.26205993  1.2780979  ...  0.5574117   0.05978028\n",
      "    0.02156186]\n",
      "  [ 0.8556773   0.26205993  1.2780979  ...  0.5574117   0.05978028\n",
      "    0.02156186]\n",
      "  [ 0.8556773   0.26205993  1.2780979  ...  0.5574117   0.05978028\n",
      "    0.02156186]]\n",
      "\n",
      " [[ 0.22803989  0.16950911  0.58905023 ...  0.29973075 -0.02850278\n",
      "    0.09827302]\n",
      "  [ 0.22803989  0.16950911  0.58905023 ...  0.29973075 -0.02850278\n",
      "    0.09827302]\n",
      "  [ 0.22803989  0.16950911  0.58905023 ...  0.29973075 -0.02850278\n",
      "    0.09827302]\n",
      "  ...\n",
      "  [ 0.22803989  0.16950911  0.58905023 ...  0.29973075 -0.02850278\n",
      "    0.09827302]\n",
      "  [ 0.22803989  0.16950911  0.58905023 ...  0.29973075 -0.02850278\n",
      "    0.09827302]\n",
      "  [ 0.22803989  0.16950911  0.58905023 ...  0.29973075 -0.02850278\n",
      "    0.09827302]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[ 1.1826813  -0.14908473 -0.38049537 ...  0.69865835  0.09731241\n",
      "   -0.63249457]\n",
      "  [ 1.1826813  -0.14908473 -0.38049537 ...  0.69865835  0.09731241\n",
      "   -0.63249457]\n",
      "  [ 1.1826813  -0.14908473 -0.38049537 ...  0.69865835  0.09731241\n",
      "   -0.63249457]\n",
      "  ...\n",
      "  [ 1.1826813  -0.14908473 -0.38049537 ...  0.69865835  0.09731241\n",
      "   -0.63249457]\n",
      "  [ 1.1826813  -0.14908473 -0.38049537 ...  0.69865835  0.09731241\n",
      "   -0.63249457]\n",
      "  [ 1.1826813  -0.14908473 -0.38049537 ...  0.69865835  0.09731241\n",
      "   -0.63249457]]\n",
      "\n",
      " [[-0.4123574   0.22553281  0.3495371  ... -0.11172449  1.4609227\n",
      "   -0.33853704]\n",
      "  [-0.4123574   0.22553281  0.3495371  ... -0.11172449  1.4609227\n",
      "   -0.33853704]\n",
      "  [-0.4123574   0.22553281  0.3495371  ... -0.11172449  1.4609227\n",
      "   -0.33853704]\n",
      "  ...\n",
      "  [-0.4123574   0.22553281  0.3495371  ... -0.11172449  1.4609227\n",
      "   -0.33853704]\n",
      "  [-0.4123574   0.22553281  0.3495371  ... -0.11172449  1.4609227\n",
      "   -0.33853704]\n",
      "  [-0.4123574   0.22553281  0.3495371  ... -0.11172449  1.4609227\n",
      "   -0.33853704]]\n",
      "\n",
      " [[ 0.07805991 -0.13385889  0.15156911 ... -0.46574485  0.03650038\n",
      "   -0.24505654]\n",
      "  [ 0.07805991 -0.13385889  0.15156911 ... -0.46574485  0.03650038\n",
      "   -0.24505654]\n",
      "  [ 0.07805991 -0.13385889  0.15156911 ... -0.46574485  0.03650038\n",
      "   -0.24505654]\n",
      "  ...\n",
      "  [ 0.07805991 -0.13385889  0.15156911 ... -0.46574485  0.03650038\n",
      "   -0.24505654]\n",
      "  [ 0.07805991 -0.13385889  0.15156911 ... -0.46574485  0.03650038\n",
      "   -0.24505654]\n",
      "  [ 0.07805991 -0.13385889  0.15156911 ... -0.46574485  0.03650038\n",
      "   -0.24505654]]]\n"
     ]
    }
   ],
   "source": [
    "print(trt_query_states.shape)\n",
    "print(trt_query_states[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e01b905a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 32, 45, 45)\n",
      "[[[0.02222222 0.02222222 0.02222222 ... 0.02222222 0.02222222 0.02222222]\n",
      "  [0.02222222 0.02222222 0.02222222 ... 0.02222222 0.02222222 0.02222222]\n",
      "  [0.02222222 0.02222222 0.02222222 ... 0.02222222 0.02222222 0.02222222]\n",
      "  ...\n",
      "  [0.02222222 0.02222222 0.02222222 ... 0.02222222 0.02222222 0.02222222]\n",
      "  [0.02222222 0.02222222 0.02222222 ... 0.02222222 0.02222222 0.02222222]\n",
      "  [0.02222222 0.02222222 0.02222222 ... 0.02222222 0.02222222 0.02222222]]\n",
      "\n",
      " [[0.02222222 0.02222222 0.02222222 ... 0.02222222 0.02222222 0.02222222]\n",
      "  [0.02222222 0.02222222 0.02222222 ... 0.02222222 0.02222222 0.02222222]\n",
      "  [0.02222222 0.02222222 0.02222222 ... 0.02222222 0.02222222 0.02222222]\n",
      "  ...\n",
      "  [0.02222222 0.02222222 0.02222222 ... 0.02222222 0.02222222 0.02222222]\n",
      "  [0.02222222 0.02222222 0.02222222 ... 0.02222222 0.02222222 0.02222222]\n",
      "  [0.02222222 0.02222222 0.02222222 ... 0.02222222 0.02222222 0.02222222]]\n",
      "\n",
      " [[0.02222222 0.02222222 0.02222222 ... 0.02222222 0.02222222 0.02222222]\n",
      "  [0.02222222 0.02222222 0.02222222 ... 0.02222222 0.02222222 0.02222222]\n",
      "  [0.02222222 0.02222222 0.02222222 ... 0.02222222 0.02222222 0.02222222]\n",
      "  ...\n",
      "  [0.02222222 0.02222222 0.02222222 ... 0.02222222 0.02222222 0.02222222]\n",
      "  [0.02222222 0.02222222 0.02222222 ... 0.02222222 0.02222222 0.02222222]\n",
      "  [0.02222222 0.02222222 0.02222222 ... 0.02222222 0.02222222 0.02222222]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[0.02222222 0.02222222 0.02222222 ... 0.02222222 0.02222222 0.02222222]\n",
      "  [0.02222222 0.02222222 0.02222222 ... 0.02222222 0.02222222 0.02222222]\n",
      "  [0.02222222 0.02222222 0.02222222 ... 0.02222222 0.02222222 0.02222222]\n",
      "  ...\n",
      "  [0.02222222 0.02222222 0.02222222 ... 0.02222222 0.02222222 0.02222222]\n",
      "  [0.02222222 0.02222222 0.02222222 ... 0.02222222 0.02222222 0.02222222]\n",
      "  [0.02222222 0.02222222 0.02222222 ... 0.02222222 0.02222222 0.02222222]]\n",
      "\n",
      " [[0.02222222 0.02222222 0.02222222 ... 0.02222222 0.02222222 0.02222222]\n",
      "  [0.02222222 0.02222222 0.02222222 ... 0.02222222 0.02222222 0.02222222]\n",
      "  [0.02222222 0.02222222 0.02222222 ... 0.02222222 0.02222222 0.02222222]\n",
      "  ...\n",
      "  [0.02222222 0.02222222 0.02222222 ... 0.02222222 0.02222222 0.02222222]\n",
      "  [0.02222222 0.02222222 0.02222222 ... 0.02222222 0.02222222 0.02222222]\n",
      "  [0.02222222 0.02222222 0.02222222 ... 0.02222222 0.02222222 0.02222222]]\n",
      "\n",
      " [[0.02222222 0.02222222 0.02222222 ... 0.02222222 0.02222222 0.02222222]\n",
      "  [0.02222222 0.02222222 0.02222222 ... 0.02222222 0.02222222 0.02222222]\n",
      "  [0.02222222 0.02222222 0.02222222 ... 0.02222222 0.02222222 0.02222222]\n",
      "  ...\n",
      "  [0.02222222 0.02222222 0.02222222 ... 0.02222222 0.02222222 0.02222222]\n",
      "  [0.02222222 0.02222222 0.02222222 ... 0.02222222 0.02222222 0.02222222]\n",
      "  [0.02222222 0.02222222 0.02222222 ... 0.02222222 0.02222222 0.02222222]]]\n"
     ]
    }
   ],
   "source": [
    "print(trt_key_states.shape)\n",
    "print(trt_key_states[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ebb3d9a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 45, 4096)\n",
      "[[-0.11104187 -0.53444463  1.2985446  ... -0.5544506  -0.20153326\n",
      "   0.60356915]\n",
      " [-0.11104187 -0.53444463  1.2985446  ... -0.5544506  -0.20153326\n",
      "   0.60356915]\n",
      " [-0.11104187 -0.53444463  1.2985446  ... -0.5544506  -0.20153326\n",
      "   0.60356915]\n",
      " ...\n",
      " [-0.11104187 -0.53444463  1.2985446  ... -0.5544506  -0.20153326\n",
      "   0.60356915]\n",
      " [-0.11104187 -0.53444463  1.2985446  ... -0.5544506  -0.20153326\n",
      "   0.60356915]\n",
      " [-0.11104187 -0.53444463  1.2985446  ... -0.5544506  -0.20153326\n",
      "   0.60356915]]\n"
     ]
    }
   ],
   "source": [
    "print(trt_value_states.shape)\n",
    "print(trt_value_states[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e8d9192",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6e7998e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1707210788.615002] [s24fvcfastllama-lq2tn:16937:f]        vfs_fuse.c:281  UCX  ERROR inotify_add_watch(/tmp) failed: No space left on device\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from cuda import cudart\n",
    "import torch\n",
    "from torch import Tensor, nn\n",
    "import tensorrt as trt\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "976de369",
   "metadata": {},
   "source": [
    "## Integration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bba43db5",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = dict()\n",
    "model_weight_dir = \"./weights/pytorch_model-00001-of-00033.bin\"\n",
    "\n",
    "batch_size, seq_len, hidden_size = 4, 45, 4096\n",
    "intermediate_size = 11008\n",
    "num_attention_heads = 32\n",
    "num_key_value_heads = 32\n",
    "max_position_embeddings = 2048\n",
    "rope_theta = 10000.0\n",
    "rms_norm_eps = 1e-6\n",
    "\n",
    "config[\"hidden_size\"] = hidden_size\n",
    "config[\"intermediate_size\"] = intermediate_size\n",
    "config[\"num_heads\"] = num_attention_heads\n",
    "config[\"head_dim\"] = config[\"hidden_size\"] // config[\"num_heads\"]\n",
    "config[\"num_key_value_heads\"] = num_key_value_heads\n",
    "config[\"num_key_value_groups\"] = config[\"num_heads\"] // config[\"num_key_value_heads\"]\n",
    "config[\"max_position_embeddings\"] = max_position_embeddings\n",
    "config[\"rope_theta\"] = rope_theta\n",
    "config[\"rms_norm_eps\"] = rms_norm_eps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bf98a2dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = torch.ones(batch_size, seq_len, hidden_size)\n",
    "attention_mask = torch.ones(batch_size, 1, seq_len, seq_len)\n",
    "position_ids = torch.arange(0, seq_len)\n",
    "position_ids = position_ids.repeat(batch_size, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23475d97",
   "metadata": {},
   "source": [
    "## Llama layernorm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "579bd0aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LlamaRMSNorm(nn.Module):\n",
    "    def __init__(self, hidden_size, eps=1e-6):\n",
    "        \"\"\"\n",
    "        LlamaRMSNorm is equivalent to T5LayerNorm\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.weight = nn.Parameter(torch.ones(hidden_size))\n",
    "        self.variance_epsilon = eps\n",
    "\n",
    "    def load(self, dir, norm_name):\n",
    "        weights = torch.load(dir)\n",
    "        RMSnorm_weight = dict()\n",
    "        for key in weights.keys():\n",
    "            if key.split(\".\")[3] == norm_name:\n",
    "                RMSnorm_weight[key[key.find(key.split(\".\")[4]):]] = weights[key]\n",
    "        self.load_state_dict(RMSnorm_weight)\n",
    "\n",
    "    def forward(self, hidden_states):\n",
    "        input_dtype = hidden_states.dtype\n",
    "        hidden_states = hidden_states.to(torch.float32)\n",
    "        variance = hidden_states.pow(2).mean(-1, keepdim=True)\n",
    "        hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)\n",
    "        return self.weight * hidden_states.to(input_dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbf65044",
   "metadata": {},
   "source": [
    "## LlamaAttention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f3a7ad88",
   "metadata": {},
   "outputs": [],
   "source": [
    "def repeat_kv(hidden_states: torch.Tensor, n_rep: int) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    This is the equivalent of torch.repeat_interleave(x, dim=1, repeats=n_rep). The hidden states go from (batch,\n",
    "    num_key_value_heads, seqlen, head_dim) to (batch, num_attention_heads, seqlen, head_dim)\n",
    "    \n",
    "    repeat at the second dimension\n",
    "    \"\"\"\n",
    "    batch, num_key_value_heads, slen, head_dim = hidden_states.shape\n",
    "    if n_rep == 1:\n",
    "        return hidden_states\n",
    "    hidden_states = hidden_states[:, :, None, :, :].expand(batch, num_key_value_heads, n_rep, slen, head_dim)\n",
    "    return hidden_states.reshape(batch, num_key_value_heads * n_rep, slen, head_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "02a87e48",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LlamaRotaryEmbedding(torch.nn.Module):\n",
    "    def __init__(self, dim, max_position_embeddings=2048, base=10000, device=None):\n",
    "        super().__init__()\n",
    "\n",
    "        self.dim = dim\n",
    "        self.max_position_embeddings = max_position_embeddings\n",
    "        self.base = base\n",
    "        inv_freq = 1.0 / (self.base ** (torch.arange(0, self.dim, 2).float().to(device) / self.dim))\n",
    "        \n",
    "        self.register_buffer(\"inv_freq\", inv_freq, persistent=False)\n",
    "\n",
    "        t = torch.arange(max_position_embeddings, device=device, dtype=self.inv_freq.dtype)\n",
    "\n",
    "        freqs = torch.einsum(\"i,j->ij\", t, self.inv_freq)\n",
    "        # Different from paper, but it uses a different permutation in order to obtain the same calculation\n",
    "        emb = torch.cat((freqs, freqs), dim=-1)\n",
    "        self.register_buffer(\"cos_cached\", emb.cos()[None, None, :, :].to(torch.get_default_dtype()), persistent=False)\n",
    "        self.register_buffer(\"sin_cached\", emb.sin()[None, None, :, :].to(torch.get_default_dtype()), persistent=False)\n",
    "\n",
    "    def rotate_half(self, x):\n",
    "        \"\"\"Rotates half the hidden dims of the input.\"\"\"\n",
    "        x1 = x[..., : x.shape[-1] // 2]\n",
    "        x2 = x[..., x.shape[-1] // 2 :]\n",
    "        return torch.cat((-x2, x1), dim=-1)\n",
    "\n",
    "    def forward(self, q, k, v, position_ids, seq_len=None):\n",
    "        # v: [bs, num_attention_heads, seq_len, head_size]\n",
    "        cos = self.cos_cached[:, :, :seq_len, ...].to(dtype=v.dtype)\n",
    "        sin = self.sin_cached[:, :, :seq_len, ...].to(dtype=v.dtype)\n",
    "        cos = cos.squeeze(1).squeeze(0)  # [seq_len, dim]\n",
    "        sin = sin.squeeze(1).squeeze(0)  # [seq_len, dim]\n",
    "        cos = cos[position_ids].unsqueeze(1)  # [bs, 1, seq_len, dim]\n",
    "        sin = sin[position_ids].unsqueeze(1)  # [bs, 1, seq_len, dim]\n",
    "\n",
    "        # The first two dimensions of cos and sin are always 1, so we can `squeeze` them.\n",
    "        q_embed = (q * cos) + (self.rotate_half(q) * sin)\n",
    "        k_embed = (k * cos) + (self.rotate_half(k) * sin)\n",
    "        return q_embed, k_embed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ba681fff",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LlamaAttention(nn.Module):\n",
    "    \"\"\"Multi-headed attention from 'Attention Is All You Need' paper\"\"\"\n",
    "\n",
    "    def __init__(self, config: dict):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.hidden_size = config[\"hidden_size\"]\n",
    "        self.num_heads = config[\"num_heads\"]\n",
    "        self.head_dim = config[\"hidden_size\"] // config[\"num_heads\"]\n",
    "        self.num_key_value_heads = config[\"num_key_value_heads\"]\n",
    "        self.num_key_value_groups = config[\"num_heads\"] // config[\"num_key_value_heads\"]\n",
    "        self.max_position_embeddings = config[\"max_position_embeddings\"]\n",
    "        self.rope_theta = config[\"rope_theta\"]\n",
    "\n",
    "        if (self.head_dim * self.num_heads) != self.hidden_size:\n",
    "            raise ValueError(\n",
    "                f\"hidden_size must be divisible by num_heads (got `hidden_size`: {self.hidden_size}\"\n",
    "                f\" and `num_heads`: {self.num_heads}).\"\n",
    "            )\n",
    "        self.q_proj = nn.Linear(self.hidden_size, self.num_heads * self.head_dim, bias=False)\n",
    "        self.k_proj = nn.Linear(self.hidden_size, self.num_key_value_heads * self.head_dim, bias=False)\n",
    "        self.v_proj = nn.Linear(self.hidden_size, self.num_key_value_heads * self.head_dim, bias=False)\n",
    "        self.o_proj = nn.Linear(self.num_heads * self.head_dim, self.hidden_size, bias=False)\n",
    "        self._init_rope()\n",
    "\n",
    "    def _init_rope(self):\n",
    "        print(\n",
    "            \"init rope\",\n",
    "            self.head_dim,\n",
    "            self.max_position_embeddings,\n",
    "            self.rope_theta,\n",
    "        )\n",
    "        self.rotary_emb = LlamaRotaryEmbedding(\n",
    "            self.head_dim,\n",
    "            max_position_embeddings=self.max_position_embeddings,\n",
    "            base=self.rope_theta,\n",
    "        )\n",
    "\n",
    "    def _shape(self, tensor: torch.Tensor, seq_len: int, bsz: int):\n",
    "        return tensor.view(bsz, seq_len, self.num_heads, self.head_dim).transpose(1, 2).contiguous()\n",
    "\n",
    "\n",
    "    def load(self, dir):\n",
    "        weights = torch.load(dir)\n",
    "        self_attn_weights = dict()\n",
    "        for key in weights.keys():\n",
    "            print(key)\n",
    "            if key == \"model.layers.0.self_attn.rotary_emb.inv_freq\":\n",
    "                print(weights[key])\n",
    "                continue\n",
    "            if key.split(\".\")[3] == \"self_attn\":\n",
    "                self_attn_weights[key[key.find(key.split(\".\")[4]):]] = weights[key]\n",
    "        self.load_state_dict(self_attn_weights)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        hidden_states: torch.Tensor,\n",
    "        attention_mask: None,\n",
    "        position_ids: None,\n",
    "        past_key_value: None,\n",
    "        output_attentions: bool = False,\n",
    "        use_cache: bool = False,\n",
    "    ):\n",
    "        # bsz = batch size; q_len = query length; _ = hidden size\n",
    "        bsz, q_len, _ = hidden_states.size()\n",
    "\n",
    "        # do projection\n",
    "        query_states = self.q_proj(hidden_states)\n",
    "        key_states = self.k_proj(hidden_states)\n",
    "        value_states = self.v_proj(hidden_states)\n",
    "\n",
    "        # reshape\n",
    "        query_states = query_states.view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        key_states = key_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n",
    "        value_states = value_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n",
    "\n",
    "#         #####################################################\n",
    "#         # in hugging face, they do have kv cache, however, they don't have other attention optimization\n",
    "#         # this could be done directly in tensorRT by using dynamic shape\n",
    "#         kv_seq_len = key_states.shape[-2]\n",
    "#         if past_key_value is not None:\n",
    "#             kv_seq_len += past_key_value[0].shape[-2]\n",
    "\n",
    "#         query_states, key_states = self.rotary_emb(query_states, key_states, value_states, position_ids, seq_len=q_len)\n",
    "\n",
    "#         if past_key_value is not None:\n",
    "#             # reuse k, v, self_attention\n",
    "#             key_states = torch.cat([past_key_value[0], key_states], dim=2)\n",
    "#             value_states = torch.cat([past_key_value[1], value_states], dim=2)\n",
    "\n",
    "#         past_key_value = (key_states, value_states) if use_cache else None\n",
    "\n",
    "#         print(self.num_key_value_groups)\n",
    "#         # repeat k/v heads if n_kv_heads < n_heads\n",
    "#         key_states = repeat_kv(key_states, self.num_key_value_groups)\n",
    "#         value_states = repeat_kv(value_states, self.num_key_value_groups)\n",
    "#         #####################################################\n",
    "\n",
    "        attn_weights = torch.matmul(query_states, key_states.transpose(2, 3)) / math.sqrt(self.head_dim)\n",
    "\n",
    "        # attention_mask needs to be infered\n",
    "        attn_weights = attn_weights + attention_mask\n",
    "\n",
    "        # upcast attention to fp32\n",
    "        attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query_states.dtype)\n",
    "\n",
    "        attn_output = torch.matmul(attn_weights, value_states)\n",
    "\n",
    "        attn_output = attn_output.transpose(1, 2) # .contiguous()\n",
    "        attn_output = attn_output.reshape(bsz, q_len, self.hidden_size)\n",
    "\n",
    "        attn_output = self.o_proj(attn_output)\n",
    "\n",
    "        return attn_output, attn_weights, None\n",
    "\n",
    "        # since normally it will be false\n",
    "#         if not output_attentions:\n",
    "#             attn_weights = None\n",
    "\n",
    "#         return attn_output, attn_weights, past_key_value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5d5b5c9",
   "metadata": {},
   "source": [
    "## Llama MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c5bbdb75",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SiLUActivation(nn.Module):\n",
    "    \"\"\"\n",
    "    See Gaussian Error Linear Units (Hendrycks et al., https://arxiv.org/abs/1606.08415) where the SiLU (Sigmoid Linear\n",
    "    Unit) was originally introduced and coined, and see Sigmoid-Weighted Linear Units for Neural Network Function\n",
    "    Approximation in Reinforcement Learbatch_sizeg (Elfwing et al., https://arxiv.org/abs/1702.03118) and Swish: a Self-Gated\n",
    "    Activation Function (Ramachandran et al., https://arxiv.org/abs/1710.05941v1) where the SiLU was experimented with\n",
    "    later.\n",
    "    \"\"\"\n",
    "\n",
    "    def forward(self, input: Tensor) -> Tensor:\n",
    "        return nn.functional.silu(input)\n",
    "    \n",
    "    def b_forward(self, input: Tensor) -> Tensor:\n",
    "        return torch.matmul(input.T, nn.functional.sigmoid(input))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1249a9c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LlamaMLP(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.hidden_size = config['hidden_size']\n",
    "        self.intermediate_size = config['intermediate_size']\n",
    "        self.gate_proj = nn.Linear(self.hidden_size, self.intermediate_size, bias=False)\n",
    "        self.up_proj = nn.Linear(self.hidden_size, self.intermediate_size, bias=False)\n",
    "        self.down_proj = nn.Linear(self.intermediate_size, self.hidden_size, bias=False)\n",
    "        self.act_fn = SiLUActivation()\n",
    "        self.init = False\n",
    "\n",
    "    def load(self, dir):\n",
    "        weights = torch.load(dir)\n",
    "        mlp_weights = dict()\n",
    "        for key in weights.keys():\n",
    "            if key.split(\".\")[3] == \"mlp\":\n",
    "                mlp_weights[key[key.find(key.split(\".\")[4]):]] = weights[key]\n",
    "        self.load_state_dict(mlp_weights)\n",
    "\n",
    "    def forward(self, x):\n",
    "        down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\n",
    "\n",
    "        return down_proj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2518e64e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LlamaDecoderLayer(nn.Module):\n",
    "    def __init__(self, config: dict):\n",
    "        super().__init__()\n",
    "        # if config.tensorRT:\n",
    "        #     exit()\n",
    "        self.hidden_size = config['hidden_size']\n",
    "        \n",
    "        self.num_heads = config[\"num_heads\"]\n",
    "        self.head_dim = config[\"hidden_size\"] // config[\"num_heads\"]\n",
    "        self.num_key_value_heads = config[\"num_key_value_heads\"]\n",
    "        self.num_key_value_groups = config[\"num_heads\"] // config[\"num_key_value_heads\"]\n",
    "        self.max_position_embeddings = config[\"max_position_embeddings\"]\n",
    "        self.rope_theta = config[\"rope_theta\"]\n",
    "        \n",
    "        self.self_attn = LlamaAttention(config=config)\n",
    "        self.mlp = LlamaMLP(config)\n",
    "        self.input_layernorm = LlamaRMSNorm(config['hidden_size'], eps=config['rms_norm_eps'])\n",
    "        self.post_attention_layernorm = LlamaRMSNorm(config['hidden_size'], eps=config['rms_norm_eps'])\n",
    "\n",
    "\n",
    "    def load(self, dir):\n",
    "        self.self_attn.load(dir)\n",
    "        self.mlp.load(dir)\n",
    "        self.input_layernorm.load(dir, \"input_layernorm\")\n",
    "        self.post_attention_layernorm.load(dir, \"post_attention_layernorm\")\n",
    "\n",
    "    def forward(\n",
    "        self,        \n",
    "        hidden_states: torch.Tensor,\n",
    "        attention_mask: None,\n",
    "        position_ids: None,\n",
    "        past_key_value: None,\n",
    "        output_attentions: bool = False,\n",
    "        use_cache: bool = False,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            hidden_states (`torch.FloatTensor`): input to the layer of shape `(batch, seq_len, embed_dim)`\n",
    "            attention_mask (`torch.FloatTensor`, *optional*): attention mask of size\n",
    "                `(batch, 1, tgt_len, src_len)` where padding elements are indicated by very large negative values.\n",
    "            output_attentions (`bool`, *optional*):\n",
    "                Whether or not to return the attentions tensors of all attention layers. See `attentions` under\n",
    "                returned tensors for more detail.\n",
    "            use_cache (`bool`, *optional*):\n",
    "                If set to `True`, `past_key_values` key value states are returned and can be used to speed up decoding\n",
    "                (see `past_key_values`).\n",
    "            past_key_value (`Tuple(torch.FloatTensor)`, *optional*): cached past key and value projection states\n",
    "        \"\"\"\n",
    "\n",
    "        residual = hidden_states\n",
    "\n",
    "        hidden_states = self.input_layernorm(hidden_states)\n",
    "\n",
    "        # Self Attention\n",
    "        hidden_states, self_attn_weights, present_key_value = self.self_attn(\n",
    "            hidden_states=hidden_states,\n",
    "            attention_mask=attention_mask,\n",
    "            position_ids=position_ids,\n",
    "            past_key_value=past_key_value,\n",
    "            output_attentions=output_attentions,\n",
    "            use_cache=use_cache,\n",
    "        )\n",
    "\n",
    "#         if past_key_value is not None:\n",
    "#             for value in past_key_value:\n",
    "#                 print(f\"past_key_value={value.shape}\")\n",
    "\n",
    "        hidden_states = residual + hidden_states\n",
    "\n",
    "        # Fully Connected\n",
    "        residual = hidden_states\n",
    "        hidden_states = self.post_attention_layernorm(hidden_states)\n",
    "        hidden_states = self.mlp(hidden_states)\n",
    "        hidden_states = residual + hidden_states\n",
    "\n",
    "        outputs = (hidden_states,)\n",
    "\n",
    "#         if output_attentions:\n",
    "#             outputs += (self_attn_weights,)\n",
    "\n",
    "#         if use_cache:\n",
    "#             outputs += (present_key_value,)\n",
    "\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12a0955a",
   "metadata": {},
   "source": [
    "## Test torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "97872f29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "init rope 128 2048 10000.0\n",
      "model.layers.0.self_attn.q_proj.weight\n",
      "model.layers.0.self_attn.k_proj.weight\n",
      "model.layers.0.self_attn.v_proj.weight\n",
      "model.layers.0.self_attn.o_proj.weight\n",
      "model.layers.0.mlp.gate_proj.weight\n",
      "model.layers.0.mlp.down_proj.weight\n",
      "model.layers.0.mlp.up_proj.weight\n",
      "model.layers.0.input_layernorm.weight\n",
      "model.layers.0.post_attention_layernorm.weight\n",
      "model.layers.0.self_attn.rotary_emb.inv_freq\n",
      "tensor([1.0000e+00, 8.6596e-01, 7.4989e-01, 6.4938e-01, 5.6234e-01, 4.8697e-01,\n",
      "        4.2170e-01, 3.6517e-01, 3.1623e-01, 2.7384e-01, 2.3714e-01, 2.0535e-01,\n",
      "        1.7783e-01, 1.5399e-01, 1.3335e-01, 1.1548e-01, 1.0000e-01, 8.6596e-02,\n",
      "        7.4989e-02, 6.4938e-02, 5.6234e-02, 4.8697e-02, 4.2170e-02, 3.6517e-02,\n",
      "        3.1623e-02, 2.7384e-02, 2.3714e-02, 2.0535e-02, 1.7783e-02, 1.5399e-02,\n",
      "        1.3335e-02, 1.1548e-02, 1.0000e-02, 8.6596e-03, 7.4989e-03, 6.4938e-03,\n",
      "        5.6234e-03, 4.8697e-03, 4.2170e-03, 3.6517e-03, 3.1623e-03, 2.7384e-03,\n",
      "        2.3714e-03, 2.0535e-03, 1.7783e-03, 1.5399e-03, 1.3335e-03, 1.1548e-03,\n",
      "        1.0000e-03, 8.6596e-04, 7.4989e-04, 6.4938e-04, 5.6234e-04, 4.8697e-04,\n",
      "        4.2170e-04, 3.6517e-04, 3.1623e-04, 2.7384e-04, 2.3714e-04, 2.0535e-04,\n",
      "        1.7783e-04, 1.5399e-04, 1.3335e-04, 1.1548e-04])\n"
     ]
    }
   ],
   "source": [
    "model = LlamaDecoderLayer(config)\n",
    "\n",
    "device = torch.device(\"cuda\")\n",
    "\n",
    "model.load(model_weight_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "63396750",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (rotary_emb): LlamaRotaryEmbedding()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "    (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "    (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
      "    (act_fn): SiLUActivation()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm()\n",
      "  (post_attention_layernorm): LlamaRMSNorm()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d24802ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.to(device)\n",
    "\n",
    "data_D = data.to(device)\n",
    "attention_mask_D = attention_mask.to(device)\n",
    "position_ids_D = position_ids.to(device)\n",
    "\n",
    "past_key_value = None\n",
    "\n",
    "# attentiona mask\n",
    "# position_ids\n",
    "# specifies the position id of the corresponding hidden state tensor element\n",
    "# e.g. hid = [3, 4, 6] => pos_id = [0, 1, 2]\n",
    "# past_key_value\n",
    "# if use cache, past key value will contain past kv values\n",
    "output = model(hidden_states=data_D,\n",
    "               attention_mask=attention_mask_D,\n",
    "               position_ids=position_ids_D,\n",
    "               past_key_value=past_key_value,\n",
    "               output_attentions=False,\n",
    "               use_cache=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f6d13e53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([[[1.0010, 1.0160, 0.9786,  ..., 1.0382, 1.0382, 1.0218],\n",
      "         [1.0010, 1.0160, 0.9786,  ..., 1.0382, 1.0382, 1.0218],\n",
      "         [1.0010, 1.0160, 0.9786,  ..., 1.0382, 1.0382, 1.0218],\n",
      "         ...,\n",
      "         [1.0010, 1.0160, 0.9786,  ..., 1.0382, 1.0382, 1.0218],\n",
      "         [1.0010, 1.0160, 0.9786,  ..., 1.0382, 1.0382, 1.0218],\n",
      "         [1.0010, 1.0160, 0.9786,  ..., 1.0382, 1.0382, 1.0218]],\n",
      "\n",
      "        [[1.0010, 1.0160, 0.9786,  ..., 1.0382, 1.0382, 1.0218],\n",
      "         [1.0010, 1.0160, 0.9786,  ..., 1.0382, 1.0382, 1.0218],\n",
      "         [1.0010, 1.0160, 0.9786,  ..., 1.0382, 1.0382, 1.0218],\n",
      "         ...,\n",
      "         [1.0010, 1.0160, 0.9786,  ..., 1.0382, 1.0382, 1.0218],\n",
      "         [1.0010, 1.0160, 0.9786,  ..., 1.0382, 1.0382, 1.0218],\n",
      "         [1.0010, 1.0160, 0.9786,  ..., 1.0382, 1.0382, 1.0218]],\n",
      "\n",
      "        [[1.0010, 1.0160, 0.9786,  ..., 1.0382, 1.0382, 1.0218],\n",
      "         [1.0010, 1.0160, 0.9786,  ..., 1.0382, 1.0382, 1.0218],\n",
      "         [1.0010, 1.0160, 0.9786,  ..., 1.0382, 1.0382, 1.0218],\n",
      "         ...,\n",
      "         [1.0010, 1.0160, 0.9786,  ..., 1.0382, 1.0382, 1.0218],\n",
      "         [1.0010, 1.0160, 0.9786,  ..., 1.0382, 1.0382, 1.0218],\n",
      "         [1.0010, 1.0160, 0.9786,  ..., 1.0382, 1.0382, 1.0218]],\n",
      "\n",
      "        [[1.0010, 1.0160, 0.9786,  ..., 1.0382, 1.0382, 1.0218],\n",
      "         [1.0010, 1.0160, 0.9786,  ..., 1.0382, 1.0382, 1.0218],\n",
      "         [1.0010, 1.0160, 0.9786,  ..., 1.0382, 1.0382, 1.0218],\n",
      "         ...,\n",
      "         [1.0010, 1.0160, 0.9786,  ..., 1.0382, 1.0382, 1.0218],\n",
      "         [1.0010, 1.0160, 0.9786,  ..., 1.0382, 1.0382, 1.0218],\n",
      "         [1.0010, 1.0160, 0.9786,  ..., 1.0382, 1.0382, 1.0218]]],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>),)\n"
     ]
    }
   ],
   "source": [
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "696cb26d",
   "metadata": {},
   "source": [
    "## tensorRT attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "27649930",
   "metadata": {},
   "outputs": [],
   "source": [
    "# seq length is not specified, since it is a dynamic size\n",
    "def trt_create(batch_size, hidden_size, intermediate_size, model):\n",
    "    \n",
    "    logger = trt.Logger(trt.Logger.ERROR)\n",
    "    builder = trt.Builder(logger)\n",
    "\n",
    "    network = builder.create_network(1 << int(trt.NetworkDefinitionCreationFlag.EXPLICIT_BATCH))\n",
    "    config = builder.create_builder_config()\n",
    "\n",
    "    # input\n",
    "    hidden_states = network.add_input('hidden_states', trt.DataType.FLOAT, (batch_size, -1, hidden_size))\n",
    "    attention_mask = network.add_input('attention_mask', trt.DataType.FLOAT, (batch_size, 1, -1, -1))\n",
    "\n",
    "    # dynamic shape optimization\n",
    "    profile = builder.create_optimization_profile();\n",
    "    profile.set_shape(\"hidden_states\", (batch_size, 1, hidden_size), (batch_size, 1, hidden_size), (batch_size, 45, hidden_size))\n",
    "    profile.set_shape(\"attention_mask\", (batch_size, 1, 1, 1), (batch_size, 1, 1, 1), (batch_size, 1, 45, 45))\n",
    "    config.add_optimization_profile(profile)\n",
    "\n",
    "    ############################# input layer norm begins #############################\n",
    "    # RMSNorm Layer: 1) Square: X^2 -> 2) Sum: sum of all x^2 -> 3) Mean: 1/N -> 4) Root: sqrt(X) -> 5) Division: 1/X\n",
    "    # 1) Square: X^2\n",
    "    input_rms_power_layer = network.add_elementwise(hidden_states, hidden_states, op=trt.ElementWiseOperation.PROD)\n",
    "\n",
    "    # 2) Sum: sum of all X^2\n",
    "    input_rms_sum_layer = network.add_reduce(input_rms_power_layer.get_output(0), op=trt.ReduceOperation.SUM, axes=1, keep_dims=True)\n",
    "\n",
    "    # 3) Mean: 1/N\n",
    "    input_rms_mean_layer = network.add_reduce(input_rms_sum_layer.get_output(0), op=trt.ReduceOperation.AVG, axes=7, keep_dims=True)\n",
    "\n",
    "    # 4) Root: sqrt(X)\n",
    "    input_rms_sqrt_layer = network.add_unary(input_rms_mean_layer.get_output(0), op=trt.UnaryOperation.SQRT)\n",
    "\n",
    "    # 5) Division: 1/X\n",
    "    input_rms_div_layer = network.add_elementwise(hidden_states, input_rms_sqrt_layer.get_output(0), op=trt.ElementWiseOperation.DIV)\n",
    "    \n",
    "    # 6) times weight\n",
    "    input_rms_weight = model.input_layernorm.weight.clone().detach().cpu().numpy() * 2\n",
    "    input_rms_weight = np.expand_dims(input_rms_weight, 0)\n",
    "    input_rms_weight = np.expand_dims(input_rms_weight, 0)\n",
    "    input_rms_weight_shape = list(input_rms_weight.shape)\n",
    "    input_rms_weight_layer = network.add_constant(shape=input_rms_weight_shape, weights=trt.Weights(input_rms_weight))\n",
    "\n",
    "    input_rms_final_layer = network.add_elementwise(input_rms_div_layer.get_output(0),\n",
    "                                                    input_rms_weight_layer.get_output(0),\n",
    "                                                    op=trt.ElementWiseOperation.PROD)\n",
    "    ############################# input layer norm ends #############################\n",
    "\n",
    "\n",
    "    ############################# self_attention begins #############################\n",
    "    # self.q_proj(hidden_states)\n",
    "    q_proj_weight = model.self_attn.q_proj.weight.clone().detach().cpu().numpy()\n",
    "    q_proj_weight = np.expand_dims(q_proj_weight, 0)\n",
    "    q_proj_weight_shape = list(q_proj_weight.shape)\n",
    "    q_proj_weight_layer = network.add_constant(shape=q_proj_weight_shape, weights=trt.Weights(q_proj_weight))\n",
    "\n",
    "    q_proj_layer = network.add_matrix_multiply(input_rms_final_layer.get_output(0),\n",
    "                                               trt.MatrixOperation.NONE,\n",
    "                                               q_proj_weight_layer.get_output(0),\n",
    "                                               trt.MatrixOperation.TRANSPOSE)\n",
    "\n",
    "    # query_states = query_states.view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "    q_proj_shuffle_layer = network.add_shuffle(q_proj_layer.get_output(0))\n",
    "    q_proj_shuffle_layer.reshape_dims = trt.Dims([batch_size, -1, model.num_heads, model.head_dim])\n",
    "    q_proj_shuffle_layer.second_transpose = trt.Permutation([0, 2, 1, 3])\n",
    "\n",
    "    \n",
    "    # self.k_proj(hidden_states)\n",
    "    k_proj_weight = model.self_attn.k_proj.weight.clone().detach().cpu().numpy()\n",
    "    k_proj_weight = np.expand_dims(k_proj_weight, 0)\n",
    "    k_proj_weight_shape = list(k_proj_weight.shape)\n",
    "    k_proj_weight_layer = network.add_constant(shape=k_proj_weight_shape, weights=trt.Weights(k_proj_weight))\n",
    "\n",
    "    k_proj_layer = network.add_matrix_multiply(input_rms_final_layer.get_output(0),\n",
    "                                               trt.MatrixOperation.NONE,\n",
    "                                               k_proj_weight_layer.get_output(0),\n",
    "                                               trt.MatrixOperation.TRANSPOSE)\n",
    "\n",
    "    # key_states = key_states.view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "    k_proj_shuffle_layer = network.add_shuffle(k_proj_layer.get_output(0))\n",
    "    k_proj_shuffle_layer.reshape_dims = trt.Dims([batch_size, -1, model.num_heads, model.head_dim])\n",
    "    k_proj_shuffle_layer.second_transpose = trt.Permutation([0, 2, 3, 1])\n",
    "\n",
    "\n",
    "    # self.v_proj(hidden_states)\n",
    "    v_proj_weight = model.self_attn.v_proj.weight.clone().detach().cpu().numpy()\n",
    "    v_proj_weight = np.expand_dims(v_proj_weight, 0)\n",
    "    v_proj_weight_shape = list(v_proj_weight.shape)\n",
    "    v_proj_weight_layer = network.add_constant(shape=v_proj_weight_shape, weights=trt.Weights(v_proj_weight))\n",
    "\n",
    "    v_proj_layer = network.add_matrix_multiply(input_rms_final_layer.get_output(0),\n",
    "                                               trt.MatrixOperation.NONE,\n",
    "                                               v_proj_weight_layer.get_output(0),\n",
    "                                               trt.MatrixOperation.TRANSPOSE)\n",
    "\n",
    "    # value_states = value_states.view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "    v_proj_shuffle_layer = network.add_shuffle(v_proj_layer.get_output(0))\n",
    "    v_proj_shuffle_layer.reshape_dims = trt.Dims([batch_size, -1, model.num_heads, model.head_dim])\n",
    "    v_proj_shuffle_layer.second_transpose = trt.Permutation([0, 2, 1, 3])\n",
    "\n",
    "    # attn_weights = torch.matmul(query_states, key_states.transpose(2, 3)) / math.sqrt(self.head_dim)\n",
    "    attn_weights_mult_layer = network.add_matrix_multiply(q_proj_shuffle_layer.get_output(0),\n",
    "                                                          trt.MatrixOperation.NONE,\n",
    "                                                          k_proj_shuffle_layer.get_output(0),\n",
    "                                                          trt.MatrixOperation.NONE)\n",
    "\n",
    "    sqrt_head_dim = np.array([1 / math.sqrt(model.head_dim)], np.float32).reshape(-1)\n",
    "    attn_weights_layer = network.add_scale(attn_weights_mult_layer.get_output(0),\n",
    "                                           trt.ScaleMode.UNIFORM,\n",
    "                                           scale=sqrt_head_dim)\n",
    "\n",
    "    # attn_weights = attn_weights + attention_mask\n",
    "    attn_mix_mask = network.add_elementwise(attn_weights_layer.get_output(0),\n",
    "                                            attention_mask,\n",
    "                                            op=trt.ElementWiseOperation.SUM)\n",
    "\n",
    "    # attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query_states.dtype)\n",
    "    softmax_layer = network.add_softmax(attn_mix_mask.get_output(0))\n",
    "    softmax_layer.axes = 1 << 3\n",
    "\n",
    "    # attn_output = torch.matmul(attn_weights, value_states)\n",
    "    attn_output_layer = network.add_matrix_multiply(softmax_layer.get_output(0),\n",
    "                                                    trt.MatrixOperation.NONE,\n",
    "                                                    v_proj_shuffle_layer.get_output(0),\n",
    "                                                    trt.MatrixOperation.NONE)\n",
    "\n",
    "    # attn_output = attn_output.transpose(1, 2).contiguous()\n",
    "    # attn_output = attn_output.reshape(bsz, q_len, self.hidden_size)\n",
    "    attn_output_shuffle_layer = network.add_shuffle(attn_output_layer.get_output(0))\n",
    "    attn_output_shuffle_layer.first_transpose = trt.Permutation([0, 2, 1, 3])\n",
    "    attn_output_shuffle_layer.reshape_dims = trt.Dims([batch_size, -1, hidden_size])\n",
    "\n",
    "    # attn_output = self.o_proj(attn_output)\n",
    "    o_proj_weight = model.self_attn.o_proj.weight.clone().detach().cpu().numpy()\n",
    "    o_proj_weight = np.expand_dims(o_proj_weight, 0)\n",
    "    o_proj_weight_shape = list(o_proj_weight.shape)\n",
    "    o_proj_weight_layer = network.add_constant(shape=o_proj_weight_shape, weights=trt.Weights(o_proj_weight))\n",
    "    \n",
    "    o_proj_layer = network.add_matrix_multiply(attn_output_shuffle_layer.get_output(0),\n",
    "                                               trt.MatrixOperation.NONE,\n",
    "                                               o_proj_weight_layer.get_output(0),\n",
    "                                               trt.MatrixOperation.TRANSPOSE)\n",
    "    ############################# self_attention ends #############################\n",
    "\n",
    "    ############################# hidden state update begins #############################\n",
    "    self_attn_mix_residual_layer = network.add_elementwise(hidden_states,\n",
    "                                                           o_proj_layer.get_output(0),\n",
    "                                                           op=trt.ElementWiseOperation.SUM)\n",
    "    ############################# hidden state update ends #############################\n",
    "\n",
    "    ############################# post attn norm begins #############################\n",
    "    # RMSNorm Layer: 1) Square: X^2 -> 2) Sum: sum of all x^2 -> 3) Mean: 1/N -> 4) Root: sqrt(X) -> 5) Division: 1/X\n",
    "    # 1) Square: X^2\n",
    "    post_attn_rms_power_layer = network.add_elementwise(self_attn_mix_residual_layer.get_output(0),\n",
    "                                                        self_attn_mix_residual_layer.get_output(0),\n",
    "                                                        op=trt.ElementWiseOperation.PROD)\n",
    "\n",
    "    # 2) Sum: sum of all X^2\n",
    "    post_attn_rms_sum_layer = network.add_reduce(post_attn_rms_power_layer.get_output(0), op=trt.ReduceOperation.SUM, axes=1, keep_dims=True)\n",
    "\n",
    "    # 3) Mean: 1/N\n",
    "    post_attn_rms_mean_layer = network.add_reduce(post_attn_rms_sum_layer.get_output(0), op=trt.ReduceOperation.AVG, axes=7, keep_dims=True)\n",
    "\n",
    "    # 4) Root: sqrt(X)\n",
    "    post_attn_rms_sqrt_layer = network.add_unary(post_attn_rms_mean_layer.get_output(0), op=trt.UnaryOperation.SQRT)\n",
    "\n",
    "    # 5) Division: 1/X\n",
    "    post_attn_rms_div_layer = network.add_elementwise(self_attn_mix_residual_layer.get_output(0),\n",
    "                                                      post_attn_rms_sqrt_layer.get_output(0),\n",
    "                                                      op=trt.ElementWiseOperation.DIV)\n",
    "    \n",
    "    # 6) times weight\n",
    "    post_attn_rms_weight = model.post_attention_layernorm.weight.clone().detach().cpu().numpy() * 2\n",
    "    post_attn_rms_weight = np.expand_dims(post_attn_rms_weight, 0)\n",
    "    post_attn_rms_weight = np.expand_dims(post_attn_rms_weight, 0)\n",
    "    post_attn_rms_weight_shape = list(post_attn_rms_weight.shape)\n",
    "    post_attn_rms_weight_layer = network.add_constant(shape=post_attn_rms_weight_shape, weights=trt.Weights(post_attn_rms_weight))\n",
    "\n",
    "    post_attn_rms_final_layer = network.add_elementwise(post_attn_rms_div_layer.get_output(0),\n",
    "                                                        post_attn_rms_weight_layer.get_output(0),\n",
    "                                                        op=trt.ElementWiseOperation.PROD)\n",
    "    ############################# post attn layer norm ends #############################\n",
    "\n",
    "    ############################# post attn MLP begins #############################\n",
    "    # self.up_proj(x)\n",
    "    up_proj_weight = model.mlp.up_proj.weight.clone().detach().cpu().numpy()\n",
    "    up_proj_weight = np.expand_dims(up_proj_weight, 0)\n",
    "    up_proj_weight_shape = list(up_proj_weight.shape)\n",
    "    up_proj_weight_layer = network.add_constant(shape=up_proj_weight_shape, weights=trt.Weights(up_proj_weight))\n",
    "\n",
    "    up_proj_layer = network.add_matrix_multiply(post_attn_rms_final_layer.get_output(0),\n",
    "                                                trt.MatrixOperation.NONE,\n",
    "                                                up_proj_weight_layer.get_output(0),\n",
    "                                                trt.MatrixOperation.TRANSPOSE)\n",
    "\n",
    "    # self.gate_proj(x)\n",
    "    gate_proj_weight = model.mlp.gate_proj.weight.clone().detach().cpu().numpy()\n",
    "    gate_proj_weight = np.expand_dims(gate_proj_weight, 0)\n",
    "    gate_proj_weight_shape = list(gate_proj_weight.shape)\n",
    "    gate_proj_weight_layer = network.add_constant(shape=gate_proj_weight_shape, weights=trt.Weights(gate_proj_weight))\n",
    "\n",
    "    gate_proj_layer = network.add_matrix_multiply(post_attn_rms_final_layer.get_output(0),\n",
    "                                                  trt.MatrixOperation.NONE,\n",
    "                                                  gate_proj_weight_layer.get_output(0),\n",
    "                                                  trt.MatrixOperation.TRANSPOSE)\n",
    "\n",
    "    # act_fn(self.gate_proj(x))\n",
    "    selu_sigmoid_layer = network.add_activation(gate_proj_layer.get_output(0), type=trt.ActivationType.SIGMOID)\n",
    "    selu_mult_layer = network.add_elementwise(gate_proj_layer.get_output(0),\n",
    "                                              selu_sigmoid_layer.get_output(0),\n",
    "                                              op=trt.ElementWiseOperation.PROD)\n",
    "\n",
    "    # act_fn(self.gate_proj(x)) * self.up_proj(x)\n",
    "    before_down_proj_layer = network.add_elementwise(selu_mult_layer.get_output(0),\n",
    "                                                     up_proj_layer.get_output(0),\n",
    "                                                     op=trt.ElementWiseOperation.PROD)\n",
    "\n",
    "    down_proj_weight = model.mlp.down_proj.weight.clone().detach().cpu().numpy()\n",
    "    down_proj_weight = np.expand_dims(down_proj_weight, 0)\n",
    "    down_proj_weight_shape = list(down_proj_weight.shape)\n",
    "    down_proj_weight_layer = network.add_constant(shape=down_proj_weight_shape, weights=trt.Weights(down_proj_weight))\n",
    "\n",
    "    down_proj_layer = network.add_matrix_multiply(before_down_proj_layer.get_output(0),\n",
    "                                                  trt.MatrixOperation.NONE,\n",
    "                                                  down_proj_weight_layer.get_output(0),\n",
    "                                                  trt.MatrixOperation.TRANSPOSE)\n",
    "    ############################# post attn MLP ends #############################\n",
    "\n",
    "    ############################# hidden state update begins #############################\n",
    "    mlp_mix_residual_layer = network.add_elementwise(self_attn_mix_residual_layer.get_output(0),\n",
    "                                                     down_proj_layer.get_output(0),\n",
    "                                                     op=trt.ElementWiseOperation.SUM)\n",
    "    ############################# hidden state update ends #############################\n",
    "\n",
    "    # output\n",
    "    # the order of output will be related to the order of the tensor creation\n",
    "    network.mark_output(self_attn_mix_residual_layer.get_output(0))\n",
    "    network.mark_output(post_attn_rms_final_layer.get_output(0))\n",
    "    network.mark_output(mlp_mix_residual_layer.get_output(0))\n",
    "\n",
    "    engineString = builder.build_serialized_network(network, config)\n",
    "\n",
    "    return engineString"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "033a3e4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "trt_engineStr = trt_create(batch_size, hidden_size, intermediate_size, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "185def87",
   "metadata": {},
   "outputs": [],
   "source": [
    "def trt_inference(batch_size, hidden_size, engineString, raw_data, raw_attn_mask):\n",
    "#     print(engineString)\n",
    "#     print(\"Runtime\")\n",
    "    logger = trt.Logger(trt.Logger.ERROR)\n",
    "    engine = trt.Runtime(logger).deserialize_cuda_engine(engineString)\n",
    "    context = engine.create_execution_context()\n",
    "\n",
    "    # dynamic shape configure\n",
    "    print(\"Set input shape\", (batch_size, seq_len, hidden_size))\n",
    "\n",
    "    context.set_input_shape(\"hidden_states\", (batch_size, seq_len, hidden_size))\n",
    "    context.set_binding_shape(0, (batch_size, seq_len, hidden_size))\n",
    "\n",
    "    context.set_input_shape(\"attention_mask\", (batch_size, 1, seq_len, seq_len))\n",
    "    context.set_binding_shape(1, (batch_size, 1, seq_len, seq_len))\n",
    "    print(\"Set input shape completed\")\n",
    "\n",
    "    data = np.array(raw_data)\n",
    "    attention_mask = np.array(raw_attn_mask)\n",
    "\n",
    "    _, stream = cudart.cudaStreamCreate()\n",
    "#     print(\"Reshaping\")\n",
    "\n",
    "    inputH0 = np.ascontiguousarray(data.reshape(-1))\n",
    "    inputH1 = np.ascontiguousarray(attention_mask.reshape(-1))\n",
    "    outputH0 = np.empty(context.get_binding_shape(2), dtype=trt.nptype(engine.get_binding_dtype(2)))\n",
    "    outputH1 = np.empty(context.get_binding_shape(3), dtype=trt.nptype(engine.get_binding_dtype(3)))\n",
    "    outputH2 = np.empty(context.get_binding_shape(4), dtype=trt.nptype(engine.get_binding_dtype(4)))\n",
    "#     print(\"Reshaped\")\n",
    "\n",
    "    # initialize input and output data\n",
    "    _, inputD0 = cudart.cudaMallocAsync(inputH0.nbytes, stream)\n",
    "    _, inputD1 = cudart.cudaMallocAsync(inputH1.nbytes, stream)\n",
    "    _, outputD0 = cudart.cudaMallocAsync(outputH0.nbytes, stream)\n",
    "    _, outputD1 = cudart.cudaMallocAsync(outputH1.nbytes, stream)\n",
    "    _, outputD2 = cudart.cudaMallocAsync(outputH2.nbytes, stream)\n",
    "\n",
    "    # move input to device\n",
    "    cudart.cudaMemcpyAsync(inputD0, inputH0.ctypes.data, inputH0.nbytes, cudart.cudaMemcpyKind.cudaMemcpyHostToDevice, stream)\n",
    "    cudart.cudaMemcpyAsync(inputD1, inputH1.ctypes.data, inputH1.nbytes, cudart.cudaMemcpyKind.cudaMemcpyHostToDevice, stream)\n",
    "\n",
    "    # execute\n",
    "#     print(\"execute\")\n",
    "    context.execute_async_v2([int(inputD0), int(inputD1), int(outputD0), int(outputD1), int(outputD2)], stream)\n",
    "\n",
    "    # move output back to host\n",
    "    cudart.cudaMemcpyAsync(outputH0.ctypes.data, outputD0, outputH0.nbytes, cudart.cudaMemcpyKind.cudaMemcpyDeviceToHost, stream)\n",
    "    cudart.cudaMemcpyAsync(outputH1.ctypes.data, outputD1, outputH1.nbytes, cudart.cudaMemcpyKind.cudaMemcpyDeviceToHost, stream)\n",
    "    cudart.cudaMemcpyAsync(outputH2.ctypes.data, outputD2, outputH2.nbytes, cudart.cudaMemcpyKind.cudaMemcpyDeviceToHost, stream)\n",
    "\n",
    "    # wait for everythidden_sizeg\n",
    "    cudart.cudaStreamSynchronize(stream)\n",
    "\n",
    "    cudart.cudaStreamDestroy(stream)\n",
    "    cudart.cudaFree(inputD0)\n",
    "    cudart.cudaFree(outputD0)\n",
    "    cudart.cudaFree(outputD1)\n",
    "    cudart.cudaFree(outputD2)\n",
    "\n",
    "    return outputH0, outputH1, outputH2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c76b40bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Set input shape (4, 45, 4096)\n",
      "Set input shape completed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_16937/2974348848.py:12: DeprecationWarning: Use set_input_shape instead.\n",
      "  context.set_binding_shape(0, (batch_size, seq_len, hidden_size))\n",
      "/tmp/ipykernel_16937/2974348848.py:15: DeprecationWarning: Use set_input_shape instead.\n",
      "  context.set_binding_shape(1, (batch_size, 1, seq_len, seq_len))\n",
      "/tmp/ipykernel_16937/2974348848.py:26: DeprecationWarning: Use get_tensor_shape instead.\n",
      "  outputH0 = np.empty(context.get_binding_shape(2), dtype=trt.nptype(engine.get_binding_dtype(2)))\n",
      "/tmp/ipykernel_16937/2974348848.py:26: DeprecationWarning: Use get_tensor_dtype instead.\n",
      "  outputH0 = np.empty(context.get_binding_shape(2), dtype=trt.nptype(engine.get_binding_dtype(2)))\n",
      "/tmp/ipykernel_16937/2974348848.py:27: DeprecationWarning: Use get_tensor_shape instead.\n",
      "  outputH1 = np.empty(context.get_binding_shape(3), dtype=trt.nptype(engine.get_binding_dtype(3)))\n",
      "/tmp/ipykernel_16937/2974348848.py:27: DeprecationWarning: Use get_tensor_dtype instead.\n",
      "  outputH1 = np.empty(context.get_binding_shape(3), dtype=trt.nptype(engine.get_binding_dtype(3)))\n",
      "/tmp/ipykernel_16937/2974348848.py:28: DeprecationWarning: Use get_tensor_shape instead.\n",
      "  outputH2 = np.empty(context.get_binding_shape(4), dtype=trt.nptype(engine.get_binding_dtype(4)))\n",
      "/tmp/ipykernel_16937/2974348848.py:28: DeprecationWarning: Use get_tensor_dtype instead.\n",
      "  outputH2 = np.empty(context.get_binding_shape(4), dtype=trt.nptype(engine.get_binding_dtype(4)))\n"
     ]
    }
   ],
   "source": [
    "trt_output = trt_inference(batch_size, hidden_size, trt_engineStr, data, attention_mask)\n",
    "\n",
    "trt_query_states, trt_key_states, trt_value_states = trt_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3e063db3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 45, 4096)\n",
      "[[0.9997836 1.0243382 0.9666825 ... 1.0478249 1.0506202 1.0213784]\n",
      " [0.9997836 1.0243382 0.9666825 ... 1.0478249 1.0506202 1.0213784]\n",
      " [0.9997836 1.0243382 0.9666825 ... 1.0478249 1.0506202 1.0213784]\n",
      " ...\n",
      " [0.9997836 1.0243382 0.9666825 ... 1.0478249 1.0506202 1.0213784]\n",
      " [0.9997836 1.0243382 0.9666825 ... 1.0478249 1.0506202 1.0213784]\n",
      " [0.9997836 1.0243382 0.9666825 ... 1.0478249 1.0506202 1.0213784]]\n"
     ]
    }
   ],
   "source": [
    "print(trt_query_states.shape)\n",
    "print(trt_query_states[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b3018803",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 45, 4096)\n",
      "[[0.05878173 0.0845028  0.08121892 ... 0.07271458 0.08750308 0.07479976]\n",
      " [0.05878173 0.0845028  0.08121892 ... 0.07271458 0.08750308 0.07479976]\n",
      " [0.05878173 0.0845028  0.08121892 ... 0.07271458 0.08750308 0.07479976]\n",
      " ...\n",
      " [0.05878173 0.0845028  0.08121892 ... 0.07271458 0.08750308 0.07479976]\n",
      " [0.05878173 0.0845028  0.08121892 ... 0.07271458 0.08750308 0.07479976]\n",
      " [0.05878173 0.0845028  0.08121892 ... 0.07271458 0.08750308 0.07479976]]\n"
     ]
    }
   ],
   "source": [
    "print(trt_key_states.shape)\n",
    "print(trt_key_states[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "da537ebf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 45, 4096)\n",
      "[[1.001025  1.0160059 0.9785594 ... 1.0381885 1.0382291 1.0217683]\n",
      " [1.001025  1.0160059 0.9785594 ... 1.0381885 1.0382291 1.0217683]\n",
      " [1.001025  1.0160059 0.9785594 ... 1.0381885 1.0382291 1.0217683]\n",
      " ...\n",
      " [1.001025  1.0160059 0.9785594 ... 1.0381885 1.0382291 1.0217683]\n",
      " [1.001025  1.0160059 0.9785594 ... 1.0381885 1.0382291 1.0217683]\n",
      " [1.001025  1.0160059 0.9785594 ... 1.0381885 1.0382291 1.0217683]]\n"
     ]
    }
   ],
   "source": [
    "print(trt_value_states.shape)\n",
    "print(trt_value_states[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0d93e43",
   "metadata": {},
   "source": [
    "## Benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "57435e52",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "796e41fb",
   "metadata": {},
   "source": [
    "### Torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d3da2c85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "init rope 128 2048 10000.0\n",
      "model.layers.0.self_attn.q_proj.weight\n",
      "model.layers.0.self_attn.k_proj.weight\n",
      "model.layers.0.self_attn.v_proj.weight\n",
      "model.layers.0.self_attn.o_proj.weight\n",
      "model.layers.0.mlp.gate_proj.weight\n",
      "model.layers.0.mlp.down_proj.weight\n",
      "model.layers.0.mlp.up_proj.weight\n",
      "model.layers.0.input_layernorm.weight\n",
      "model.layers.0.post_attention_layernorm.weight\n",
      "model.layers.0.self_attn.rotary_emb.inv_freq\n",
      "tensor([1.0000e+00, 8.6596e-01, 7.4989e-01, 6.4938e-01, 5.6234e-01, 4.8697e-01,\n",
      "        4.2170e-01, 3.6517e-01, 3.1623e-01, 2.7384e-01, 2.3714e-01, 2.0535e-01,\n",
      "        1.7783e-01, 1.5399e-01, 1.3335e-01, 1.1548e-01, 1.0000e-01, 8.6596e-02,\n",
      "        7.4989e-02, 6.4938e-02, 5.6234e-02, 4.8697e-02, 4.2170e-02, 3.6517e-02,\n",
      "        3.1623e-02, 2.7384e-02, 2.3714e-02, 2.0535e-02, 1.7783e-02, 1.5399e-02,\n",
      "        1.3335e-02, 1.1548e-02, 1.0000e-02, 8.6596e-03, 7.4989e-03, 6.4938e-03,\n",
      "        5.6234e-03, 4.8697e-03, 4.2170e-03, 3.6517e-03, 3.1623e-03, 2.7384e-03,\n",
      "        2.3714e-03, 2.0535e-03, 1.7783e-03, 1.5399e-03, 1.3335e-03, 1.1548e-03,\n",
      "        1.0000e-03, 8.6596e-04, 7.4989e-04, 6.4938e-04, 5.6234e-04, 4.8697e-04,\n",
      "        4.2170e-04, 3.6517e-04, 3.1623e-04, 2.7384e-04, 2.3714e-04, 2.0535e-04,\n",
      "        1.7783e-04, 1.5399e-04, 1.3335e-04, 1.1548e-04])\n",
      "torch memory exe 17.1552441 ms\n"
     ]
    }
   ],
   "source": [
    "model = LlamaDecoderLayer(config)\n",
    "\n",
    "device = torch.device(\"cuda\")\n",
    "\n",
    "model.load(model_weight_dir)\n",
    "\n",
    "torch_start = time.time_ns()\n",
    "\n",
    "model = model.to(device)\n",
    "\n",
    "data_D = data.to(device)\n",
    "attention_mask_D = attention_mask.to(device)\n",
    "position_ids_D = position_ids.to(device)\n",
    "\n",
    "output = model(hidden_states=data_D,\n",
    "               attention_mask=attention_mask_D,\n",
    "               position_ids=position_ids_D,\n",
    "               past_key_value=past_key_value,\n",
    "               output_attentions=False,\n",
    "               use_cache=True)\n",
    "\n",
    "torch_complete = time.time_ns()\n",
    "\n",
    "print(\"torch memory exe\", (torch_complete - torch_start) / 10e6, \"ms\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "604c233f",
   "metadata": {},
   "source": [
    "### TensorRT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57cbc51d",
   "metadata": {},
   "source": [
    "### profile CPU/GPU time for tensorRT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "07dd9e77",
   "metadata": {},
   "outputs": [],
   "source": [
    "def profile_trt_inference(batch_size, hidden_size, engineString, raw_data, raw_attn_mask):\n",
    "    trt_prep_start = time.time_ns()\n",
    "#     print(engineString)\n",
    "#     print(\"Runtime\")\n",
    "    logger = trt.Logger(trt.Logger.ERROR)\n",
    "    engine = trt.Runtime(logger).deserialize_cuda_engine(engineString)\n",
    "    context = engine.create_execution_context()\n",
    "\n",
    "    # dynamic shape configure\n",
    "    print(\"Set input shape\", (batch_size, seq_len, hidden_size))\n",
    "\n",
    "    context.set_input_shape(\"hidden_states\", (batch_size, seq_len, hidden_size))\n",
    "    context.set_binding_shape(0, (batch_size, seq_len, hidden_size))\n",
    "\n",
    "    context.set_input_shape(\"attention_mask\", (batch_size, 1, seq_len, seq_len))\n",
    "    context.set_binding_shape(1, (batch_size, 1, seq_len, seq_len))\n",
    "    print(\"Set input shape completed\")\n",
    "\n",
    "    trt_prep_complete = time.time_ns()\n",
    "\n",
    "    data = np.array(raw_data)\n",
    "    attention_mask = np.array(raw_attn_mask)\n",
    "\n",
    "    memory_alloc_complete = time.time_ns()\n",
    "\n",
    "    _, stream = cudart.cudaStreamCreate()\n",
    "#     print(\"Reshaping\")\n",
    "\n",
    "    inputH0 = np.ascontiguousarray(data.reshape(-1))\n",
    "    inputH1 = np.ascontiguousarray(attention_mask.reshape(-1))\n",
    "    outputH0 = np.empty(context.get_binding_shape(2), dtype=trt.nptype(engine.get_binding_dtype(2)))\n",
    "    outputH1 = np.empty(context.get_binding_shape(3), dtype=trt.nptype(engine.get_binding_dtype(3)))\n",
    "    outputH2 = np.empty(context.get_binding_shape(4), dtype=trt.nptype(engine.get_binding_dtype(4)))\n",
    "#     print(\"Reshaped\")\n",
    "\n",
    "    # initialize input and output data\n",
    "    _, inputD0 = cudart.cudaMallocAsync(inputH0.nbytes, stream)\n",
    "    _, inputD1 = cudart.cudaMallocAsync(inputH1.nbytes, stream)\n",
    "    _, outputD0 = cudart.cudaMallocAsync(outputH0.nbytes, stream)\n",
    "    _, outputD1 = cudart.cudaMallocAsync(outputH1.nbytes, stream)\n",
    "    _, outputD2 = cudart.cudaMallocAsync(outputH2.nbytes, stream)\n",
    "\n",
    "    # move input to device\n",
    "    cudart.cudaMemcpyAsync(inputD0, inputH0.ctypes.data, inputH0.nbytes, cudart.cudaMemcpyKind.cudaMemcpyHostToDevice, stream)\n",
    "    cudart.cudaMemcpyAsync(inputD1, inputH1.ctypes.data, inputH1.nbytes, cudart.cudaMemcpyKind.cudaMemcpyHostToDevice, stream)\n",
    "\n",
    "    # execute\n",
    "#     print(\"execute\")\n",
    "    context.execute_async_v2([int(inputD0), int(inputD1), int(outputD0), int(outputD1), int(outputD2)], stream)\n",
    "\n",
    "    # move output back to host\n",
    "    cudart.cudaMemcpyAsync(outputH0.ctypes.data, outputD0, outputH0.nbytes, cudart.cudaMemcpyKind.cudaMemcpyDeviceToHost, stream)\n",
    "    cudart.cudaMemcpyAsync(outputH1.ctypes.data, outputD1, outputH1.nbytes, cudart.cudaMemcpyKind.cudaMemcpyDeviceToHost, stream)\n",
    "    cudart.cudaMemcpyAsync(outputH2.ctypes.data, outputD2, outputH2.nbytes, cudart.cudaMemcpyKind.cudaMemcpyDeviceToHost, stream)\n",
    "\n",
    "    # wait for everythidden_sizeg\n",
    "    cudart.cudaStreamSynchronize(stream)\n",
    "\n",
    "    cudart.cudaStreamDestroy(stream)\n",
    "    cudart.cudaFree(inputD0)\n",
    "    cudart.cudaFree(outputD0)\n",
    "    cudart.cudaFree(outputD1)\n",
    "    cudart.cudaFree(outputD2)\n",
    "\n",
    "    trt_complete = time.time_ns()\n",
    "\n",
    "    print(\"trt_prep\", (trt_prep_complete - trt_prep_start) / 10e6, \"ms\")\n",
    "    print(\"memory_alloc CPU\", (memory_alloc_complete - trt_prep_complete) / 10e6, \"ms\")\n",
    "    print(\"trt memory alloc & mv & exe\", (trt_complete - memory_alloc_complete) / 10e6, \"ms\")\n",
    "\n",
    "    return outputH0, outputH1, outputH2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5c01be0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Set input shape (4, 45, 4096)\n",
      "Set input shape completed\n",
      "trt_prep 18.3360933 ms\n",
      "memory_alloc CPU 0.072891 ms\n",
      "trt memory alloc & mv & exe 1.080253 ms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_16937/1604114800.py:13: DeprecationWarning: Use set_input_shape instead.\n",
      "  context.set_binding_shape(0, (batch_size, seq_len, hidden_size))\n",
      "/tmp/ipykernel_16937/1604114800.py:16: DeprecationWarning: Use set_input_shape instead.\n",
      "  context.set_binding_shape(1, (batch_size, 1, seq_len, seq_len))\n",
      "/tmp/ipykernel_16937/1604114800.py:31: DeprecationWarning: Use get_tensor_shape instead.\n",
      "  outputH0 = np.empty(context.get_binding_shape(2), dtype=trt.nptype(engine.get_binding_dtype(2)))\n",
      "/tmp/ipykernel_16937/1604114800.py:31: DeprecationWarning: Use get_tensor_dtype instead.\n",
      "  outputH0 = np.empty(context.get_binding_shape(2), dtype=trt.nptype(engine.get_binding_dtype(2)))\n",
      "/tmp/ipykernel_16937/1604114800.py:32: DeprecationWarning: Use get_tensor_shape instead.\n",
      "  outputH1 = np.empty(context.get_binding_shape(3), dtype=trt.nptype(engine.get_binding_dtype(3)))\n",
      "/tmp/ipykernel_16937/1604114800.py:32: DeprecationWarning: Use get_tensor_dtype instead.\n",
      "  outputH1 = np.empty(context.get_binding_shape(3), dtype=trt.nptype(engine.get_binding_dtype(3)))\n",
      "/tmp/ipykernel_16937/1604114800.py:33: DeprecationWarning: Use get_tensor_shape instead.\n",
      "  outputH2 = np.empty(context.get_binding_shape(4), dtype=trt.nptype(engine.get_binding_dtype(4)))\n",
      "/tmp/ipykernel_16937/1604114800.py:33: DeprecationWarning: Use get_tensor_dtype instead.\n",
      "  outputH2 = np.empty(context.get_binding_shape(4), dtype=trt.nptype(engine.get_binding_dtype(4)))\n"
     ]
    }
   ],
   "source": [
    "trt_output = profile_trt_inference(batch_size, hidden_size, trt_engineStr, data, attention_mask)\n",
    "\n",
    "trt_query_states, trt_key_states, trt_value_states = trt_output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d68ad43",
   "metadata": {},
   "source": [
    "## Is the result valid?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c0106319",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.allclose(output[0].clone().detach().cpu().numpy(), trt_value_states, atol=1e-04)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

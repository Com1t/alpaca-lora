{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3d8264d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from cuda import cudart\n",
    "import torch\n",
    "from torch import Tensor, nn\n",
    "import tensorrt as trt\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "81e3b82b",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = dict()\n",
    "\n",
    "batch_size, seq_len, hidden_size = 4, 45, 4096\n",
    "intermediate_size = 11008\n",
    "num_attention_heads = 32\n",
    "num_key_value_heads = 32\n",
    "max_position_embeddings = 2048\n",
    "rope_theta = 10000.0\n",
    "\n",
    "config[\"hidden_size\"] = hidden_size\n",
    "config[\"intermediate_size\"] = intermediate_size\n",
    "config[\"num_heads\"] = num_attention_heads\n",
    "config[\"head_dim\"] = config[\"hidden_size\"] // config[\"num_heads\"]\n",
    "config[\"num_key_value_heads\"] = num_key_value_heads\n",
    "config[\"num_key_value_groups\"] = config[\"num_heads\"] // config[\"num_key_value_heads\"]\n",
    "config[\"max_position_embeddings\"] = max_position_embeddings\n",
    "config[\"rope_theta\"] = rope_theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7063486c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = torch.ones(batch_size, seq_len, hidden_size)\n",
    "attention_mask = torch.ones(batch_size, 1, seq_len, seq_len)\n",
    "position_ids = torch.arange(0, seq_len)\n",
    "position_ids = position_ids.repeat(batch_size, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6960c816",
   "metadata": {},
   "source": [
    "## tensorRT dynamic shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "85c26cb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# seq length is not specified, since it is a dynamic size\n",
    "def trt_create(batch_size, hidden_size, intermediate_size, model):\n",
    "    \n",
    "    logger = trt.Logger(trt.Logger.ERROR)\n",
    "    builder = trt.Builder(logger)\n",
    "\n",
    "    network = builder.create_network(1 << int(trt.NetworkDefinitionCreationFlag.EXPLICIT_BATCH))\n",
    "    config = builder.create_builder_config()\n",
    "\n",
    "    # input\n",
    "    inputT0 = network.add_input('inputT0', trt.DataType.FLOAT, (batch_size, -1, hidden_size))\n",
    "\n",
    "    # dynamic shape optimization\n",
    "    profile = builder.create_optimization_profile();\n",
    "    profile.set_shape(\"inputT0\", (batch_size, 1, hidden_size), (batch_size, 1, hidden_size), (batch_size, 45, hidden_size))\n",
    "    config.add_optimization_profile(profile)\n",
    "    \n",
    "    q_proj_weight = np.ones((hidden_size, model['head_dim']), dtype=np.float32)\n",
    "    q_proj_weight = q_proj_weight.reshape(1, hidden_size, model['head_dim'])\n",
    "\n",
    "    input_shape = [1, hidden_size, model['head_dim']]\n",
    "    \n",
    "    q_proj_weight_layer = network.add_constant(shape=input_shape, weights=trt.Weights(q_proj_weight))\n",
    "\n",
    "    q_proj_layer = network.add_matrix_multiply(inputT0, trt.MatrixOperation.NONE, q_proj_weight_layer.get_output(0), trt.MatrixOperation.NONE)\n",
    "\n",
    "    # output\n",
    "    network.mark_output(q_proj_layer.get_output(0))\n",
    "\n",
    "    engineString = builder.build_serialized_network(network, config)\n",
    "    \n",
    "    return engineString"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "01c411f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "trt_engineStr = trt_create(batch_size, hidden_size, intermediate_size, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c866cfce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def trt_inference(batch_size, hidden_size, engineString, raw_data):\n",
    "#     print(engineString)\n",
    "#     print(\"Runtime\")\n",
    "    logger = trt.Logger(trt.Logger.ERROR)\n",
    "    engine = trt.Runtime(logger).deserialize_cuda_engine(engineString)\n",
    "    context = engine.create_execution_context()\n",
    "\n",
    "#     dynamic shape configure\n",
    "    print(\"Set input shape\", (batch_size, 15, hidden_size))\n",
    "    context.set_input_shape(\"inputT0\", (4, 15, 4096))\n",
    "    context.set_binding_shape(0, (batch_size, 15, hidden_size))\n",
    "    origin_inputshape = context.get_binding_shape(0)\n",
    "\n",
    "    print(\"Set input shape completed\")\n",
    "\n",
    "    data = np.array(raw_data)\n",
    "\n",
    "    _, stream = cudart.cudaStreamCreate()\n",
    "#     print(\"Reshaping\")\n",
    "\n",
    "    inputH0 = np.ascontiguousarray(data.reshape(-1))\n",
    "    outputH0 = np.empty(context.get_binding_shape(1), dtype=trt.nptype(engine.get_binding_dtype(1)))\n",
    "#     print(\"Reshaped\")\n",
    "\n",
    "    # initialize input and output data\n",
    "    _, inputD0 = cudart.cudaMallocAsync(inputH0.nbytes, stream)\n",
    "    _, outputD0 = cudart.cudaMallocAsync(outputH0.nbytes, stream)\n",
    "\n",
    "    # move input to device\n",
    "    cudart.cudaMemcpyAsync(inputD0, inputH0.ctypes.data, inputH0.nbytes, cudart.cudaMemcpyKind.cudaMemcpyHostToDevice, stream)\n",
    "\n",
    "    # execute\n",
    "#     print(\"execute\")\n",
    "    context.execute_async_v2([int(inputD0), int(outputD0)], stream)\n",
    "\n",
    "    # move output back to host\n",
    "    cudart.cudaMemcpyAsync(outputH0.ctypes.data, outputD0, outputH0.nbytes, cudart.cudaMemcpyKind.cudaMemcpyDeviceToHost, stream)\n",
    "\n",
    "    # wait for everythidden_sizeg\n",
    "    cudart.cudaStreamSynchronize(stream)\n",
    "\n",
    "    cudart.cudaStreamDestroy(stream)\n",
    "    cudart.cudaFree(inputD0)\n",
    "    cudart.cudaFree(outputD0)\n",
    "\n",
    "    return outputH0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8fc529df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 45, 4096])\n",
      "Set input shape (4, 15, 4096)\n",
      "Set input shape completed\n",
      "output_trt : (4, 15, 128)\n",
      "[[[4096. 4096. 4096. ... 4096. 4096. 4096.]\n",
      "  [4096. 4096. 4096. ... 4096. 4096. 4096.]\n",
      "  [4096. 4096. 4096. ... 4096. 4096. 4096.]\n",
      "  ...\n",
      "  [4096. 4096. 4096. ... 4096. 4096. 4096.]\n",
      "  [4096. 4096. 4096. ... 4096. 4096. 4096.]\n",
      "  [4096. 4096. 4096. ... 4096. 4096. 4096.]]\n",
      "\n",
      " [[4096. 4096. 4096. ... 4096. 4096. 4096.]\n",
      "  [4096. 4096. 4096. ... 4096. 4096. 4096.]\n",
      "  [4096. 4096. 4096. ... 4096. 4096. 4096.]\n",
      "  ...\n",
      "  [4096. 4096. 4096. ... 4096. 4096. 4096.]\n",
      "  [4096. 4096. 4096. ... 4096. 4096. 4096.]\n",
      "  [4096. 4096. 4096. ... 4096. 4096. 4096.]]\n",
      "\n",
      " [[4096. 4096. 4096. ... 4096. 4096. 4096.]\n",
      "  [4096. 4096. 4096. ... 4096. 4096. 4096.]\n",
      "  [4096. 4096. 4096. ... 4096. 4096. 4096.]\n",
      "  ...\n",
      "  [4096. 4096. 4096. ... 4096. 4096. 4096.]\n",
      "  [4096. 4096. 4096. ... 4096. 4096. 4096.]\n",
      "  [4096. 4096. 4096. ... 4096. 4096. 4096.]]\n",
      "\n",
      " [[4096. 4096. 4096. ... 4096. 4096. 4096.]\n",
      "  [4096. 4096. 4096. ... 4096. 4096. 4096.]\n",
      "  [4096. 4096. 4096. ... 4096. 4096. 4096.]\n",
      "  ...\n",
      "  [4096. 4096. 4096. ... 4096. 4096. 4096.]\n",
      "  [4096. 4096. 4096. ... 4096. 4096. 4096.]\n",
      "  [4096. 4096. 4096. ... 4096. 4096. 4096.]]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2082/3792254111.py:11: DeprecationWarning: Use set_input_shape instead.\n",
      "  context.set_binding_shape(0, (batch_size, 15, hidden_size))\n",
      "/tmp/ipykernel_2082/3792254111.py:12: DeprecationWarning: Use get_tensor_shape instead.\n",
      "  origin_inputshape = context.get_binding_shape(0)\n",
      "/tmp/ipykernel_2082/3792254111.py:22: DeprecationWarning: Use get_tensor_shape instead.\n",
      "  outputH0 = np.empty(context.get_binding_shape(1), dtype=trt.nptype(engine.get_binding_dtype(1)))\n",
      "/tmp/ipykernel_2082/3792254111.py:22: DeprecationWarning: Use get_tensor_dtype instead.\n",
      "  outputH0 = np.empty(context.get_binding_shape(1), dtype=trt.nptype(engine.get_binding_dtype(1)))\n"
     ]
    }
   ],
   "source": [
    "data = data.reshape(batch_size, seq_len, hidden_size)\n",
    "print(data.shape)\n",
    "\n",
    "trt_output = trt_inference(batch_size, hidden_size, trt_engineStr, data)\n",
    "\n",
    "# trt_output = trt_output.reshape(batch_size, seq_len, hidden_size)\n",
    "print(\"output_trt :\", trt_output.shape)\n",
    "print(trt_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb2dd3fc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

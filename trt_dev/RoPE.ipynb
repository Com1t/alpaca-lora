{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "562e5097",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from cuda import cudart\n",
    "import torch\n",
    "from torch import Tensor, nn\n",
    "import tensorrt as trt\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e32d3eb9",
   "metadata": {},
   "source": [
    "## Generate input and data shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4d30be86",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = dict()\n",
    "\n",
    "batch_size, seq_len, hidden_size = 4, 45, 4096\n",
    "intermediate_size = 11008\n",
    "num_attention_heads = 32\n",
    "num_key_value_heads = 32\n",
    "max_position_embeddings = 2048\n",
    "rope_theta = 10000.0\n",
    "\n",
    "config[\"hidden_size\"] = hidden_size\n",
    "config[\"intermediate_size\"] = intermediate_size\n",
    "config[\"num_heads\"] = num_attention_heads\n",
    "config[\"head_dim\"] = config[\"hidden_size\"] // config[\"num_heads\"]\n",
    "config[\"num_key_value_heads\"] = num_key_value_heads\n",
    "config[\"num_key_value_groups\"] = config[\"num_heads\"] // config[\"num_key_value_heads\"]\n",
    "config[\"max_position_embeddings\"] = max_position_embeddings\n",
    "config[\"rope_theta\"] = rope_theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "80092fba",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = torch.ones(batch_size, seq_len, hidden_size)\n",
    "attention_mask = torch.ones(batch_size, 1, seq_len, seq_len)\n",
    "position_ids = torch.arange(0, seq_len)\n",
    "position_ids = position_ids.repeat(batch_size, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61171e7e",
   "metadata": {},
   "source": [
    "## torch attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cadbfad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def repeat_kv(hidden_states: torch.Tensor, n_rep: int) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    This is the equivalent of torch.repeat_interleave(x, dim=1, repeats=n_rep). The hidden states go from (batch,\n",
    "    num_key_value_heads, seqlen, head_dim) to (batch, num_attention_heads, seqlen, head_dim)\n",
    "    \n",
    "    repeat at the second dimension\n",
    "    \"\"\"\n",
    "    batch, num_key_value_heads, slen, head_dim = hidden_states.shape\n",
    "    if n_rep == 1:\n",
    "        return hidden_states\n",
    "    hidden_states = hidden_states[:, :, None, :, :].expand(batch, num_key_value_heads, n_rep, slen, head_dim)\n",
    "    return hidden_states.reshape(batch, num_key_value_heads * n_rep, slen, head_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "869e6377",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LlamaRotaryEmbedding(torch.nn.Module):\n",
    "    def __init__(self, dim, max_position_embeddings=2048, base=10000, device=None):\n",
    "        super().__init__()\n",
    "\n",
    "        self.dim = dim\n",
    "        self.max_position_embeddings = max_position_embeddings\n",
    "        self.base = base\n",
    "        inv_freq = 1.0 / (self.base ** (torch.arange(0, self.dim, 2).float().to(device) / self.dim))\n",
    "        \n",
    "        self.register_buffer(\"inv_freq\", inv_freq, persistent=False)\n",
    "\n",
    "        t = torch.arange(max_position_embeddings, device=device, dtype=self.inv_freq.dtype)\n",
    "\n",
    "        freqs = torch.einsum(\"i,j->ij\", t, self.inv_freq)\n",
    "        # Different from paper, but it uses a different permutation in order to obtain the same calculation\n",
    "        emb = torch.cat((freqs, freqs), dim=-1)\n",
    "        self.register_buffer(\"cos_cached\", emb.cos()[None, None, :, :].to(torch.get_default_dtype()), persistent=False)\n",
    "        self.register_buffer(\"sin_cached\", emb.sin()[None, None, :, :].to(torch.get_default_dtype()), persistent=False)\n",
    "\n",
    "    def rotate_half(self, x):\n",
    "        \"\"\"Rotates half the hidden dims of the input.\"\"\"\n",
    "        x1 = x[..., : x.shape[-1] // 2]\n",
    "        x2 = x[..., x.shape[-1] // 2 :]\n",
    "        return torch.cat((-x2, x1), dim=-1)\n",
    "\n",
    "    def forward(self, q, k, v, position_ids, seq_len=None):\n",
    "        # v: [bs, num_attention_heads, seq_len, head_size]\n",
    "        cos = self.cos_cached[:, :, :seq_len, ...].to(dtype=v.dtype)\n",
    "        sin = self.sin_cached[:, :, :seq_len, ...].to(dtype=v.dtype)\n",
    "        cos = cos.squeeze(1).squeeze(0)  # [seq_len, dim]\n",
    "        sin = sin.squeeze(1).squeeze(0)  # [seq_len, dim]\n",
    "        cos = cos[position_ids].unsqueeze(1)  # [bs, 1, seq_len, dim]\n",
    "        sin = sin[position_ids].unsqueeze(1)  # [bs, 1, seq_len, dim]\n",
    "\n",
    "        # The first two dimensions of cos and sin are always 1, so we can `squeeze` them.\n",
    "        q_embed = (q * cos) + (self.rotate_half(q) * sin)\n",
    "        k_embed = (k * cos) + (self.rotate_half(k) * sin)\n",
    "        return q_embed, k_embed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b211018b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LlamaAttention(nn.Module):\n",
    "    \"\"\"Multi-headed attention from 'Attention Is All You Need' paper\"\"\"\n",
    "\n",
    "    def __init__(self, config: dict):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.hidden_size = config[\"hidden_size\"]\n",
    "        self.num_heads = config[\"num_heads\"]\n",
    "        self.head_dim = config[\"hidden_size\"] // config[\"num_heads\"]\n",
    "        self.num_key_value_heads = config[\"num_key_value_heads\"]\n",
    "        self.num_key_value_groups = config[\"num_heads\"] // config[\"num_key_value_heads\"]\n",
    "        self.max_position_embeddings = config[\"max_position_embeddings\"]\n",
    "        self.rope_theta = config[\"rope_theta\"]\n",
    "\n",
    "        if (self.head_dim * self.num_heads) != self.hidden_size:\n",
    "            raise ValueError(\n",
    "                f\"hidden_size must be divisible by num_heads (got `hidden_size`: {self.hidden_size}\"\n",
    "                f\" and `num_heads`: {self.num_heads}).\"\n",
    "            )\n",
    "        self.q_proj = nn.Linear(self.hidden_size, self.num_heads * self.head_dim, bias=False)\n",
    "        self.k_proj = nn.Linear(self.hidden_size, self.num_key_value_heads * self.head_dim, bias=False)\n",
    "        self.v_proj = nn.Linear(self.hidden_size, self.num_key_value_heads * self.head_dim, bias=False)\n",
    "        self.o_proj = nn.Linear(self.num_heads * self.head_dim, self.hidden_size, bias=False)\n",
    "        self._init_rope()\n",
    "\n",
    "    def _init_rope(self):\n",
    "        print(\n",
    "            \"init rope\",\n",
    "            self.head_dim,\n",
    "            self.max_position_embeddings,\n",
    "            self.rope_theta,\n",
    "        )\n",
    "        self.rotary_emb = LlamaRotaryEmbedding(\n",
    "            self.head_dim,\n",
    "            max_position_embeddings=self.max_position_embeddings,\n",
    "            base=self.rope_theta,\n",
    "        )\n",
    "\n",
    "    def _shape(self, tensor: torch.Tensor, seq_len: int, bsz: int):\n",
    "        return tensor.view(bsz, seq_len, self.num_heads, self.head_dim).transpose(1, 2).contiguous()\n",
    "\n",
    "\n",
    "    def load(self, dir):\n",
    "        weights = torch.load(dir)\n",
    "        self_attn_weights = dict()\n",
    "        for key in weights.keys():\n",
    "            print(key)\n",
    "            if key == \"model.layers.18.self_attn.rotary_emb.inv_freq\":\n",
    "                print(weights[key])\n",
    "                continue\n",
    "            if key.split(\".\")[3] == \"self_attn\":\n",
    "                self_attn_weights[key[key.find(key.split(\".\")[4]):]] = weights[key]\n",
    "\n",
    "        self.load_state_dict(self_attn_weights)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        hidden_states: torch.Tensor,\n",
    "        attention_mask: None,\n",
    "        position_ids: None,\n",
    "        past_key_value: None,\n",
    "        output_attentions: bool = False,\n",
    "        use_cache: bool = False,\n",
    "    ):\n",
    "        # bsz = batch size; q_len = query length; _ = hidden size\n",
    "        bsz, q_len, _ = hidden_states.size()\n",
    "\n",
    "        # do projection\n",
    "        query_states = self.q_proj(hidden_states)\n",
    "        key_states = self.k_proj(hidden_states)\n",
    "        value_states = self.v_proj(hidden_states)\n",
    "\n",
    "        # reshape\n",
    "        query_states = query_states.view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        key_states = key_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n",
    "        value_states = value_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n",
    "\n",
    "        #####################################################\n",
    "        # in hugging face, they do have kv cache, however, they don't have other attention optimization\n",
    "        # this could be done directly in tensorRT by using dynamic shape\n",
    "        kv_seq_len = key_states.shape[-2]\n",
    "        if past_key_value is not None:\n",
    "            kv_seq_len += past_key_value[0].shape[-2]\n",
    "\n",
    "        query_states, key_states = self.rotary_emb(query_states, key_states, value_states, position_ids, seq_len=q_len)\n",
    "\n",
    "        if past_key_value is not None:\n",
    "            # reuse k, v, self_attention\n",
    "            key_states = torch.cat([past_key_value[0], key_states], dim=2)\n",
    "            value_states = torch.cat([past_key_value[1], value_states], dim=2)\n",
    "\n",
    "        past_key_value = (key_states, value_states) if use_cache else None\n",
    "\n",
    "        print(self.num_key_value_groups)\n",
    "        # repeat k/v heads if n_kv_heads < n_heads\n",
    "        key_states = repeat_kv(key_states, self.num_key_value_groups)\n",
    "        value_states = repeat_kv(value_states, self.num_key_value_groups)\n",
    "        #####################################################\n",
    "\n",
    "        attn_weights = torch.matmul(query_states, key_states.transpose(2, 3)) / math.sqrt(self.head_dim)\n",
    "\n",
    "        attn_weights = attn_weights + attention_mask\n",
    "\n",
    "        # upcast attention to fp32\n",
    "        attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query_states.dtype)\n",
    "        attn_output = torch.matmul(attn_weights, value_states)\n",
    "\n",
    "        attn_output = attn_output.transpose(1, 2).contiguous()\n",
    "        attn_output = attn_output.reshape(bsz, q_len, self.hidden_size)\n",
    "\n",
    "        attn_output = self.o_proj(attn_output)\n",
    "\n",
    "        if not output_attentions:\n",
    "            attn_weights = None\n",
    "\n",
    "        return attn_output, attn_weights, past_key_value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "155298f1",
   "metadata": {},
   "source": [
    "## Test torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "36f627c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "init rope 128 2048 10000.0\n",
      "model.layers.18.self_attn.q_proj.weight\n",
      "model.layers.18.self_attn.k_proj.weight\n",
      "model.layers.18.self_attn.v_proj.weight\n",
      "model.layers.18.self_attn.o_proj.weight\n",
      "model.layers.18.mlp.gate_proj.weight\n",
      "model.layers.18.mlp.down_proj.weight\n",
      "model.layers.18.mlp.up_proj.weight\n",
      "model.layers.18.input_layernorm.weight\n",
      "model.layers.18.post_attention_layernorm.weight\n",
      "model.layers.18.self_attn.rotary_emb.inv_freq\n",
      "tensor([1.0000e+00, 8.6596e-01, 7.4989e-01, 6.4938e-01, 5.6234e-01, 4.8697e-01,\n",
      "        4.2170e-01, 3.6517e-01, 3.1623e-01, 2.7384e-01, 2.3714e-01, 2.0535e-01,\n",
      "        1.7783e-01, 1.5399e-01, 1.3335e-01, 1.1548e-01, 1.0000e-01, 8.6596e-02,\n",
      "        7.4989e-02, 6.4938e-02, 5.6234e-02, 4.8697e-02, 4.2170e-02, 3.6517e-02,\n",
      "        3.1623e-02, 2.7384e-02, 2.3714e-02, 2.0535e-02, 1.7783e-02, 1.5399e-02,\n",
      "        1.3335e-02, 1.1548e-02, 1.0000e-02, 8.6596e-03, 7.4989e-03, 6.4938e-03,\n",
      "        5.6234e-03, 4.8697e-03, 4.2170e-03, 3.6517e-03, 3.1623e-03, 2.7384e-03,\n",
      "        2.3714e-03, 2.0535e-03, 1.7783e-03, 1.5399e-03, 1.3335e-03, 1.1548e-03,\n",
      "        1.0000e-03, 8.6596e-04, 7.4989e-04, 6.4938e-04, 5.6234e-04, 4.8697e-04,\n",
      "        4.2170e-04, 3.6517e-04, 3.1623e-04, 2.7384e-04, 2.3714e-04, 2.0535e-04,\n",
      "        1.7783e-04, 1.5399e-04, 1.3335e-04, 1.1548e-04])\n",
      "kv_seq_len 45 tensor([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
      "         18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,\n",
      "         36, 37, 38, 39, 40, 41, 42, 43, 44],\n",
      "        [ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
      "         18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,\n",
      "         36, 37, 38, 39, 40, 41, 42, 43, 44],\n",
      "        [ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
      "         18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,\n",
      "         36, 37, 38, 39, 40, 41, 42, 43, 44],\n",
      "        [ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
      "         18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,\n",
      "         36, 37, 38, 39, 40, 41, 42, 43, 44]], device='cuda:0')\n",
      "1\n",
      "(tensor([[[-0.3422, -1.3077,  3.2849,  ..., -1.2631, -0.5749,  1.4960],\n",
      "         [-0.3422, -1.3077,  3.2849,  ..., -1.2631, -0.5749,  1.4960],\n",
      "         [-0.3422, -1.3077,  3.2849,  ..., -1.2631, -0.5749,  1.4960],\n",
      "         ...,\n",
      "         [-0.3422, -1.3077,  3.2849,  ..., -1.2631, -0.5749,  1.4960],\n",
      "         [-0.3422, -1.3077,  3.2849,  ..., -1.2631, -0.5749,  1.4960],\n",
      "         [-0.3422, -1.3077,  3.2849,  ..., -1.2631, -0.5749,  1.4960]],\n",
      "\n",
      "        [[-0.3422, -1.3077,  3.2849,  ..., -1.2631, -0.5749,  1.4960],\n",
      "         [-0.3422, -1.3077,  3.2849,  ..., -1.2631, -0.5749,  1.4960],\n",
      "         [-0.3422, -1.3077,  3.2849,  ..., -1.2631, -0.5749,  1.4960],\n",
      "         ...,\n",
      "         [-0.3422, -1.3077,  3.2849,  ..., -1.2631, -0.5749,  1.4960],\n",
      "         [-0.3422, -1.3077,  3.2849,  ..., -1.2631, -0.5749,  1.4960],\n",
      "         [-0.3422, -1.3077,  3.2849,  ..., -1.2631, -0.5749,  1.4960]],\n",
      "\n",
      "        [[-0.3422, -1.3077,  3.2849,  ..., -1.2631, -0.5749,  1.4960],\n",
      "         [-0.3422, -1.3077,  3.2849,  ..., -1.2631, -0.5749,  1.4960],\n",
      "         [-0.3422, -1.3077,  3.2849,  ..., -1.2631, -0.5749,  1.4960],\n",
      "         ...,\n",
      "         [-0.3422, -1.3077,  3.2849,  ..., -1.2631, -0.5749,  1.4960],\n",
      "         [-0.3422, -1.3077,  3.2849,  ..., -1.2631, -0.5749,  1.4960],\n",
      "         [-0.3422, -1.3077,  3.2849,  ..., -1.2631, -0.5749,  1.4960]],\n",
      "\n",
      "        [[-0.3422, -1.3077,  3.2849,  ..., -1.2631, -0.5749,  1.4960],\n",
      "         [-0.3422, -1.3077,  3.2849,  ..., -1.2631, -0.5749,  1.4960],\n",
      "         [-0.3422, -1.3077,  3.2849,  ..., -1.2631, -0.5749,  1.4960],\n",
      "         ...,\n",
      "         [-0.3422, -1.3077,  3.2849,  ..., -1.2631, -0.5749,  1.4960],\n",
      "         [-0.3422, -1.3077,  3.2849,  ..., -1.2631, -0.5749,  1.4960],\n",
      "         [-0.3422, -1.3077,  3.2849,  ..., -1.2631, -0.5749,  1.4960]]],\n",
      "       device='cuda:0', grad_fn=<UnsafeViewBackward0>), None, (tensor([[[[-0.7064, -1.8095,  0.9089,  ..., -1.7266,  0.6913,  1.4727],\n",
      "          [-0.0502, -0.7395,  1.6062,  ..., -1.7264,  0.6918,  1.4726],\n",
      "          [ 0.6521,  0.8512,  1.4418,  ..., -1.7262,  0.6923,  1.4726],\n",
      "          ...,\n",
      "          [-0.0785, -0.9856,  1.0159,  ..., -1.7175,  0.7131,  1.4702],\n",
      "          [-0.7198, -1.8729,  1.6322,  ..., -1.7172,  0.7136,  1.4701],\n",
      "          [-0.6993, -1.4413,  1.3729,  ..., -1.7170,  0.7141,  1.4700]],\n",
      "\n",
      "         [[ 2.4114,  1.3076, -0.4585,  ...,  1.1612,  0.0569, -2.3169],\n",
      "          [ 1.3495,  1.7765,  0.1263,  ...,  1.1614,  0.0568, -2.3172],\n",
      "          [-0.9531,  0.9944,  0.6433,  ...,  1.1616,  0.0566, -2.3175],\n",
      "          ...,\n",
      "          [-1.0153, -0.8707, -0.4032,  ...,  1.1712,  0.0510, -2.3288],\n",
      "          [ 1.2925,  0.6257,  0.1902,  ...,  1.1715,  0.0508, -2.3291],\n",
      "          [ 2.4120,  1.6815,  0.6815,  ...,  1.1717,  0.0507, -2.3294]],\n",
      "\n",
      "         [[ 1.2487,  1.4503,  1.4415,  ..., -1.7049, -0.1272, -1.5117],\n",
      "          [-0.5350,  1.6345,  0.6317,  ..., -1.7049, -0.1275, -1.5116],\n",
      "          [-1.8268,  0.6677, -0.5170,  ..., -1.7050, -0.1279, -1.5116],\n",
      "          ...,\n",
      "          [ 0.8181, -0.5378,  1.3876,  ..., -1.7062, -0.1429, -1.5083],\n",
      "          [ 1.8889,  0.8907,  0.5155,  ..., -1.7062, -0.1432, -1.5083],\n",
      "          [ 1.2231,  1.6919, -0.6332,  ..., -1.7063, -0.1436, -1.5082]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-1.2882, -1.0448,  0.1326,  ...,  0.3346,  1.6627,  2.8040],\n",
      "          [-0.6226,  0.3435,  0.4637,  ...,  0.3347,  1.6623,  2.8039],\n",
      "          [ 0.6154,  1.4900,  0.5460,  ...,  0.3348,  1.6618,  2.8037],\n",
      "          ...,\n",
      "          [ 0.4353, -1.5512,  0.1750,  ...,  0.3383,  1.6429,  2.7970],\n",
      "          [-0.7877, -1.5329,  0.4863,  ...,  0.3384,  1.6424,  2.7969],\n",
      "          [-1.2865, -0.4352,  0.5368,  ...,  0.3385,  1.6419,  2.7967]],\n",
      "\n",
      "         [[ 0.1599,  0.4762,  2.1982,  ..., -1.6771, -2.3109,  3.5271],\n",
      "          [-0.5995,  0.8749,  1.7686,  ..., -1.6765, -2.3107,  3.5272],\n",
      "          [-0.8077,  0.6575,  0.3901,  ..., -1.6760, -2.3106,  3.5273],\n",
      "          ...,\n",
      "          [ 0.6831, -0.6077,  2.2099,  ..., -1.6539, -2.3042,  3.5317],\n",
      "          [ 0.7668,  0.0942,  1.6574,  ..., -1.6534, -2.3040,  3.5318],\n",
      "          [ 0.1455,  0.7298,  0.2158,  ..., -1.6528, -2.3038,  3.5319]],\n",
      "\n",
      "         [[ 0.4606, -0.7993,  0.6332,  ...,  1.1837,  5.8394, -2.0186],\n",
      "          [ 1.4908, -0.3157,  0.1025,  ...,  1.1841,  5.8397, -2.0186],\n",
      "          [ 1.1504,  0.3902, -0.4832,  ...,  1.1846,  5.8400, -2.0186],\n",
      "          ...,\n",
      "          [-1.5370, -0.4493,  0.5891,  ...,  1.2025,  5.8520, -2.0179],\n",
      "          [-0.9719, -0.8337,  0.0370,  ...,  1.2030,  5.8523, -2.0179],\n",
      "          [ 0.4867, -0.6310, -0.5349,  ...,  1.2034,  5.8526, -2.0178]]],\n",
      "\n",
      "\n",
      "        [[[-0.7064, -1.8095,  0.9089,  ..., -1.7266,  0.6913,  1.4727],\n",
      "          [-0.0502, -0.7395,  1.6062,  ..., -1.7264,  0.6918,  1.4726],\n",
      "          [ 0.6521,  0.8512,  1.4418,  ..., -1.7262,  0.6923,  1.4726],\n",
      "          ...,\n",
      "          [-0.0785, -0.9856,  1.0159,  ..., -1.7175,  0.7131,  1.4702],\n",
      "          [-0.7198, -1.8729,  1.6322,  ..., -1.7172,  0.7136,  1.4701],\n",
      "          [-0.6993, -1.4413,  1.3729,  ..., -1.7170,  0.7141,  1.4700]],\n",
      "\n",
      "         [[ 2.4114,  1.3076, -0.4585,  ...,  1.1612,  0.0569, -2.3169],\n",
      "          [ 1.3495,  1.7765,  0.1263,  ...,  1.1614,  0.0568, -2.3172],\n",
      "          [-0.9531,  0.9944,  0.6433,  ...,  1.1616,  0.0566, -2.3175],\n",
      "          ...,\n",
      "          [-1.0153, -0.8707, -0.4032,  ...,  1.1712,  0.0510, -2.3288],\n",
      "          [ 1.2925,  0.6257,  0.1902,  ...,  1.1715,  0.0508, -2.3291],\n",
      "          [ 2.4120,  1.6815,  0.6815,  ...,  1.1717,  0.0507, -2.3294]],\n",
      "\n",
      "         [[ 1.2487,  1.4503,  1.4415,  ..., -1.7049, -0.1272, -1.5117],\n",
      "          [-0.5350,  1.6345,  0.6317,  ..., -1.7049, -0.1275, -1.5116],\n",
      "          [-1.8268,  0.6677, -0.5170,  ..., -1.7050, -0.1279, -1.5116],\n",
      "          ...,\n",
      "          [ 0.8181, -0.5378,  1.3876,  ..., -1.7062, -0.1429, -1.5083],\n",
      "          [ 1.8889,  0.8907,  0.5155,  ..., -1.7062, -0.1432, -1.5083],\n",
      "          [ 1.2231,  1.6919, -0.6332,  ..., -1.7063, -0.1436, -1.5082]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-1.2882, -1.0448,  0.1326,  ...,  0.3346,  1.6627,  2.8040],\n",
      "          [-0.6226,  0.3435,  0.4637,  ...,  0.3347,  1.6623,  2.8039],\n",
      "          [ 0.6154,  1.4900,  0.5460,  ...,  0.3348,  1.6618,  2.8037],\n",
      "          ...,\n",
      "          [ 0.4353, -1.5512,  0.1750,  ...,  0.3383,  1.6429,  2.7970],\n",
      "          [-0.7877, -1.5329,  0.4863,  ...,  0.3384,  1.6424,  2.7969],\n",
      "          [-1.2865, -0.4352,  0.5368,  ...,  0.3385,  1.6419,  2.7967]],\n",
      "\n",
      "         [[ 0.1599,  0.4762,  2.1982,  ..., -1.6771, -2.3109,  3.5271],\n",
      "          [-0.5995,  0.8749,  1.7686,  ..., -1.6765, -2.3107,  3.5272],\n",
      "          [-0.8077,  0.6575,  0.3901,  ..., -1.6760, -2.3106,  3.5273],\n",
      "          ...,\n",
      "          [ 0.6831, -0.6077,  2.2099,  ..., -1.6539, -2.3042,  3.5317],\n",
      "          [ 0.7668,  0.0942,  1.6574,  ..., -1.6534, -2.3040,  3.5318],\n",
      "          [ 0.1455,  0.7298,  0.2158,  ..., -1.6528, -2.3038,  3.5319]],\n",
      "\n",
      "         [[ 0.4606, -0.7993,  0.6332,  ...,  1.1837,  5.8394, -2.0186],\n",
      "          [ 1.4908, -0.3157,  0.1025,  ...,  1.1841,  5.8397, -2.0186],\n",
      "          [ 1.1504,  0.3902, -0.4832,  ...,  1.1846,  5.8400, -2.0186],\n",
      "          ...,\n",
      "          [-1.5370, -0.4493,  0.5891,  ...,  1.2025,  5.8520, -2.0179],\n",
      "          [-0.9719, -0.8337,  0.0370,  ...,  1.2030,  5.8523, -2.0179],\n",
      "          [ 0.4867, -0.6310, -0.5349,  ...,  1.2034,  5.8526, -2.0178]]],\n",
      "\n",
      "\n",
      "        [[[-0.7064, -1.8095,  0.9089,  ..., -1.7266,  0.6913,  1.4727],\n",
      "          [-0.0502, -0.7395,  1.6062,  ..., -1.7264,  0.6918,  1.4726],\n",
      "          [ 0.6521,  0.8512,  1.4418,  ..., -1.7262,  0.6923,  1.4726],\n",
      "          ...,\n",
      "          [-0.0785, -0.9856,  1.0159,  ..., -1.7175,  0.7131,  1.4702],\n",
      "          [-0.7198, -1.8729,  1.6322,  ..., -1.7172,  0.7136,  1.4701],\n",
      "          [-0.6993, -1.4413,  1.3729,  ..., -1.7170,  0.7141,  1.4700]],\n",
      "\n",
      "         [[ 2.4114,  1.3076, -0.4585,  ...,  1.1612,  0.0569, -2.3169],\n",
      "          [ 1.3495,  1.7765,  0.1263,  ...,  1.1614,  0.0568, -2.3172],\n",
      "          [-0.9531,  0.9944,  0.6433,  ...,  1.1616,  0.0566, -2.3175],\n",
      "          ...,\n",
      "          [-1.0153, -0.8707, -0.4032,  ...,  1.1712,  0.0510, -2.3288],\n",
      "          [ 1.2925,  0.6257,  0.1902,  ...,  1.1715,  0.0508, -2.3291],\n",
      "          [ 2.4120,  1.6815,  0.6815,  ...,  1.1717,  0.0507, -2.3294]],\n",
      "\n",
      "         [[ 1.2487,  1.4503,  1.4415,  ..., -1.7049, -0.1272, -1.5117],\n",
      "          [-0.5350,  1.6345,  0.6317,  ..., -1.7049, -0.1275, -1.5116],\n",
      "          [-1.8268,  0.6677, -0.5170,  ..., -1.7050, -0.1279, -1.5116],\n",
      "          ...,\n",
      "          [ 0.8181, -0.5378,  1.3876,  ..., -1.7062, -0.1429, -1.5083],\n",
      "          [ 1.8889,  0.8907,  0.5155,  ..., -1.7062, -0.1432, -1.5083],\n",
      "          [ 1.2231,  1.6919, -0.6332,  ..., -1.7063, -0.1436, -1.5082]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-1.2882, -1.0448,  0.1326,  ...,  0.3346,  1.6627,  2.8040],\n",
      "          [-0.6226,  0.3435,  0.4637,  ...,  0.3347,  1.6623,  2.8039],\n",
      "          [ 0.6154,  1.4900,  0.5460,  ...,  0.3348,  1.6618,  2.8037],\n",
      "          ...,\n",
      "          [ 0.4353, -1.5512,  0.1750,  ...,  0.3383,  1.6429,  2.7970],\n",
      "          [-0.7877, -1.5329,  0.4863,  ...,  0.3384,  1.6424,  2.7969],\n",
      "          [-1.2865, -0.4352,  0.5368,  ...,  0.3385,  1.6419,  2.7967]],\n",
      "\n",
      "         [[ 0.1599,  0.4762,  2.1982,  ..., -1.6771, -2.3109,  3.5271],\n",
      "          [-0.5995,  0.8749,  1.7686,  ..., -1.6765, -2.3107,  3.5272],\n",
      "          [-0.8077,  0.6575,  0.3901,  ..., -1.6760, -2.3106,  3.5273],\n",
      "          ...,\n",
      "          [ 0.6831, -0.6077,  2.2099,  ..., -1.6539, -2.3042,  3.5317],\n",
      "          [ 0.7668,  0.0942,  1.6574,  ..., -1.6534, -2.3040,  3.5318],\n",
      "          [ 0.1455,  0.7298,  0.2158,  ..., -1.6528, -2.3038,  3.5319]],\n",
      "\n",
      "         [[ 0.4606, -0.7993,  0.6332,  ...,  1.1837,  5.8394, -2.0186],\n",
      "          [ 1.4908, -0.3157,  0.1025,  ...,  1.1841,  5.8397, -2.0186],\n",
      "          [ 1.1504,  0.3902, -0.4832,  ...,  1.1846,  5.8400, -2.0186],\n",
      "          ...,\n",
      "          [-1.5370, -0.4493,  0.5891,  ...,  1.2025,  5.8520, -2.0179],\n",
      "          [-0.9719, -0.8337,  0.0370,  ...,  1.2030,  5.8523, -2.0179],\n",
      "          [ 0.4867, -0.6310, -0.5349,  ...,  1.2034,  5.8526, -2.0178]]],\n",
      "\n",
      "\n",
      "        [[[-0.7064, -1.8095,  0.9089,  ..., -1.7266,  0.6913,  1.4727],\n",
      "          [-0.0502, -0.7395,  1.6062,  ..., -1.7264,  0.6918,  1.4726],\n",
      "          [ 0.6521,  0.8512,  1.4418,  ..., -1.7262,  0.6923,  1.4726],\n",
      "          ...,\n",
      "          [-0.0785, -0.9856,  1.0159,  ..., -1.7175,  0.7131,  1.4702],\n",
      "          [-0.7198, -1.8729,  1.6322,  ..., -1.7172,  0.7136,  1.4701],\n",
      "          [-0.6993, -1.4413,  1.3729,  ..., -1.7170,  0.7141,  1.4700]],\n",
      "\n",
      "         [[ 2.4114,  1.3076, -0.4585,  ...,  1.1612,  0.0569, -2.3169],\n",
      "          [ 1.3495,  1.7765,  0.1263,  ...,  1.1614,  0.0568, -2.3172],\n",
      "          [-0.9531,  0.9944,  0.6433,  ...,  1.1616,  0.0566, -2.3175],\n",
      "          ...,\n",
      "          [-1.0153, -0.8707, -0.4032,  ...,  1.1712,  0.0510, -2.3288],\n",
      "          [ 1.2925,  0.6257,  0.1902,  ...,  1.1715,  0.0508, -2.3291],\n",
      "          [ 2.4120,  1.6815,  0.6815,  ...,  1.1717,  0.0507, -2.3294]],\n",
      "\n",
      "         [[ 1.2487,  1.4503,  1.4415,  ..., -1.7049, -0.1272, -1.5117],\n",
      "          [-0.5350,  1.6345,  0.6317,  ..., -1.7049, -0.1275, -1.5116],\n",
      "          [-1.8268,  0.6677, -0.5170,  ..., -1.7050, -0.1279, -1.5116],\n",
      "          ...,\n",
      "          [ 0.8181, -0.5378,  1.3876,  ..., -1.7062, -0.1429, -1.5083],\n",
      "          [ 1.8889,  0.8907,  0.5155,  ..., -1.7062, -0.1432, -1.5083],\n",
      "          [ 1.2231,  1.6919, -0.6332,  ..., -1.7063, -0.1436, -1.5082]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-1.2882, -1.0448,  0.1326,  ...,  0.3346,  1.6627,  2.8040],\n",
      "          [-0.6226,  0.3435,  0.4637,  ...,  0.3347,  1.6623,  2.8039],\n",
      "          [ 0.6154,  1.4900,  0.5460,  ...,  0.3348,  1.6618,  2.8037],\n",
      "          ...,\n",
      "          [ 0.4353, -1.5512,  0.1750,  ...,  0.3383,  1.6429,  2.7970],\n",
      "          [-0.7877, -1.5329,  0.4863,  ...,  0.3384,  1.6424,  2.7969],\n",
      "          [-1.2865, -0.4352,  0.5368,  ...,  0.3385,  1.6419,  2.7967]],\n",
      "\n",
      "         [[ 0.1599,  0.4762,  2.1982,  ..., -1.6771, -2.3109,  3.5271],\n",
      "          [-0.5995,  0.8749,  1.7686,  ..., -1.6765, -2.3107,  3.5272],\n",
      "          [-0.8077,  0.6575,  0.3901,  ..., -1.6760, -2.3106,  3.5273],\n",
      "          ...,\n",
      "          [ 0.6831, -0.6077,  2.2099,  ..., -1.6539, -2.3042,  3.5317],\n",
      "          [ 0.7668,  0.0942,  1.6574,  ..., -1.6534, -2.3040,  3.5318],\n",
      "          [ 0.1455,  0.7298,  0.2158,  ..., -1.6528, -2.3038,  3.5319]],\n",
      "\n",
      "         [[ 0.4606, -0.7993,  0.6332,  ...,  1.1837,  5.8394, -2.0186],\n",
      "          [ 1.4908, -0.3157,  0.1025,  ...,  1.1841,  5.8397, -2.0186],\n",
      "          [ 1.1504,  0.3902, -0.4832,  ...,  1.1846,  5.8400, -2.0186],\n",
      "          ...,\n",
      "          [-1.5370, -0.4493,  0.5891,  ...,  1.2025,  5.8520, -2.0179],\n",
      "          [-0.9719, -0.8337,  0.0370,  ...,  1.2030,  5.8523, -2.0179],\n",
      "          [ 0.4867, -0.6310, -0.5349,  ...,  1.2034,  5.8526, -2.0178]]]],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>), tensor([[[[-0.7616, -0.3200,  0.5075,  ...,  0.8155,  1.3459,  1.7692],\n",
      "          [-0.7616, -0.3200,  0.5075,  ...,  0.8155,  1.3459,  1.7692],\n",
      "          [-0.7616, -0.3200,  0.5075,  ...,  0.8155,  1.3459,  1.7692],\n",
      "          ...,\n",
      "          [-0.7616, -0.3200,  0.5075,  ...,  0.8155,  1.3459,  1.7692],\n",
      "          [-0.7616, -0.3200,  0.5075,  ...,  0.8155,  1.3459,  1.7692],\n",
      "          [-0.7616, -0.3200,  0.5075,  ...,  0.8155,  1.3459,  1.7692]],\n",
      "\n",
      "         [[ 2.1508,  0.6693,  3.2077,  ...,  1.4209,  0.1253,  0.0490],\n",
      "          [ 2.1508,  0.6693,  3.2077,  ...,  1.4209,  0.1253,  0.0490],\n",
      "          [ 2.1508,  0.6693,  3.2077,  ...,  1.4209,  0.1253,  0.0490],\n",
      "          ...,\n",
      "          [ 2.1508,  0.6693,  3.2077,  ...,  1.4209,  0.1253,  0.0490],\n",
      "          [ 2.1508,  0.6693,  3.2077,  ...,  1.4209,  0.1253,  0.0490],\n",
      "          [ 2.1508,  0.6693,  3.2077,  ...,  1.4209,  0.1253,  0.0490]],\n",
      "\n",
      "         [[ 0.5354,  0.4022,  1.4193,  ...,  0.7244, -0.0793,  0.2868],\n",
      "          [ 0.5354,  0.4022,  1.4193,  ...,  0.7244, -0.0793,  0.2868],\n",
      "          [ 0.5354,  0.4022,  1.4193,  ...,  0.7244, -0.0793,  0.2868],\n",
      "          ...,\n",
      "          [ 0.5354,  0.4022,  1.4193,  ...,  0.7244, -0.0793,  0.2868],\n",
      "          [ 0.5354,  0.4022,  1.4193,  ...,  0.7244, -0.0793,  0.2868],\n",
      "          [ 0.5354,  0.4022,  1.4193,  ...,  0.7244, -0.0793,  0.2868]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 2.9793, -0.3205, -0.9637,  ...,  1.7678,  0.2464, -1.6140],\n",
      "          [ 2.9793, -0.3205, -0.9637,  ...,  1.7678,  0.2464, -1.6140],\n",
      "          [ 2.9793, -0.3205, -0.9637,  ...,  1.7678,  0.2464, -1.6140],\n",
      "          ...,\n",
      "          [ 2.9793, -0.3205, -0.9637,  ...,  1.7678,  0.2464, -1.6140],\n",
      "          [ 2.9793, -0.3205, -0.9637,  ...,  1.7678,  0.2464, -1.6140],\n",
      "          [ 2.9793, -0.3205, -0.9637,  ...,  1.7678,  0.2464, -1.6140]],\n",
      "\n",
      "         [[-1.0813,  0.5821,  0.8996,  ..., -0.2768,  3.6704, -0.8124],\n",
      "          [-1.0813,  0.5821,  0.8996,  ..., -0.2768,  3.6704, -0.8124],\n",
      "          [-1.0813,  0.5821,  0.8996,  ..., -0.2768,  3.6704, -0.8124],\n",
      "          ...,\n",
      "          [-1.0813,  0.5821,  0.8996,  ..., -0.2768,  3.6704, -0.8124],\n",
      "          [-1.0813,  0.5821,  0.8996,  ..., -0.2768,  3.6704, -0.8124],\n",
      "          [-1.0813,  0.5821,  0.8996,  ..., -0.2768,  3.6704, -0.8124]],\n",
      "\n",
      "         [[ 0.1065, -0.2949,  0.4204,  ..., -1.2553,  0.0716, -0.6233],\n",
      "          [ 0.1065, -0.2949,  0.4204,  ..., -1.2553,  0.0716, -0.6233],\n",
      "          [ 0.1065, -0.2949,  0.4204,  ..., -1.2553,  0.0716, -0.6233],\n",
      "          ...,\n",
      "          [ 0.1065, -0.2949,  0.4204,  ..., -1.2553,  0.0716, -0.6233],\n",
      "          [ 0.1065, -0.2949,  0.4204,  ..., -1.2553,  0.0716, -0.6233],\n",
      "          [ 0.1065, -0.2949,  0.4204,  ..., -1.2553,  0.0716, -0.6233]]],\n",
      "\n",
      "\n",
      "        [[[-0.7616, -0.3200,  0.5075,  ...,  0.8155,  1.3459,  1.7692],\n",
      "          [-0.7616, -0.3200,  0.5075,  ...,  0.8155,  1.3459,  1.7692],\n",
      "          [-0.7616, -0.3200,  0.5075,  ...,  0.8155,  1.3459,  1.7692],\n",
      "          ...,\n",
      "          [-0.7616, -0.3200,  0.5075,  ...,  0.8155,  1.3459,  1.7692],\n",
      "          [-0.7616, -0.3200,  0.5075,  ...,  0.8155,  1.3459,  1.7692],\n",
      "          [-0.7616, -0.3200,  0.5075,  ...,  0.8155,  1.3459,  1.7692]],\n",
      "\n",
      "         [[ 2.1508,  0.6693,  3.2077,  ...,  1.4209,  0.1253,  0.0490],\n",
      "          [ 2.1508,  0.6693,  3.2077,  ...,  1.4209,  0.1253,  0.0490],\n",
      "          [ 2.1508,  0.6693,  3.2077,  ...,  1.4209,  0.1253,  0.0490],\n",
      "          ...,\n",
      "          [ 2.1508,  0.6693,  3.2077,  ...,  1.4209,  0.1253,  0.0490],\n",
      "          [ 2.1508,  0.6693,  3.2077,  ...,  1.4209,  0.1253,  0.0490],\n",
      "          [ 2.1508,  0.6693,  3.2077,  ...,  1.4209,  0.1253,  0.0490]],\n",
      "\n",
      "         [[ 0.5354,  0.4022,  1.4193,  ...,  0.7244, -0.0793,  0.2868],\n",
      "          [ 0.5354,  0.4022,  1.4193,  ...,  0.7244, -0.0793,  0.2868],\n",
      "          [ 0.5354,  0.4022,  1.4193,  ...,  0.7244, -0.0793,  0.2868],\n",
      "          ...,\n",
      "          [ 0.5354,  0.4022,  1.4193,  ...,  0.7244, -0.0793,  0.2868],\n",
      "          [ 0.5354,  0.4022,  1.4193,  ...,  0.7244, -0.0793,  0.2868],\n",
      "          [ 0.5354,  0.4022,  1.4193,  ...,  0.7244, -0.0793,  0.2868]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 2.9793, -0.3205, -0.9637,  ...,  1.7678,  0.2464, -1.6140],\n",
      "          [ 2.9793, -0.3205, -0.9637,  ...,  1.7678,  0.2464, -1.6140],\n",
      "          [ 2.9793, -0.3205, -0.9637,  ...,  1.7678,  0.2464, -1.6140],\n",
      "          ...,\n",
      "          [ 2.9793, -0.3205, -0.9637,  ...,  1.7678,  0.2464, -1.6140],\n",
      "          [ 2.9793, -0.3205, -0.9637,  ...,  1.7678,  0.2464, -1.6140],\n",
      "          [ 2.9793, -0.3205, -0.9637,  ...,  1.7678,  0.2464, -1.6140]],\n",
      "\n",
      "         [[-1.0813,  0.5821,  0.8996,  ..., -0.2768,  3.6704, -0.8124],\n",
      "          [-1.0813,  0.5821,  0.8996,  ..., -0.2768,  3.6704, -0.8124],\n",
      "          [-1.0813,  0.5821,  0.8996,  ..., -0.2768,  3.6704, -0.8124],\n",
      "          ...,\n",
      "          [-1.0813,  0.5821,  0.8996,  ..., -0.2768,  3.6704, -0.8124],\n",
      "          [-1.0813,  0.5821,  0.8996,  ..., -0.2768,  3.6704, -0.8124],\n",
      "          [-1.0813,  0.5821,  0.8996,  ..., -0.2768,  3.6704, -0.8124]],\n",
      "\n",
      "         [[ 0.1065, -0.2949,  0.4204,  ..., -1.2553,  0.0716, -0.6233],\n",
      "          [ 0.1065, -0.2949,  0.4204,  ..., -1.2553,  0.0716, -0.6233],\n",
      "          [ 0.1065, -0.2949,  0.4204,  ..., -1.2553,  0.0716, -0.6233],\n",
      "          ...,\n",
      "          [ 0.1065, -0.2949,  0.4204,  ..., -1.2553,  0.0716, -0.6233],\n",
      "          [ 0.1065, -0.2949,  0.4204,  ..., -1.2553,  0.0716, -0.6233],\n",
      "          [ 0.1065, -0.2949,  0.4204,  ..., -1.2553,  0.0716, -0.6233]]],\n",
      "\n",
      "\n",
      "        [[[-0.7616, -0.3200,  0.5075,  ...,  0.8155,  1.3459,  1.7692],\n",
      "          [-0.7616, -0.3200,  0.5075,  ...,  0.8155,  1.3459,  1.7692],\n",
      "          [-0.7616, -0.3200,  0.5075,  ...,  0.8155,  1.3459,  1.7692],\n",
      "          ...,\n",
      "          [-0.7616, -0.3200,  0.5075,  ...,  0.8155,  1.3459,  1.7692],\n",
      "          [-0.7616, -0.3200,  0.5075,  ...,  0.8155,  1.3459,  1.7692],\n",
      "          [-0.7616, -0.3200,  0.5075,  ...,  0.8155,  1.3459,  1.7692]],\n",
      "\n",
      "         [[ 2.1508,  0.6693,  3.2077,  ...,  1.4209,  0.1253,  0.0490],\n",
      "          [ 2.1508,  0.6693,  3.2077,  ...,  1.4209,  0.1253,  0.0490],\n",
      "          [ 2.1508,  0.6693,  3.2077,  ...,  1.4209,  0.1253,  0.0490],\n",
      "          ...,\n",
      "          [ 2.1508,  0.6693,  3.2077,  ...,  1.4209,  0.1253,  0.0490],\n",
      "          [ 2.1508,  0.6693,  3.2077,  ...,  1.4209,  0.1253,  0.0490],\n",
      "          [ 2.1508,  0.6693,  3.2077,  ...,  1.4209,  0.1253,  0.0490]],\n",
      "\n",
      "         [[ 0.5354,  0.4022,  1.4193,  ...,  0.7244, -0.0793,  0.2868],\n",
      "          [ 0.5354,  0.4022,  1.4193,  ...,  0.7244, -0.0793,  0.2868],\n",
      "          [ 0.5354,  0.4022,  1.4193,  ...,  0.7244, -0.0793,  0.2868],\n",
      "          ...,\n",
      "          [ 0.5354,  0.4022,  1.4193,  ...,  0.7244, -0.0793,  0.2868],\n",
      "          [ 0.5354,  0.4022,  1.4193,  ...,  0.7244, -0.0793,  0.2868],\n",
      "          [ 0.5354,  0.4022,  1.4193,  ...,  0.7244, -0.0793,  0.2868]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 2.9793, -0.3205, -0.9637,  ...,  1.7678,  0.2464, -1.6140],\n",
      "          [ 2.9793, -0.3205, -0.9637,  ...,  1.7678,  0.2464, -1.6140],\n",
      "          [ 2.9793, -0.3205, -0.9637,  ...,  1.7678,  0.2464, -1.6140],\n",
      "          ...,\n",
      "          [ 2.9793, -0.3205, -0.9637,  ...,  1.7678,  0.2464, -1.6140],\n",
      "          [ 2.9793, -0.3205, -0.9637,  ...,  1.7678,  0.2464, -1.6140],\n",
      "          [ 2.9793, -0.3205, -0.9637,  ...,  1.7678,  0.2464, -1.6140]],\n",
      "\n",
      "         [[-1.0813,  0.5821,  0.8996,  ..., -0.2768,  3.6704, -0.8124],\n",
      "          [-1.0813,  0.5821,  0.8996,  ..., -0.2768,  3.6704, -0.8124],\n",
      "          [-1.0813,  0.5821,  0.8996,  ..., -0.2768,  3.6704, -0.8124],\n",
      "          ...,\n",
      "          [-1.0813,  0.5821,  0.8996,  ..., -0.2768,  3.6704, -0.8124],\n",
      "          [-1.0813,  0.5821,  0.8996,  ..., -0.2768,  3.6704, -0.8124],\n",
      "          [-1.0813,  0.5821,  0.8996,  ..., -0.2768,  3.6704, -0.8124]],\n",
      "\n",
      "         [[ 0.1065, -0.2949,  0.4204,  ..., -1.2553,  0.0716, -0.6233],\n",
      "          [ 0.1065, -0.2949,  0.4204,  ..., -1.2553,  0.0716, -0.6233],\n",
      "          [ 0.1065, -0.2949,  0.4204,  ..., -1.2553,  0.0716, -0.6233],\n",
      "          ...,\n",
      "          [ 0.1065, -0.2949,  0.4204,  ..., -1.2553,  0.0716, -0.6233],\n",
      "          [ 0.1065, -0.2949,  0.4204,  ..., -1.2553,  0.0716, -0.6233],\n",
      "          [ 0.1065, -0.2949,  0.4204,  ..., -1.2553,  0.0716, -0.6233]]],\n",
      "\n",
      "\n",
      "        [[[-0.7616, -0.3200,  0.5075,  ...,  0.8155,  1.3459,  1.7692],\n",
      "          [-0.7616, -0.3200,  0.5075,  ...,  0.8155,  1.3459,  1.7692],\n",
      "          [-0.7616, -0.3200,  0.5075,  ...,  0.8155,  1.3459,  1.7692],\n",
      "          ...,\n",
      "          [-0.7616, -0.3200,  0.5075,  ...,  0.8155,  1.3459,  1.7692],\n",
      "          [-0.7616, -0.3200,  0.5075,  ...,  0.8155,  1.3459,  1.7692],\n",
      "          [-0.7616, -0.3200,  0.5075,  ...,  0.8155,  1.3459,  1.7692]],\n",
      "\n",
      "         [[ 2.1508,  0.6693,  3.2077,  ...,  1.4209,  0.1253,  0.0490],\n",
      "          [ 2.1508,  0.6693,  3.2077,  ...,  1.4209,  0.1253,  0.0490],\n",
      "          [ 2.1508,  0.6693,  3.2077,  ...,  1.4209,  0.1253,  0.0490],\n",
      "          ...,\n",
      "          [ 2.1508,  0.6693,  3.2077,  ...,  1.4209,  0.1253,  0.0490],\n",
      "          [ 2.1508,  0.6693,  3.2077,  ...,  1.4209,  0.1253,  0.0490],\n",
      "          [ 2.1508,  0.6693,  3.2077,  ...,  1.4209,  0.1253,  0.0490]],\n",
      "\n",
      "         [[ 0.5354,  0.4022,  1.4193,  ...,  0.7244, -0.0793,  0.2868],\n",
      "          [ 0.5354,  0.4022,  1.4193,  ...,  0.7244, -0.0793,  0.2868],\n",
      "          [ 0.5354,  0.4022,  1.4193,  ...,  0.7244, -0.0793,  0.2868],\n",
      "          ...,\n",
      "          [ 0.5354,  0.4022,  1.4193,  ...,  0.7244, -0.0793,  0.2868],\n",
      "          [ 0.5354,  0.4022,  1.4193,  ...,  0.7244, -0.0793,  0.2868],\n",
      "          [ 0.5354,  0.4022,  1.4193,  ...,  0.7244, -0.0793,  0.2868]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 2.9793, -0.3205, -0.9637,  ...,  1.7678,  0.2464, -1.6140],\n",
      "          [ 2.9793, -0.3205, -0.9637,  ...,  1.7678,  0.2464, -1.6140],\n",
      "          [ 2.9793, -0.3205, -0.9637,  ...,  1.7678,  0.2464, -1.6140],\n",
      "          ...,\n",
      "          [ 2.9793, -0.3205, -0.9637,  ...,  1.7678,  0.2464, -1.6140],\n",
      "          [ 2.9793, -0.3205, -0.9637,  ...,  1.7678,  0.2464, -1.6140],\n",
      "          [ 2.9793, -0.3205, -0.9637,  ...,  1.7678,  0.2464, -1.6140]],\n",
      "\n",
      "         [[-1.0813,  0.5821,  0.8996,  ..., -0.2768,  3.6704, -0.8124],\n",
      "          [-1.0813,  0.5821,  0.8996,  ..., -0.2768,  3.6704, -0.8124],\n",
      "          [-1.0813,  0.5821,  0.8996,  ..., -0.2768,  3.6704, -0.8124],\n",
      "          ...,\n",
      "          [-1.0813,  0.5821,  0.8996,  ..., -0.2768,  3.6704, -0.8124],\n",
      "          [-1.0813,  0.5821,  0.8996,  ..., -0.2768,  3.6704, -0.8124],\n",
      "          [-1.0813,  0.5821,  0.8996,  ..., -0.2768,  3.6704, -0.8124]],\n",
      "\n",
      "         [[ 0.1065, -0.2949,  0.4204,  ..., -1.2553,  0.0716, -0.6233],\n",
      "          [ 0.1065, -0.2949,  0.4204,  ..., -1.2553,  0.0716, -0.6233],\n",
      "          [ 0.1065, -0.2949,  0.4204,  ..., -1.2553,  0.0716, -0.6233],\n",
      "          ...,\n",
      "          [ 0.1065, -0.2949,  0.4204,  ..., -1.2553,  0.0716, -0.6233],\n",
      "          [ 0.1065, -0.2949,  0.4204,  ..., -1.2553,  0.0716, -0.6233],\n",
      "          [ 0.1065, -0.2949,  0.4204,  ..., -1.2553,  0.0716, -0.6233]]]],\n",
      "       device='cuda:0', grad_fn=<TransposeBackward0>)))\n"
     ]
    }
   ],
   "source": [
    "model = LlamaAttention(config)\n",
    "\n",
    "device = torch.device(\"cuda\")\n",
    "\n",
    "model.load(\"/home/fuchiang137/.cache/huggingface/hub/models--decapoda-research--llama-7b-hf/snapshots/5f98eefcc80e437ef68d457ad7bf167c2c6a1348/pytorch_model-00019-of-00033.bin\")\n",
    "\n",
    "model = model.to(device)\n",
    "\n",
    "data_D = data.to(device)\n",
    "attention_mask_D = attention_mask.to(device)\n",
    "position_ids_D = position_ids.to(device)\n",
    "# output = model(data)\n",
    "\n",
    "past_key_value = None\n",
    "\n",
    "# attentiona mask\n",
    "# position_ids\n",
    "# specifies the position id of the corresponding hidden state tensor element\n",
    "# e.g. hid = [3, 4, 6] => pos_id = [0, 1, 2]\n",
    "# past_key_value\n",
    "# if use cache, past key value will contain past kv values\n",
    "output = model(hidden_states=data_D,\n",
    "               attention_mask=attention_mask_D,\n",
    "               position_ids=position_ids_D,\n",
    "               past_key_value=past_key_value,\n",
    "               output_attentions=False,\n",
    "               use_cache=True,)\n",
    "\n",
    "print(output)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f32abdc1",
   "metadata": {},
   "source": [
    "## Breaking down LlamaRotaryEmbedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99fd0d61",
   "metadata": {},
   "source": [
    "## tensorRT reshape (for apporximation of squeeze or unsqueeze)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4a051276",
   "metadata": {},
   "outputs": [],
   "source": [
    "# seq length is not specified, since it is a dynamic size\n",
    "def trt_create(batch_size, hidden_size, intermediate_size):\n",
    "    \n",
    "    logger = trt.Logger(trt.Logger.ERROR)\n",
    "    builder = trt.Builder(logger)\n",
    "\n",
    "    network = builder.create_network(1 << int(trt.NetworkDefinitionCreationFlag.EXPLICIT_BATCH))\n",
    "    config = builder.create_builder_config()\n",
    "\n",
    "    # input\n",
    "    inputT0 = network.add_input('inputT0', trt.DataType.FLOAT, (1, 24))\n",
    "\n",
    "    shuffle_layer = network.add_shuffle(inputT0)\n",
    "    shuffle_layer.reshape_dims = trt.Dims([1, 2, 3, 4])\n",
    "    shuffle_layer.second_transpose = trt.Permutation([0, 2, 1, 3])\n",
    "    \n",
    "    # slice_layer = network.add_slice(inputT0, start=(0, 0, 0), shape=inputT0.shape, stride=(1, 1, 1))\n",
    "\n",
    "    # output\n",
    "    network.mark_output(shuffle_layer.get_output(0))\n",
    "\n",
    "    engineString = builder.build_serialized_network(network, config)\n",
    "\n",
    "    return engineString"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "19f04923",
   "metadata": {},
   "outputs": [],
   "source": [
    "trt_engineStr = trt_create(batch_size, hidden_size, intermediate_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7d75453f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def trt_inference(batch_size, hidden_size, engineString, raw_data):\n",
    "#     print(engineString)\n",
    "#     print(\"Runtime\")\n",
    "    logger = trt.Logger(trt.Logger.ERROR)\n",
    "    engine = trt.Runtime(logger).deserialize_cuda_engine(engineString)\n",
    "    context = engine.create_execution_context()\n",
    "\n",
    "    # dynamic shape configure\n",
    "#     print(\"Set input shape\", (batch_size, 1, hidden_size))\n",
    "#     context.set_input_shape(\"inputT0\", (batch_size, 1, hidden_size))\n",
    "#     context.set_binding_shape(0, (batch_size, 1, hidden_size))\n",
    "#     origin_inputshape = context.get_binding_shape(0)\n",
    "\n",
    "#     print(\"Set input shape completed\")\n",
    "\n",
    "    data = np.array(raw_data)\n",
    "\n",
    "    _, stream = cudart.cudaStreamCreate()\n",
    "#     print(\"Reshaping\")\n",
    "\n",
    "    inputH0 = np.ascontiguousarray(data.reshape(-1))\n",
    "    outputH0 = np.empty(context.get_binding_shape(1), dtype=trt.nptype(engine.get_binding_dtype(1)))\n",
    "#     print(\"Reshaped\")\n",
    "\n",
    "    # initialize input and output data\n",
    "    _, inputD0 = cudart.cudaMallocAsync(inputH0.nbytes, stream)\n",
    "    _, outputD0 = cudart.cudaMallocAsync(outputH0.nbytes, stream)\n",
    "\n",
    "    # move input to device\n",
    "    cudart.cudaMemcpyAsync(inputD0, inputH0.ctypes.data, inputH0.nbytes, cudart.cudaMemcpyKind.cudaMemcpyHostToDevice, stream)\n",
    "\n",
    "    # execute\n",
    "#     print(\"execute\")\n",
    "    context.execute_async_v2([int(inputD0), int(outputD0)], stream)\n",
    "\n",
    "    # move output back to host\n",
    "    cudart.cudaMemcpyAsync(outputH0.ctypes.data, outputD0, outputH0.nbytes, cudart.cudaMemcpyKind.cudaMemcpyDeviceToHost, stream)\n",
    "\n",
    "    # wait for everythidden_sizeg\n",
    "    cudart.cudaStreamSynchronize(stream)\n",
    "\n",
    "    cudart.cudaStreamDestroy(stream)\n",
    "    cudart.cudaFree(inputD0)\n",
    "    cudart.cudaFree(outputD0)\n",
    "\n",
    "    return outputH0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cee21ba4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 24])\n",
      "tensor([[[[ 0.,  1.,  2.,  3.],\n",
      "          [ 4.,  5.,  6.,  7.],\n",
      "          [ 8.,  9., 10., 11.]],\n",
      "\n",
      "         [[12., 13., 14., 15.],\n",
      "          [16., 17., 18., 19.],\n",
      "          [20., 21., 22., 23.]]]])\n",
      "torch.Size([1, 2, 3, 4])\n",
      "tensor([[[[ 0.,  1.,  2.,  3.],\n",
      "          [12., 13., 14., 15.]],\n",
      "\n",
      "         [[ 4.,  5.,  6.,  7.],\n",
      "          [16., 17., 18., 19.]],\n",
      "\n",
      "         [[ 8.,  9., 10., 11.],\n",
      "          [20., 21., 22., 23.]]]])\n",
      "torch.Size([1, 3, 2, 4])\n"
     ]
    }
   ],
   "source": [
    "query_states = torch.arange(24, dtype=torch.float).reshape(1,24)\n",
    "print(query_states.shape)\n",
    "query_states = query_states.view(1, 2, 3, 4)\n",
    "print(query_states)\n",
    "print(query_states.shape)\n",
    "query_states = query_states.transpose(1, 2)\n",
    "print(query_states)\n",
    "print(query_states.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a1c17f2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11., 12., 13.,\n",
      "         14., 15., 16., 17., 18., 19., 20., 21., 22., 23.]])\n",
      "output_trt : (1, 3, 2, 4)\n",
      "[[[[ 0.  1.  2.  3.]\n",
      "   [12. 13. 14. 15.]]\n",
      "\n",
      "  [[ 4.  5.  6.  7.]\n",
      "   [16. 17. 18. 19.]]\n",
      "\n",
      "  [[ 8.  9. 10. 11.]\n",
      "   [20. 21. 22. 23.]]]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1579/1007095647.py:22: DeprecationWarning: Use get_tensor_shape instead.\n",
      "  outputH0 = np.empty(context.get_binding_shape(1), dtype=trt.nptype(engine.get_binding_dtype(1)))\n",
      "/tmp/ipykernel_1579/1007095647.py:22: DeprecationWarning: Use get_tensor_dtype instead.\n",
      "  outputH0 = np.empty(context.get_binding_shape(1), dtype=trt.nptype(engine.get_binding_dtype(1)))\n"
     ]
    }
   ],
   "source": [
    "query_states = torch.arange(24, dtype=torch.float).reshape(1,24)\n",
    "print(query_states)\n",
    "\n",
    "trt_output = trt_inference(batch_size, hidden_size, trt_engineStr, query_states)\n",
    "\n",
    "print(\"output_trt :\", trt_output.shape)\n",
    "print(trt_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cced848c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
